@article{agersnap2020,
  title = {Sermons as Data: {{Introducing}} a Corpus of 11,955 {{Danish}} Sermons},
  shorttitle = {Sermons as Data},
  author = {Agersnap, Anne and Kristensen-McLachlan, Ross Deans and Johansen, Kirstine Helboe and Schjødt, Uffe and Nielbo, Kristoffer Laigaard},
  date = {2020-12-08},
  journaltitle = {Journal of Cultural Analytics},
  shortjournal = {Journal of Cultural Analytics},
  volume = {5},
  number = {2},
  pages = {18238},
  publisher = {{Department of Languages, Literatures, and Cultures}},
  doi = {10.22148/001c.18238},
  url = {https://culturalanalytics.org/article/18238-sermons-as-data-introducing-a-corpus-of-11-955-danish-sermons},
  urldate = {2021-10-14},
  abstract = {In this article, we present a newly established corpus of 11,955 sermon manuscripts written by pastors in the Evangelical-Lutheran Church in Denmark (ELCD) in 2011-2016. We argue that this corpus provides a resource for studying how pastors within the same religious institution attend to general themes in church and society, respond to contemporary events, and represent social worlds. The aim of the article is twofold. 1) To present and discuss our approach to acquire and assemble the sermons corpus. This approach entailed sampling sermons directly from Danish pastors, and cleaning the corpus and annotating it with metadata manually. 2) To demonstrate the research potential of the corpus through a case study on gender representations in the sermons. We find that male and female pastors differ in their use of fundamental linguistic components, namely gendered pronouns and associated verbs. This affects how they assign agency to male and female characters in the corpus, and indicate that male and female pastors shape the social worlds in sermons in quite different ways. This case study therefore illustrates just one of the ways in which corpus-based research of Danish sermons may provide novel insights in the field of religion and society.},
  langid = {english},
  keywords = {NU-LEZEN},
  file = {/media/cloud/Zotero/storage/MUFJRXTW/agersnap_2020_sermons_as_data.pdf;/media/cloud/Zotero/storage/ZMN2WIGH/18238-sermons-as-data-introducing-a-corpus-of-11-955-danish-sermons.html}
}

@thesis{agersnap2021a,
  type = {phdthesis},
  title = {Collective {{Testimonies}} to {{Christianity}} and {{Time}}},
  author = {Agersnap, Anne},
  date = {2021},
  institution = {{Aarhus University}},
  location = {{Aarhus}},
  url = {https://pure.au.dk/portal/en/publications/collective-testimonies-to-christianity-and-time-a-collection-and-},
  langid = {english},
  keywords = {NU-LEZEN},
  file = {/media/cloud/Zotero/storage/K8FLWPME/Agersnap - Collective Testimonies to Christianity and Time.pdf}
}

@article{agersnap2022,
  title = {“{{What}} Is {{Love}} …?”: {{A Study}} of {{Thematic Constructions}} in 11.955 {{Danish Sermons}}},
  shorttitle = {“{{What}} Is {{Love}} …?},
  author = {Agersnap, Anne and Nielbo, Kristoffer L.},
  date = {2022-11-03},
  journaltitle = {International Journal of Homiletics},
  volume = {5},
  number = {1},
  pages = {86--116},
  issn = {2366-7958},
  doi = {10.21827/ijh.5.1.86-116},
  url = {https://ugp.rug.nl/ijh/article/view/39671},
  urldate = {2023-03-01},
  abstract = {Writing sermons in the Evangelical-Lutheran Church in Denmark (ELCD) is a shared practice, which means that pastors collectively produce a comprehensive text material, when they prepare their weekly sermons. This article studies a corpus of 11,955 ELCD sermons as a collective text production, as we investigate the role of theological discourses in emergent thematic fields in the corpus. With the aid of computational tools from the field of digital humanities, we investigate the following question: How can we map and interpret theological discourses in the collective production of sermons from the Evangelical-Lutheran Church of Denmark (ELCD)? Based on three complementary case studies, we explore the overall thematic framework of the corpus, the construction of specific theological concepts (love and sin) and the relationship between thematic constructions in the corpus and thematic content in the liturgical texts that pastors expound in sermons.},
  issue = {1},
  langid = {english},
  keywords = {computational humanities},
  file = {/media/cloud/Zotero/storage/PFVRPBDI/Agersnap_Nielbo_2022_“What is Love ….pdf}
}

@article{agersnap2022a,
  title = {The Legacy of the {{Danish}} ‘Church Fathers’ {{N}}.{{F}}.{{S}}. {{Grundtvig}} And},
  author = {Agersnap, Anne and Johansen, Kristine Helboe and Frøkj, Katrine},
  date = {2022},
  journaltitle = {The 6th Digital Humanities in the Nordic and Baltic Countries Conference},
  abstract = {In this paper, we investigate representations of the two influential Danish nineteenth-century theologians N.F.S. Grundtvig (1783-1872) and S.A. Kierkegaard (1813–1855) in a text corpus of 11,955 modern-day sermons from the Evangelical-Lutheran Church in Denmark (ELCD). Through a word embedding analysis we map the semantic habitat of the two theologians individually and in relation to key theological concepts. The study thus illuminates the reception and role of these historical figures in the ELCD today.},
  langid = {english},
  file = {/media/cloud/Zotero/storage/I3EK3YSS/Agersnap e.a. - The legacy of the Danish ‘church fathers’ N.F.S. G.pdf}
}

@article{ardanuy2021,
  title = {Neural {{Language Models}} for {{Nineteenth-Century English}}},
  author = {Ardanuy, Mariona Coll},
  date = {2021-09-27},
  volume = {7},
  number = {0},
  pages = {22},
  publisher = {{Ubiquity Press}},
  issn = {2059-481X},
  doi = {10.5334/johd.48},
  url = {https://openhumanitiesdata.metajnl.com/articles/10.5334/johd.48},
  urldate = {2023-04-04},
  abstract = {We present four types of neural language models trained on a large historical dataset of books in English, published between 1760 and 1900, and comprised of ≈5.1 billion tokens. The language model architectures include word type embeddings (word2vec and fastText) and contextualized models (BERT and Flair). For each architecture, we trained a model instance using the whole dataset. Additionally, we trained separate instances on text published before 1850 for the type embeddings, and four instances considering different time slices for BERT. Our models have already been used in various downstream tasks where they consistently improved performance. In this paper, we describe how the models have been created and outline their reuse potential.},
  issue = {0},
  langid = {american},
  file = {/media/cloud/Zotero/storage/3DBUBWDB/Ardanuy_2021_Neural Language Models for Nineteenth-Century English.pdf}
}

@thesis{arevalo2021,
  title = {Computational {{Approaches}} to {{Intertextuality}}},
  author = {Arévalo, Enrique Manjavacas},
  date = {2021},
  institution = {{University of Antwerp}},
  location = {{Antwerpen}},
  langid = {english},
  file = {/media/cloud/Zotero/storage/DAQ4Y4FL/Arévalo - Computational Approaches to Intertextuality.pdf}
}

@article{arnold2019,
  title = {Beyond Lexical Frequencies: Using {{R}} for Text Analysis in the Digital Humanities},
  shorttitle = {Beyond Lexical Frequencies},
  author = {Arnold, Taylor and Ballier, Nicolas and Lissón, Paula and Tilton, Lauren},
  date = {2019-12-01},
  journaltitle = {Language Resources and Evaluation},
  shortjournal = {Lang Resources \& Evaluation},
  volume = {53},
  number = {4},
  pages = {707--733},
  issn = {1574-0218},
  doi = {10.1007/s10579-019-09456-6},
  url = {https://doi.org/10.1007/s10579-019-09456-6},
  urldate = {2023-03-25},
  abstract = {This paper presents a combination of R packages—user contributed toolkits written in a common core programming language—to facilitate the humanistic investigation of digitised, text-based corpora.Our survey of text analysis packages includes those of our own creation (cleanNLP and fasttextM) as well as packages built by other research groups (stringi, readtext, hyphenatr, quanteda, and hunspell). By operating on generic object types, these packages unite research innovations in corpus linguistics, natural language processing, machine learning, statistics, and digital humanities. We begin by extrapolating on the theoretical benefits of R as an elaborate gluing language for bringing together several areas of expertise and compare it to linguistic concordancers and other tool-based approaches to text analysis in the digital humanities. We then showcase the practical benefits of an ecosystem by illustrating how R packages have been integrated into a digital humanities project. Throughout, the focus is on moving beyond the bag-of-words, lexical frequency model by incorporating linguistically-driven analyses in research.},
  langid = {english},
  keywords = {Digital humanities,NOG-LEZEN,R,Text interoperability,Text mining},
  file = {/media/cloud/Zotero/storage/Y28ICQBF/Arnold et al_2019_Beyond lexical frequencies.pdf}
}

@book{atteveldt2022,
  title = {Computational {{Analysis}} of {{Communication}}},
  author = {family=Atteveldt, given=Wouter, prefix=van, useprefix=false and Trilling, Damian and Calderon, Carlos Arcila},
  date = {2022-02-11},
  edition = {1st edition},
  publisher = {{Wiley-Blackwell}},
  location = {{Hoboken, NJ}},
  url = {https://cssbook.net/},
  abstract = {Provides clear guidance on leveraging computational techniques to answer social science questionsIn disciplines such as political science, sociology, psychology, and media studies, the use of computational analysis is rapidly increasing. Statistical modeling, machine learning, and other computational techniques are revolutionizing the way electoral results are predicted, social sentiment is measured, consumer interest is evaluated, and much more. Computational Analysis of Communication teaches social science students and practitioners how computational methods can be used in a broad range of applications, providing discipline-relevant examples, clear explanations, and practical guidance.Assuming little or no background in data science or computer linguistics, this accessible textbook teaches readers how to use state-of-the art computational methods to perform data-driven analyses of social science issues. A cross-disciplinary team of authors―with expertise in both the social sciences and computer science―explains how to gather and clean data, manage textual, audio-visual, and network data, conduct statistical and quantitative analysis, and interpret, summarize, and visualize the results. Offered in a unique hybrid format that integrates print, ebook, and open-access online viewing, this innovative resource:Covers the essential skills for social sciences courses on big data, data visualization, text analysis, predictive analytics, and othersIntegrates theory, methods, and tools to provide unified approach to the subjectIncludes sample code in Python and links to actual research questions and cases from social science and communication studiesDiscusses ethical and normative issues relevant to privacy, data ownership, and reproducible social scienceDeveloped in partnership with the International Communication Association and by the editors of Computational Communication ResearchComputational Analysis of Communication is an invaluable textbook and reference for students\,taking computational methods courses in social sciences, and for professional social scientists looking to incorporate computational methods into their work.},
  isbn = {978-1-119-68023-9},
  langid = {english},
  pagetotal = {336},
  keywords = {GELEZEN}
}

@book{baayen2008,
  title = {Analyzing {{Linguistic Data}}: {{A Practical Introduction}} to {{Statistics}} Using {{R}}},
  shorttitle = {Analyzing {{Linguistic Data}}},
  author = {Baayen, R. H.},
  date = {2008},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge}},
  doi = {10.1017/CBO9780511801686},
  url = {https://www.cambridge.org/core/books/analyzing-linguistic-data/B2AF752A30911F4144CA35E075C6B233},
  urldate = {2020-10-29},
  abstract = {Statistical analysis is a useful skill for linguists and psycholinguists, allowing them to understand the quantitative structure of their data. This textbook provides a straightforward introduction to the statistical analysis of language. Designed for linguists with a non-mathematical background, it clearly introduces the basic principles and methods of statistical analysis, using 'R', the leading computational statistics programme. The reader is guided step-by-step through a range of real data sets, allowing them to analyse acoustic data, construct grammatical trees for a variety of languages, quantify register variation in corpus linguistics, and measure experimental data using state-of-the-art models. The visualization of data plays a key role, both in the initial stages of data exploration and later on when the reader is encouraged to criticize various models. Containing over 40 exercises with model answers, this book will be welcomed by all linguists wishing to learn more about working with and presenting quantitative data.},
  isbn = {978-0-521-70918-7},
  file = {/media/cloud/Zotero/storage/29PWSBMR/03.0_pp_x_xiv_Preface.pdf;/media/cloud/Zotero/storage/4GWHUD66/08.0_pp_118_164_Clustering_and_classification.pdf;/media/cloud/Zotero/storage/6RKLFY43/05.0_pp_20_43_Graphical_data_exploration.pdf;/media/cloud/Zotero/storage/A8HMBDXD/02.0_pp_vii_ix_Contents.pdf;/media/cloud/Zotero/storage/ATVBF7AF/10.0_pp_241_302_Mixed_models.pdf;/media/cloud/Zotero/storage/CJDWHC8C/07.0_pp_68_117_Basic_statistical_methods.pdf;/media/cloud/Zotero/storage/EQKBZDMW/04.0_pp_1_19_An_introduction_to_R.pdf;/media/cloud/Zotero/storage/HLEHFUUP/11.0_pp_303_334_Solutions_to_the_exercises.pdf;/media/cloud/Zotero/storage/I328SICM/06.0_pp_44_67_Probability_distributions.pdf;/media/cloud/Zotero/storage/LAJ2NUVE/13.0_pp_342_346_References.pdf;/media/cloud/Zotero/storage/RZZ7U6DQ/01.0_pp_i_vi_Frontmatter.pdf;/media/cloud/Zotero/storage/T57SSUM5/09.0_pp_165_240_Regression_modeling.pdf;/media/cloud/Zotero/storage/ZS5PRDCV/12.0_pp_335_341_Overview_of_R_functions.pdf;/media/cloud/Zotero/storage/RLS54CRK/B2AF752A30911F4144CA35E075C6B233.html}
}

@book{ballod2000,
  title = {Predigthilfen aus dem Rechner? Computerunterstützte Predigtanalysen},
  author = {Ballod, Georg and Ballod, Matthias},
  date = {2000},
  publisher = {{Bern Verlag}},
  location = {{Marnheim}},
  langid = {ngerman},
  keywords = {GELEZEN}
}

@inproceedings{barbu2017,
  title = {Intertextuality Detection in Literary Texts Using {{Word2Vec}} Models},
  booktitle = {2017 21st {{International Conference}} on {{System Theory}}, {{Control}} and {{Computing}} ({{ICSTCC}})},
  author = {Barbu, Miruna-Stefania and Trausan-Matu, Stefan},
  date = {2017-10},
  pages = {262--265},
  doi = {10.1109/ICSTCC.2017.8107044},
  abstract = {This paper presents a research on using Google's Word2Vec model for intertextuality detection in literary texts. Word2Vec is considered a new and powerful Natural Language Processing semantic tool, used for computing text cohesion and similarity between documents. This article analyzes intertextuality in terms of semantic relations between words in novels. A similarity is calculated as a score: the higher it is, the stronger the relatedness between those two parts of text is. For experiments, different windows of text are used considering the dimensions of the two texts analyzed. For the highest scores matchings, a graphical representation is made to illustrate the parts of text which are linked.},
  eventtitle = {2017 21st {{International Conference}} on {{System Theory}}, {{Control}} and {{Computing}} ({{ICSTCC}})},
  keywords = {Computational modeling,Indexes,intertextuality,Large scale integration,Natural language processing,semantic models,Semantics,Tools,Word2Vec},
  file = {/media/cloud/Zotero/storage/N87UH43X/8107044.html}
}

@article{benoit2018,
  title = {Quanteda: {{An R}} Package for the Quantitative Analysis of Textual Data},
  author = {Benoit, Kenneth and Watanabe, Kohei and Wang, Haiyan and Nulty, Paul and Obeng, Adam and Müller, Stefan and Matsuo, Akitaka},
  date = {2018},
  journaltitle = {Journal of Open Source Software},
  volume = {3},
  number = {30},
  pages = {774},
  doi = {10.21105/joss.00774},
  url = {https://quanteda.io},
  keywords = {GELEZEN},
  file = {/media/cloud/Zotero/storage/75BVQJXW/Benoit et al_2018_quanteda.pdf}
}

@article{bernau2021,
  title = {From {{Christ}} to {{Compassion}}: {{The Changing Language}} of {{Pastoral Care}}},
  shorttitle = {From {{Christ}} to {{Compassion}}},
  author = {Bernau, John A.},
  date = {2021},
  journaltitle = {Journal for the Scientific Study of Religion},
  volume = {60},
  number = {2},
  pages = {362--381},
  issn = {1468-5906},
  doi = {10.1111/jssr.12711},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1111/jssr.12711},
  urldate = {2022-05-19},
  abstract = {The rise of neurology, psychology, and psychiatry over the last 100 years has challenged the clergy's historical monopoly on dealing with “personal problems” and mental well-being. In this study, I document the changing language of pastoral care by analyzing over seventy years of academic articles in the Journal of Pastoral Care and Counseling (N = 4,054) using structural topic modeling. Ultimately, I reveal a linguistic shift from the universal to the particular as pastoral care professionals drop language of human nature and morality for that of individual narratives. I also find a decline of overtly religious language since the 1950s in favor of a more ecumenical language of spirituality, hope, and presence. Both of these trends take place alongside a push for “evidence-based” pastoral care. Together, these linguistic shifts offer insight into a seventy-year struggle to provide authentic religious care in a world of competing alternatives.},
  langid = {english},
  keywords = {GELEZEN,health,historical sociology,pastoral care,religion,secularization,topic modeling},
  file = {/media/cloud/Zotero/storage/2VF9XEBY/Bernau_2021_From Christ to Compassion.pdf;/media/cloud/Zotero/storage/SQLKDLW4/jssr.html}
}

@online{bishop1990,
  title = {John {{Henry Newman}}: {{Pursuit}} of Truth in Preaching},
  shorttitle = {John {{Henry Newman}}},
  author = {Bishop, John},
  date = {1990-11-01T00:00:00+00:00},
  url = {https://www.preaching.com/articles/past-masters/john-henry-newman-pursuit-of-truth-in-preaching/},
  urldate = {2023-10-24},
  abstract = {This year we celebrate the centenary of John Henry Newman's death. The new ecumenical interest in Newman has centered largely in his parochial preaching in sermons he first gave as an Anglican and reissued as},
  langid = {american},
  organization = {{Preaching.com}},
  file = {/media/cloud/Zotero/storage/63IJU5KR/john-henry-newman-pursuit-of-truth-in-preaching.html}
}

@article{black2016,
  title = {The {{World Wide Web}} as {{Complex Data Set}}: {{Expanding}} the {{Digital Humanities}} into the {{Twentieth Century}} and {{Beyond}} through {{Internet Research}}},
  shorttitle = {The {{World Wide Web}} as {{Complex Data Set}}},
  author = {Black, Michael L.},
  date = {2016-03},
  journaltitle = {International Journal of Humanities and Arts Computing},
  shortjournal = {IJHAC},
  volume = {10},
  number = {1},
  pages = {95--109},
  publisher = {{Edinburgh University Press}},
  issn = {1753-8548},
  doi = {10.3366/ijhac.2016.0162},
  url = {https://www.euppublishing.com/doi/10.3366/ijhac.2016.0162},
  urldate = {2023-11-10},
  abstract = {While intellectual property protections effectively frame digital humanities text mining as a field primarily for the study of the nineteenth century, the Internet offers an intriguing object of study for humanists working in later periods. As a complex data source, the World Wide Web presents its own methodological challenges for digital humanists, but lessons learned from projects studying large nineteenth century corpora offer helpful starting points. Complicating matters further, legal and ethical questions surrounding web scraping, or the practice of large scale data retrieval over the Internet, will require humanists to frame their research to distinguish it from commercial and malicious activities. This essay reviews relevant research in the digital humanities and new media studies in order to show how web scraping might contribute to humanities research questions. In addition to recommendations for addressing the complex concerns surrounding web scraping this essay also provides a basic overview of the process and some recommendations for resources.},
  keywords = {20th century,intellectual property,text mining,webscraping,world wide web},
  file = {/media/cloud/Zotero/storage/X9DS2DU2/Black_2016_The World Wide Web as Complex Data Set.pdf}
}

@incollection{blair2012,
  title = {The {{Poet-Preachers}}},
  booktitle = {The {{Oxford Handbook}} of the {{British Sermon}} 1689-1901},
  author = {Blair, Kirstie},
  editor = {Francis, Keith A. and Gibson, William and Morgan-Guy, John and Tennant, Bob and Ellison, Robert H.},
  date = {2012-10-04},
  pages = {0},
  publisher = {{Oxford University Press}},
  doi = {10.1093/oxfordhb/9780199583591.013.0034},
  url = {https://doi.org/10.1093/oxfordhb/9780199583591.013.0034},
  urldate = {2023-07-20},
  abstract = {This article examines nineteenth-century perceptions of the interactions between poetry and preaching. It suggests that it was the perceived sympathetic force in the great nineteenth-century sermon writers, such as John Henry Newman, Frederick W. Robertson, and Charles Kingsley, that made their sermons appear “poetic” to their listeners and readers. Many nineteenth-century preachers also used poetry in their sermons because of its assumed value in conveying sympathy.},
  isbn = {978-0-19-958359-1},
  file = {/media/cloud/Zotero/storage/R4SAHGTQ/Blair_2012_The Poet-Preachers.pdf;/media/cloud/Zotero/storage/R2RJG7UE/291473594.html}
}

@book{boehmke2019,
  title = {Hands-{{On Machine Learning}} with {{R}}},
  author = {Boehmke, Bradley and Greenwell, Brandon},
  date = {2019},
  publisher = {{Taylor \& Francis}},
  url = {https://bradleyboehmke.github.io/HOML/},
  urldate = {2021-02-11},
  abstract = {A Machine Learning Algorithmic Deep Dive Using R.},
  keywords = {NU-LEZEN},
  file = {/media/cloud/Zotero/storage/MEAAAWN4/HOML.html}
}

@article{boguraev,
  title = {{{TimeML-Compliant Text Analysis}} for {{Temporal Reasoning}}},
  author = {Boguraev, Branimir and Ando, Rie Kubota},
  pages = {7},
  abstract = {Reasoning with time1 needs more than just a list of temporal expressions. TimeML—an emerging standard for temporal annotation as a language capturing properties and relationships among timedenoting expressions and events in text—is a good starting point for bridging the gap between temporal analysis of documents and reasoning with the information derived from them. Hard as TimeMLcompliant analysis is, the small size of the only currently available annotated corpus makes it even harder. We address this problem with a hybrid TimeML annotator, which uses cascaded finite-state grammars (for temporal expression analysis, shallow syntactic parsing, and feature generation) together with a machine learning component capable of effectively using large amounts of unannotated data.},
  langid = {english},
  file = {/media/cloud/Zotero/storage/NDBPXVG6/Boguraev en Ando - TimeML-Compliant Text Analysis for Temporal Reason.pdf}
}

@mvbook{bondi2010,
  title = {Keyness in texts},
  author = {Bondi, Marina and Scott, Mike},
  date = {2010},
  series = {Studies in corpus linguistics},
  number = {v. 41},
  publisher = {{John Benjamins Pub. Co.}},
  location = {{Amsterdam ;}},
  url = {http://www.dawsonera.com/abstract/9789027287663},
  urldate = {2023-04-01},
  isbn = {978-90-272-8766-3 978-1-282-89733-5},
  langid = {Engels},
  volumes = {1 online resource (vi, 251 pages) : illustrations},
  file = {/media/cloud/Zotero/storage/68XYWZ9W/ScottMikeBondiM_2010_ProblemsInInvestigati_KeynessInTexts.pdf;/media/cloud/Zotero/storage/6VBSGVAP/ScottMikeBondiM_2010_ThreeConceptsOfKeywor_KeynessInTexts.pdf;/media/cloud/Zotero/storage/BTZVU4XG/ScottMikeBondiM_2010_MetaphoricalKeynessIn_KeynessInTexts.pdf;/media/cloud/Zotero/storage/CBMB746E/Bondi_Scott_2010_Keyness in texts.pdf;/media/cloud/Zotero/storage/UBWMFIHS/ScottMikeBondiM_2010_PerspectivesOnKeyword_KeynessInTexts.pdf}
}

@article{boss2020,
  title = {Visualizing {{Jonathan Edwards}}},
  author = {Boss, Robert L.},
  date = {2020-23},
  journaltitle = {Polish Journal for American Studies},
  volume = {14},
  pages = {187--196,269},
  publisher = {{University of Warsaw}},
  location = {{Warsaw, Poland}},
  issn = {17339154},
  url = {https://www.proquest.com/docview/2473443528/abstract/14707CDD29B34DD0PQ/1},
  urldate = {2023-05-24},
  abstract = {This article begins by introducing Jonathan Edwards, the eighteenth century American philosopher theologian from Northampton, Massachusetts. Edwards believed that the world of nature had communicative properties, full of types and symbols, and indeed, was a kind of language of God. This article posits that Edwards' typological language of nature, encapsulated in his notebook "Images of Divine Things" and throughout his written corpus, can be explored through the lense of Digital Humanities and network analysis using Processing and Python programming languages. Next, the article summarizes recent Edwards-focused DH projects by Kenneth Minkema, Michał Choiński, and Michael Keller. The article then recounts the history and development of the Visual Edwards project and how it expands exploration of the 26 volume Yale letterpress edition of The Works of Jonathan Edwards. Features of the Visual Edwards software are introduced briefly, as well as print publications flowing from the project.},
  langid = {english},
  pagetotal = {187-196,269},
  keywords = {18th century,21st century,Corpus analysis,Dictionaries,Digital humanities,God,History--History Of North And South America,Humanities: Comprehensive Works,Interdisciplinary aspects,Nature,NU-LEZEN,Philosophers,Programming languages,Software,Spirituality,Theologians,Theology},
  file = {/media/cloud/Zotero/storage/I8GZC5A7/Boss_2020_Visualizing Jonathan Edwards.pdf}
}

@article{boukes2020,
  title = {What’s the {{Tone}}? {{Easy Doesn}}’t {{Do It}}: {{Analyzing Performance}} and {{Agreement Between Off-the-Shelf Sentiment Analysis Tools}}},
  shorttitle = {What’s the {{Tone}}?},
  author = {Boukes, Mark and family=Velde, given=Bob, prefix=van de, useprefix=true and Araujo, Theo and Vliegenthart, Rens},
  date = {2020-04-02},
  journaltitle = {Communication Methods and Measures},
  shortjournal = {Communication Methods and Measures},
  volume = {14},
  number = {2},
  pages = {83--104},
  issn = {1931-2458, 1931-2466},
  doi = {10.1080/19312458.2019.1671966},
  url = {https://www.tandfonline.com/doi/full/10.1080/19312458.2019.1671966},
  urldate = {2022-03-23},
  abstract = {This article scrutinizes the method of automated content analysis to measure the tone of news coverage. We compare a range of off-the-shelf sentiment analysis tools to manually coded economic news as well as examine the agreement between these dictionary approaches themselves. We assess the performance of five off-the-shelf sentiment analysis tools and two tailor-made dictionary-based approaches. The analyses result in five conclusions. First, there is little overlap between the off-the-shelf tools; causing wide divergence in terms of tone measurement. Second, there is no stronger overlap with manual coding for short texts (i.e., headlines) than for long texts (i.e., full articles). Third, an approach that combines individual dictionaries achieves a comparably good performance. Fourth, precision may increase to acceptable levels at higher levels of granularity. Fifth, performance of dictionary approaches depends more on the number of relevant keywords in the dictionary than on the number of valenced words as such; a small tailor-made lexicon was not inferior to large established dictionaries. Altogether, we conclude that off-the-shelf sentiment analysis tools are mostly unreliable and unsuitable for research purposes – at least in the context of Dutch economic news – and manual validation for the specific language, domain, and genre of the research project at hand is always warranted.},
  langid = {english},
  keywords = {NOG-LEZEN},
  file = {/media/cloud/Zotero/storage/VS5VDFBG/Boukes e.a. - 2020 - What’s the Tone Easy Doesn’t Do It Analyzing Per.pdf}
}

@article{boussalis2020,
  title = {Political {{Speech}} in {{Religious Sermons}}},
  author = {Boussalis, Constantine and Coan, Travis G. and Holman, Mirya R.},
  date = {2020},
  journaltitle = {Politics and Religion},
  pages = {1--28},
  publisher = {{Cambridge University Press}},
  issn = {1755-0483, 1755-0491},
  doi = {10.1017/S1755048320000334},
  url = {http://www.cambridge.org/core/journals/politics-and-religion/article/political-speech-in-religious-sermons/53583EA3BD5F4223B5E31AA279698563},
  urldate = {2020-11-11},
  abstract = {Religious leaders and congregants alike report high levels of political discussions in their churches. Yet, direct observations of political topics in a wide set of religious settings are rare. We examine the nature of political speech by clergy with a novel dataset of over 110,000 sermons. Using a computational text analysis approach and multiple forms of validation, we find political content in more than a third of religious sermons and that seven of 10 pastors discuss political topics at some point. Common topics include the economy, war, homosexuality, welfare, and abortion. We then use a geographic data to link the sermons to demographic and political information around the church and to information about the church and pastor to evaluate the variation of political content in sermons. We find that most pastors—across location and denomination—engage around political topics, confirming the intertwined nature of religion and politics in the United States.},
  langid = {english},
  keywords = {NOG-LEZEN},
  file = {/media/cloud/Zotero/storage/EHSXR2NM/boussalis_undefineded_political_speech_in_religious.pdf;/media/cloud/Zotero/storage/7Y4ITQCW/53583EA3BD5F4223B5E31AA279698563.html}
}

@unpublished{bowman2015,
  title = {A Large Annotated Corpus for Learning Natural Language Inference},
  author = {Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher and Manning, Christopher D.},
  date = {2015-08-21},
  eprint = {1508.05326},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1508.05326},
  urldate = {2022-02-01},
  abstract = {Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.},
  keywords = {Computer Science - Computation and Language},
  file = {/media/cloud/Zotero/storage/TPUVN46K/Bowman et al_2015_A large annotated corpus for learning natural language inference.pdf;/media/cloud/Zotero/storage/9EACIQTW/1508.html}
}

@article{broeker2023,
  title = {Can an Algorithm Become Delusional? {{Evaluating}} Ontological Commitments and Methodology of Computational Psychiatry},
  shorttitle = {Can an Algorithm Become Delusional?},
  author = {Broeker, Marianne D. and Broome, Matthew R.},
  date = {2023-02-23},
  journaltitle = {Phenomenology and the Cognitive Sciences},
  shortjournal = {Phenom Cogn Sci},
  issn = {1572-8676},
  doi = {10.1007/s11097-023-09895-1},
  url = {https://doi.org/10.1007/s11097-023-09895-1},
  urldate = {2023-03-21},
  abstract = {The computational approach to psychiatric disorders, including delusions, promises explanation and treatment. Here, we argue that an information processing approach might be misleading to understand psychopathology and requires further refinement. We explore the claim of computational psychiatry being a bridge between phenomenology and physiology while focussing on the ontological commitments and corresponding methodology computational psychiatry is based on. Interconnecting ontological claims and methodological practices, the paper illustrates the structure of theory-building and testing in computational psychiatry.},
  langid = {english},
  keywords = {Bayes theorem,Delusions,Deontic approach,Predictive Processing,Unconscious inference},
  file = {/media/cloud/Zotero/storage/A4LGWHW8/Broeker_Broome_2023_Can an algorithm become delusional.pdf}
}

@article{chavasse2009,
  title = {Newman the {{Preacher}}},
  author = {Chavasse, Paul},
  date = {2009-12-31},
  journaltitle = {Cahiers victoriens et édouardiens},
  number = {70 automne},
  publisher = {{Presses universitaires de la Méditerranée}},
  issn = {0220-5610},
  doi = {10.4000/cve.4798},
  url = {https://journals.openedition.org/cve/4798},
  urldate = {2023-03-31},
  abstract = {This article illustrates the remarkable impact which Newman’s preaching had upon his hearers and his extraordinary ability to enter into the minds and hearts of both hearers and readers. But emphasis is placed above all on the underlying aim of all his preaching, summed up in the formula of Fr Henry Tristram, one time superior of the Birmingham Oratory which Newman founded: for Newman, preaching must constitute an incentive not only to ‘living better’ but also to ‘praying better’. However stern a moralist he may appear to be at times, he never indulges in mere moralising, any more than he simply expounds doctrine for its own sake. Both are, on the contrary, placed in the service of helping hearers and readers to deepen progressively a lived relationship with the God whom Newman himself discovered in the depths of his own consciousness. Thus his rediscovery, through his reading of the Church Fathers, of the central role in Christianity of the doctrines of the Incarnation, the Resurrection and the Trinity, led him to explore the implications for the Christian of the theme of the ‘indwelling’ of the Holy Spirit and to suggest that the ‘true Christian’ may ‘almost be defined’ as ‘one who has a ruling sense of God’s presence within him’. It is above all in his exploration of the intricate relationship existing between dogma, ethics and spirituality, combined with his keen psychological penetration, that Newman’s greatness as a preacher lies.},
  issue = {70 automne},
  langid = {english}
}

@book{chen,
  title = {Corpus {{Linguistics}}},
  author = {Chen, Alvin Cheng-Hsien},
  url = {https://alvinntnu.github.io/NTNU_ENC2036_LECTURES/},
  urldate = {2023-03-22},
  abstract = {ENC2036 Course material first edition},
  file = {/media/cloud/Zotero/storage/5LBSTBI3/NTNU_ENC2036_LECTURES.html}
}

@article{chen2018,
  title = {Using {{Machine Learning}} to {{Support Qualitative Coding}} in {{Social Science}}: {{Shifting}} the {{Focus}} to {{Ambiguity}}},
  shorttitle = {Using {{Machine Learning}} to {{Support Qualitative Coding}} in {{Social Science}}},
  author = {Chen, Nan-Chen and Drouhard, Margaret and Kocielnik, Rafal and Suh, Jina and Aragon, Cecilia R.},
  date = {2018-06-21},
  journaltitle = {ACM Transactions on Interactive Intelligent Systems},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  volume = {8},
  number = {2},
  pages = {9:1--9:20},
  issn = {2160-6455},
  doi = {10.1145/3185515},
  url = {http://doi.org/10.1145/3185515},
  urldate = {2022-03-05},
  abstract = {Machine learning (ML) has become increasingly influential to human society, yet the primary advancements and applications of ML are driven by research in only a few computational disciplines. Even applications that affect or analyze human behaviors and social structures are often developed with limited input from experts outside of computational fields. Social scientists—experts trained to examine and explain the complexity of human behavior and interactions in the world—have considerable expertise to contribute to the development of ML applications for human-generated data, and their analytic practices could benefit from more human-centered ML methods. Although a few researchers have highlighted some gaps between ML and social sciences [51, 57, 70], most discussions only focus on quantitative methods. Yet many social science disciplines rely heavily on qualitative methods to distill patterns that are challenging to discover through quantitative data. One common analysis method for qualitative data is qualitative coding. In this article, we highlight three challenges of applying ML to qualitative coding. Additionally, we utilize our experience of designing a visual analytics tool for collaborative qualitative coding to demonstrate the potential in using ML to support qualitative coding by shifting the focus to identifying ambiguity. We illustrate dimensions of ambiguity and discuss the relationship between disagreement and ambiguity. Finally, we propose three research directions to ground ML applications for social science as part of the progression toward human-centered machine learning.},
  keywords = {ambiguity,computational social science,human-centered machine learning,machine learning,qualitative coding,Social scientists},
  file = {/media/cloud/Zotero/storage/YWG3QTA4/Chen et al_2018_Using Machine Learning to Support Qualitative Coding in Social Science.pdf}
}

@book{chollet2018,
  title = {Deep {{Learning}} with {{R}}},
  author = {Chollet, Francois and Allaire, J. J.},
  date = {2018-02-09},
  edition = {1 edition},
  publisher = {{Manning Publications}},
  location = {{Shelter Island, NY}},
  abstract = {Summary Deep Learning with R introduces the world of deep learning using the powerful Keras library and its R language interface. The book builds your understanding of deep learning through intuitive explanations and practical examples.  Continue your journey into the world of deep learning with Deep Learning with R in Motion, a practical, hands-on video course available exclusively at Manning.com (www.manning.com/livevideo/deep-\hspace{0pt}learning-with-r-in-motion). Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications. About the Technology Machine learning has made remarkable progress in recent years. Deep-learning systems now enable previously impossible smart applications, revolutionizing image recognition and natural-language processing, and identifying complex patterns in data. The Keras deep-learning library provides data scientists and developers working in R a state-of-the-art toolset for tackling deep-learning tasks. About the Book Deep Learning with R introduces the world of deep learning using the powerful Keras library and its R language interface. Initially written for Python as Deep Learning with Python by Keras creator and Google AI researcher François Chollet and adapted for R by RStudio founder J. J. Allaire, this book builds your understanding of deep learning through intuitive explanations and practical examples. You'll practice your new skills with R-based applications in computer vision, natural-language processing, and generative models. What's Inside Deep learning from first principlesSetting up your own deep-learning environmentImage classification and generationDeep learning for text and sequences About the Reader You'll need intermediate R programming skills. No previous experience with machine learning or deep learning is assumed. About the Authors François Chollet is a deep-learning researcher at Google and the author of the Keras library. J.J. Allaire is the founder of RStudio and the author of the R interfaces to TensorFlow and Keras. Table of Contents PART 1 - FUNDAMENTALS OF DEEP LEARNINGWhat is deep learning?Before we begin: the mathematical building blocks of neural networksGetting started with neural networksFundamentals of machine learningPART 2 - DEEP LEARNING IN PRACTICEDeep learning for computer visionDeep learning for text and sequencesAdvanced deep-learning best practicesGenerative deep learningConclusions},
  isbn = {978-1-61729-554-6},
  langid = {english},
  pagetotal = {360},
  keywords = {NU-LEZEN},
  file = {/media/cloud/Zotero/storage/288XADV9/Chollet en Allaire - 2018 - Deep Learning with R.pdf}
}

@article{clarke2022,
  title = {Keywords through Time: {{Tracking}} Changes in Press Discourses of {{Islam}}},
  shorttitle = {Keywords through Time},
  author = {Clarke, Isobelle and Brookes, Gavin and McEnery, Tony},
  date = {2022-10-20},
  journaltitle = {International Journal of Corpus Linguistics},
  volume = {27},
  number = {4},
  pages = {399--427},
  publisher = {{John Benjamins}},
  issn = {1384-6655, 1569-9811},
  doi = {10.1075/ijcl.22011.cla},
  url = {https://www.jbe-platform.com/content/journals/10.1075/ijcl.22011.cla},
  urldate = {2023-04-11},
  abstract = {Abstract This paper applies a new approach to the identification of discourses, based on Multiple Correspondence Analysis (MCA), to the study of discourse variation over time. The MCA approach to keywords deals with a major issue with the use of keywords to identify discourses: the allocation of individual keywords to multiple discourses. Yet, as this paper demonstrates, the approach also allows us to observe variation in the prevalence of discourses over time. The MCA approach to keywords allows the allocation of individual texts to multiple discourses based on patterns of keyword co-occurrence. Metadata in the corpus data analysed (here, UK newspaper articles about Islam) can then be used to map those discourses over time, resulting in a clear view of how the discourses vary relative to one another as time progresses. The paper argues that the drivers for these fluctuations are language external; the real-world events reported on in the newspapers.},
  langid = {english},
  file = {/media/cloud/Zotero/storage/C2SI2NXS/Clarke et al_2022_Keywords through time.pdf;/media/cloud/Zotero/storage/YZCT2WCI/ijcl.22011.html}
}

@article{cohena,
  title = {Learning to {{Classify Email}} into "{{Speech Acts}}"},
  author = {Cohen, William W and Carvalho, Vitor R and Mitchell, Tom M},
  pages = {8},
  abstract = {It is often useful to classify email according to the intent of the sender (e.g., "propose a meeting", "deliver information"). We present experimental results in learning to classify email in this fashion, where each class corresponds to a verbnoun pair taken from a predefined ontology describing typical “email speech acts”. We demonstrate that, although this categorization problem is quite different from “topical” text classification, certain categories of messages can nonetheless be detected with high precision (above 80\%) and reasonable recall (above 50\%) using existing text-classification learning methods. This result suggests that useful task-tracking tools could be constructed based on automatic classification into this taxonomy.},
  langid = {english},
  file = {/media/cloud/Zotero/storage/6D2WEMSS/Cohen et al. - Learning to Classify Email into Speech Acts.pdf}
}

@article{cooper2017,
  title = {Assessing the {{Possible Relationship}} between the {{Sentiment}} of {{Church-related Tweets}} and {{Church Growth}}},
  author = {Cooper, Anthony-Paul},
  date = {2017-03-01},
  journaltitle = {Studies in Religion/Sciences Religieuses},
  volume = {46},
  number = {1},
  pages = {37--49},
  publisher = {{SAGE Publications Ltd}},
  issn = {0008-4298},
  doi = {10.1177/0008429816664215},
  url = {https://doi.org/10.1177/0008429816664215},
  urldate = {2023-12-06},
  abstract = {This article examines the possible relationship between the sentiment of church-related tweets and church growth. It finds that within the sample of tweets analysed, there is a statistically significant relationship between the sentiment of a church-related tweet and the presence of church growth in the geographical area from which the tweet was posted. This work builds on the body of knowledge surrounding church growth and decline in the United Kingdom, by seeking to better understand how new sources of data, in this case freely available social media data, can be used to gain a better understanding of the behaviours of churches which regularly form, merge, move, split and close.},
  langid = {english},
  file = {/media/cloud/Zotero/storage/VKXE5YGQ/Cooper_2017_Assessing the Possible Relationship between the Sentiment of Church-related.pdf}
}

@article{covington2010,
  title = {Cutting the {{Gordian Knot}}: {{The Moving-Average Type}}–{{Token Ratio}} ({{MATTR}})},
  shorttitle = {Cutting the {{Gordian Knot}}},
  author = {Covington, Michael A. and McFall, Joe D.},
  date = {2010-05-01},
  journaltitle = {Journal of Quantitative Linguistics},
  volume = {17},
  number = {2},
  pages = {94--100},
  publisher = {{Routledge}},
  issn = {0929-6174},
  doi = {10.1080/09296171003643098},
  url = {https://doi.org/10.1080/09296171003643098},
  urldate = {2023-03-25},
  abstract = {Type–token ratio (TTR), or vocabulary size divided by text length (V/N), is a time-honoured but unsatisfactory measure of lexical diversity. The problem is that the TTR of a text sample is affected by its length. We present an algorithm for rapidly computing TTR through a moving window that is independent of text length, and we demonstrate that this measurement can detect changes within a text as well as differences between texts.}
}

@article{debruyne2021,
  title = {Mixing and {{Matching Emotion Frameworks}}: {{Investigating Cross-Framework Transfer Learning}} for {{Dutch Emotion Detection}}},
  shorttitle = {Mixing and {{Matching Emotion Frameworks}}},
  author = {De Bruyne, Luna and De Clercq, Orphée and Hoste, Véronique},
  date = {2021-01},
  journaltitle = {Electronics},
  volume = {10},
  number = {21},
  pages = {2643},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2079-9292},
  doi = {10.3390/electronics10212643},
  url = {https://www.mdpi.com/2079-9292/10/21/2643},
  urldate = {2022-02-17},
  abstract = {Emotion detection has become a growing field of study, especially seeing its broad application potential. Research usually focuses on emotion classification, but performance tends to be rather low, especially when dealing with more advanced emotion categories that are tailored to specific tasks and domains. Therefore, we propose the use of the dimensional emotion representations valence, arousal and dominance (VAD), in an emotion regression task. Firstly, we hypothesize that they can improve performance of the classification task, and secondly, they might be used as a pivot mechanism to map towards any given emotion framework, which allows tailoring emotion frameworks to specific applications. In this paper, we examine three cross-framework transfer methodologies: multi-task learning, in which VAD regression and classification are learned simultaneously; meta-learning, where VAD regression and emotion classification are learned separately and predictions are jointly used as input for a meta-learner; and a pivot mechanism, which converts the predictions of the VAD model to emotion classes. We show that dimensional representations can indeed boost performance for emotion classification, especially in the meta-learning setting (up to 7\% macro F1-score compared to regular emotion classification). The pivot method was not able to compete with the base model, but further inspection suggests that it could be efficient, provided that the VAD regression model is further improved.},
  issue = {21},
  langid = {english},
  keywords = {emotion detection,emotion frameworks,multi-task learning,transfer learning},
  file = {/media/cloud/Zotero/storage/M3RJECFZ/De Bruyne et al_2021_Mixing and Matching Emotion Frameworks.pdf;/media/cloud/Zotero/storage/RKEYRS3H/2643.html}
}

@book{deng2018,
  title = {Deep {{Learning}} in {{Natural Language Processing}}},
  editor = {Deng, Li and Liu, Yang},
  date = {2018},
  publisher = {{Springer Singapore}},
  location = {{Singapore}},
  doi = {10.1007/978-981-10-5209-5},
  url = {http://link.springer.com/10.1007/978-981-10-5209-5},
  urldate = {2022-01-31},
  isbn = {978-981-10-5208-8 978-981-10-5209-5},
  langid = {english},
  file = {/media/cloud/Zotero/storage/7ND4M6PT/Deng en Liu - 2018 - Deep Learning in Natural Language Processing.pdf}
}

@incollection{dix2021a,
  title = {Methodische Grundlagen},
  booktitle = {Die christliche Predigt im 21. Jahrhundert: Multimodale Analyse einer Kommunikativen Gattung},
  author = {Dix, Carolin},
  editor = {Dix, Carolin},
  date = {2021},
  series = {Wissen, Kommunikation und Gesellschaft},
  pages = {71--90},
  publisher = {{Springer Fachmedien}},
  location = {{Wiesbaden}},
  doi = {10.1007/978-3-658-33855-8_10},
  url = {https://doi.org/10.1007/978-3-658-33855-8_10},
  urldate = {2023-02-13},
  abstract = {Das Kapitel stellt die zentralen methodischen Forschungslinien dar, denen die Analyse folgt. Im Fokus steht die Entwicklung von der Ethnomethodologie bis hin zur Multimodalen Interaktionsanalyse sowie Entwicklungen innerhalb der Vidoanalyse als wichtigem Instrument einer mikrostrukturellen, multimodalen, empirischen Untersuchung.},
  isbn = {978-3-658-33855-8},
  langid = {ngerman},
  file = {/media/cloud/Zotero/storage/MEAZJX67/Dix_2021_Methodische Grundlagen.pdf}
}

@book{drucker2021,
  title = {The {{Digital Humanities Coursebook}}: {{An Introduction}} to {{Digital Methods}} for {{Research}} and {{Scholarship}}},
  shorttitle = {The {{Digital Humanities Coursebook}}},
  author = {Drucker, Johanna},
  date = {2021-03-25},
  publisher = {{Routledge}},
  location = {{London}},
  doi = {10.4324/9781003106531},
  abstract = {The Digital Humanities Coursebook provides critical frameworks for the application of digital humanities tools and platforms, which have become an integral part of work across a wide range of disciplines.  Written by an expert with twenty years of experience in this field, the book is focused on the principles and fundamental concepts for application, rather than on specific tools or platforms. Each chapter contains examples of projects, tools, or platforms that demonstrate these principles in action. The book is structured to complement courses on digital humanities and provides a series of modules, each of which is organized around a set of concerns and topics, thought experiments and questions, as well as specific discussions of the ways in which tools and platforms work. The book covers a wide range of topics and clearly details how to integrate the acquisition of expertise in data, metadata, classification, interface, visualization, network analysis, topic modeling, data mining, mapping, and web presentation with issues in intellectual property, sustainability, privacy, and the ethical use of information.  Written in an accessible and engaging manner, The Digital Humanities Coursebook will be a useful guide for anyone teaching or studying a course in the areas of digital humanities, library and information science, English, or computer science. The book will provide a framework for direct engagement with digital humanities and, as such, should be of interest to others working across the humanities as well.},
  isbn = {978-1-00-310653-1},
  pagetotal = {252},
  keywords = {NOG-LEZEN},
  file = {/media/cloud/Zotero/storage/BZPLJ2XZ/Drucker_2021_The Digital Humanities Coursebook.pdf}
}

@book{dunn2022,
  title = {Natural {{Language Processing}} for {{Corpus Linguistics}}},
  author = {Dunn, Jonathan},
  date = {2022-04-30},
  edition = {1},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/9781009070447},
  url = {https://www.cambridge.org/core/product/identifier/9781009070447/type/element},
  urldate = {2023-03-30},
  abstract = {Corpus analysis can be expanded and scaled up by incorporating computational methods from natural language processing. This Element shows how text classification and text similarity models can extend our ability to undertake corpus linguistics across very large corpora. These computational methods are becoming increasingly important as corpora grow too large for more traditional types of linguistic analysis. We draw on five case studies to show how and why to use computational methods, ranging from usage-based grammar to authorship analysis to using social media for corpus-based sociolinguistics. Each section is accompanied by an interactive code notebook that shows how to implement the analysis in Python. A stand-alone Python package is also available to help readers use these methods with their own data. Because large-scale analysis introduces new ethical problems, this Element pairs each new methodology with a discussion of potential ethical implications.},
  isbn = {978-1-00-907044-7 978-1-00-907443-8},
  langid = {english},
  keywords = {NOG-LEZEN},
  file = {/media/cloud/Zotero/storage/KDWPTCHU/Dunn - 2022 - Natural Language Processing for Corpus Linguistics.pdf}
}

@book{edwards2004,
  title = {A {{History}} of {{Preaching}}},
  shorttitle = {History},
  author = {Edwards, Jr., O.C.},
  date = {2004},
  publisher = {{Abingdon Press}},
  location = {{Nashville}}
}

@article{eisele2023,
  title = {Capturing a {{News Frame}} – {{Comparing Machine-Learning Approaches}} to {{Frame Analysis}} with {{Different Degrees}} of {{Supervision}}},
  author = {Eisele, Olga and Heidenreich, Tobias and Litvyak, Olga and Boomgaarden, Hajo G.},
  date = {2023-07-03},
  journaltitle = {Communication Methods and Measures},
  volume = {17},
  number = {3},
  pages = {205--226},
  publisher = {{Routledge}},
  issn = {1931-2458},
  doi = {10.1080/19312458.2023.2230560},
  url = {https://doi.org/10.1080/19312458.2023.2230560},
  urldate = {2023-10-16},
  abstract = {The empirical identification of frames drawing on automated text analysis has been discussed intensely with regard to the validity of measurements. Adding to an evolving discussion on automated frame identification, we systematically contrast different machine-learning approaches with a manually coded gold standard to shed light on the implications of using one or the other: (1) topic modeling, (2) keyword-assisted topic modeling (keyATM), and (3) supervised machine learning as three popular and/or promising approaches. Manual coding is based on the Policy Frames codebook, providing an established base that allows future research to dovetail our contribution. Analysing a large dataset of 12 Austrian newspapers’ EU coverage over 11 years (2009–2019), we contribute to addressing the methodological challenges that have emerged for social scientists interested in employing automated tools for frame analysis. While results confirm the superiority of supervised machine-learning, the semi-supervised approach (keyATM) seems unfit for frame analysis, whereas the topic model covers the middle ground. Results are extensively discussed regarding their implications for the validity of approaches.},
  file = {/media/cloud/Zotero/storage/VK36VHXL/Eisele et al_2023_Capturing a News Frame – Comparing Machine-Learning Approaches to Frame.pdf}
}

@inbook{elwert2021,
  title = {Computational {{Text Analysis}}},
  booktitle = {The {{Routledge Handbook}} of {{Research Methods}} in the {{Study}} of {{Religion}}},
  author = {Elwert, Frederik},
  date = {2021-11-11},
  edition = {2},
  pages = {164--179},
  publisher = {{Routledge}},
  location = {{London}},
  doi = {10.4324/9781003222491-12},
  url = {https://www.taylorfrancis.com/books/9781003222491/chapters/10.4324/9781003222491-12},
  urldate = {2023-04-17},
  bookauthor = {Engler, Steven and Stausberg, Michael},
  isbn = {978-1-00-322249-1},
  langid = {english},
  keywords = {GELEZEN},
  file = {/media/cloud/Zotero/storage/RN3HJPYG/Elwert - 2021 - Computational Text Analysis.pdf}
}

@book{engemann2019b,
  title = {Homiletics. {{Principles}} and {{Patterns}} of {{Reasoning}}},
  shorttitle = {Homiletics},
  author = {Engemann, Wilfried},
  date = {2019},
  publisher = {{Walter de Gruyter}},
  location = {{Berlin / Boston}},
  abstract = {Die vorliegende Homiletik geht von einer methodisch kohärenten und interdisziplinär konvergenten Erörterung der Prämissen, Elemente, und Strukturen des Predigtprozesses selbst aus, die in eine zeitgenössische "Theologie der Predigt" mündet. Das Spektrum homiletischer Fragen wird auf verschiedene Sequenzen der Kommunikation des Evangeliums bezogen (z. B. Person, Text, Situation, Struktur, Sprache und Rede) wobei psychologische, semiotische, soziologische und rhetorische u. a. Reflexionsperspektiven zum Tragen kommen. Den Modellen der Predigtanalyse wird ein eigenes Kapitel gewidmet. Predigt- und theologiegeschichtliche Fragen werden in die problemorientierte Durchdringung des Stoffs eingebettet, so dass Studierende, Pfarrer und Lehrende je nach Bedarf optimal mit den Grundlagen und Herausforderungen des Fachs vertraut gemacht werden.},
  isbn = {978-3-11-044025-6},
  langid = {english},
  pagetotal = {602},
  keywords = {Religion / Christian Life / General,Religion / Christian Ministry / Counseling \& Recovery,Religion / Christian Ministry / Pastoral Resources,Religion / Christian Ministry / Preaching,Religion / Christian Rituals \& Practice / Worship \& Liturgy,Religion / General,Religion / Sermons / Christian},
  file = {/media/cloud/Zotero/storage/F2DLUGRD/10.1515_9783110440256-001.pdf;/media/cloud/Zotero/storage/JZWFABI5/Engemann_2019_Homiletics.pdf;/media/cloud/Zotero/storage/K87X7XVP/10.1515_9783110440256-002.pdf}
}

@article{farinola2023,
  title = {Hermeneutical Postphenomenology: {{Computational}} Tools and the Lure of Objectivity},
  shorttitle = {Hermeneutical Postphenomenology},
  author = {Farinola, Augustine},
  date = {2023-01-09},
  journaltitle = {Digital Scholarship in the Humanities},
  shortjournal = {Digital Scholarship in the Humanities},
  pages = {fqac074},
  issn = {2055-7671},
  doi = {10.1093/llc/fqac074},
  url = {https://doi.org/10.1093/llc/fqac074},
  urldate = {2023-07-26},
  abstract = {This paper examines some of the issues surrounding this ‘computational turn’ and proposes a ‘human focused’ approach. It begins by addressing questions concerning whether there is a need empirical evidence in literary studies, and the roles played by human and technical agents in interpretative practices. It adopts Don Ihde Postphenomenological ideas (especially ‘embodiment’ and ‘hermeneutics’ human-technology relations) to expatiate on the nature of relationship that exists between a literary scholar and a computational tool during interpretative practices. On one hand, it uses postphenomenomelogy as a theoretical framework to provides rich conceptual terminologies by which we could interrogate humanists-computer relationship within the practice of computational textual analysis. And on the other hand, uses postphenomenology method to highlight the notion of subjectivity in contrast to the promise of observer-independent objectivity.The research appraise the impact of quantification and visualizations in literary studies using the research output of Franco Moretti and his colleagues at the Stanford Lit Lab, as well as performs textual experimentation on some corpora using Stefan Sinclair and Geoffrey Rockwell’s Voyant tools and Python NLP packages. But refutes the idea that quantification and visualization of textual data with the use of computational tools and methods could guarantee objectivity of textual interpretation in literary studies. The argument in this paper is divided across its three sections: The first section discusses the goal of reading; it concerns ‘Close reading’ and ‘Distant reading’. The second section questions the possibility of objectivity in textual interpretation using quantifiable and visual evidence provided as the output of the computational analysis of humanistic texts. Then, in the third section defends the following claims: a) the human person is the principal actor in the interpretation process, and all other forms of representation or visualisation of the text are meant to aid humans; b. data in the humanities are not limited to printed texts, but include digitised, born-digital and electronic text in various digital forms (images, sounds, videos, etc.); c. the humanities aim more at interpretive practices than a quest for verifiable knowledge; d. interpretative practices in the humanities focus on humans and their experiences; e. attention must be drawn to human developers' subjectivity whenever we are using computational interpretative tools – on the ground that this will help in bracketing of our biases, prejudices, preconceptions, and theoretical frameworks. The paper concludes with the argument that computational tools used for textual analysis in the humanities need interpretation as they are not neutral in hermeneutic practices. It argues that the humans involved in the creation of those tools are prone to errors, have preferences, and incorporate their subjective ideas into developmental processes. Then proposes ‘Hermeneutical Postphenomenology’ as an ideal lens through which the claim to objectivity could be debunked. Then recommends that our productivity in textual scholarship can be enriched when we understand the true nature of the relationship between the human inquirer and technical agents within the cognitive assemblage.},
  file = {/media/cloud/Zotero/storage/QVJB5R2I/Farinola_2023_Hermeneutical postphenomenology.pdf;/media/cloud/Zotero/storage/U24WHR37/6965075.html}
}

@incollection{francis2012,
  title = {Sermon {{Studies}}: {{Major Issues}} and {{Future Directions}}},
  shorttitle = {Sermon {{Studies}}},
  booktitle = {The {{Oxford Handbook}} of the {{British Sermon}} 1689-1901},
  author = {Francis, Keith A.},
  editor = {Francis, Keith A. and Gibson, William and Morgan-Guy, John and Tennant, Bob and Ellison, Robert H.},
  date = {2012-10-04},
  pages = {0},
  publisher = {{Oxford University Press}},
  doi = {10.1093/oxfordhb/9780199583591.013.0037},
  url = {https://doi.org/10.1093/oxfordhb/9780199583591.013.0037},
  urldate = {2023-07-20},
  abstract = {This article begins with a brief description of scholars’ long neglect of sermons, in their efforts to understand the phenomenon of British religion in the modern era. It argues that sermons are worthy of further integration into the historiography of British history. The article also discusses some future directions for sermon studies.},
  isbn = {978-0-19-958359-1},
  file = {/media/cloud/Zotero/storage/M2HAIE8F/Francis_2012_Sermon Studies.pdf;/media/cloud/Zotero/storage/T53BFP57/291474945.html}
}

@book{friginal2020,
  title = {The {{Routledge Handbook}} of {{Corpus Approaches}} to {{Discourse Analysis}}},
  editor = {Friginal, Eric and Hardy, Jack A.},
  date = {2020-12-18},
  publisher = {{Routledge}},
  location = {{London}},
  doi = {10.4324/9780429259982},
  abstract = {The Routledge Handbook of Corpus Approaches to Discourse Analysis highlights the diversity, breadth, and depth of corpus approaches to discourse analysis, compiling new and original research from notable scholars across the globe. Chapters showcase recent developments influenced by the exponential growth in linguistic computing, advances in corpus design and compilation, and the applications of sound quantitative and interpretive techniques in analyzing text and discourse patterns. Key discourse domains covered by 35 empirical chapters include: • Research contexts and methodological considerations; • Naturally occurring spoken, professional, and academic discourse; • Corpus approaches to conversational discourse, media discourse, and professional and academic writing. The Routledge Handbook of Corpus Approaches to Discourse Analysis is key reading for both experienced and novice researchers working at the intersection of corpus linguistics and discourse analysis, as well as anyone undertaking study in these areas, as well as anyone interested in related fields and adjacent research approaches.},
  isbn = {978-0-429-25998-2},
  pagetotal = {662},
  file = {/media/cloud/Zotero/storage/GG8TLCE3/Friginal_Hardy_2020_The Routledge Handbook of Corpus Approaches to Discourse Analysis.pdf}
}

@article{gatti2020,
  title = {Improving {{Dutch}} Sentiment Analysis in {{Pattern}}},
  author = {Gatti, Lorenzo and family=Stegeren, given=Judith, prefix=van, useprefix=false},
  date = {2020-12-12},
  journaltitle = {Computational Linguistics in the Netherlands Journal},
  volume = {10},
  pages = {73--89},
  issn = {2211-4009},
  url = {https://www.clinjournal.org/index.php/clinj/article/view/105},
  urldate = {2022-02-16},
  abstract = {In this paper we investigate methods for improving the sentiment analysis functionality of Pattern.nl, the Dutch submodule of Pattern, an open-source library for web mining and natural language processing. We discuss the impact on performance of three different potential improvements: extending the module’s internal sentiment lexicon; removing subsets of neutral words from the sentiment lexicon; and improving the algorithm for combining multiple word-level sentiment ratings into a sentence-level sentiment rating. We evaluated the improvements on datasets from the product review domain (books, clothing and music) and a dataset of short emotional stories. The experiments show that lexicon expansion does not lead to better results; new normalization techniques, on the other hand, show a limited but consistent performance increase for sentiment ratings.},
  langid = {english},
  keywords = {NOG-LEZEN},
  file = {/media/cloud/Zotero/storage/EY8IX2AG/Gatti_Stegeren_2020_Improving Dutch sentiment analysis in Pattern.pdf}
}

@article{geiger2020,
  title = {Garbage {{In}}, {{Garbage Out}}? {{Do Machine Learning Application Papers}} in {{Social Computing Report Where Human-Labeled Training Data Comes From}}?},
  shorttitle = {Garbage {{In}}, {{Garbage Out}}?},
  author = {Geiger, R. Stuart and Yu, Kevin and Yang, Yanlai and Dai, Mindy and Qiu, Jie and Tang, Rebekah and Huang, Jenny},
  date = {2020-01-27},
  journaltitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  eprint = {1912.08320},
  eprinttype = {arxiv},
  pages = {325--336},
  doi = {10.1145/3351095.3372862},
  url = {http://arxiv.org/abs/1912.08320},
  urldate = {2022-01-13},
  abstract = {Many machine learning projects for new application areas involve teams of humans who label data for a particular purpose, from hiring crowdworkers to the paper's authors labeling the data themselves. Such a task is quite similar to (or a form of) structured content analysis, which is a longstanding methodology in the social sciences and humanities, with many established best practices. In this paper, we investigate to what extent a sample of machine learning application papers in social computing --- specifically papers from ArXiv and traditional publications performing an ML classification task on Twitter data --- give specific details about whether such best practices were followed. Our team conducted multiple rounds of structured content analysis of each paper, making determinations such as: Does the paper report who the labelers were, what their qualifications were, whether they independently labeled the same items, whether inter-rater reliability metrics were disclosed, what level of training and/or instructions were given to labelers, whether compensation for crowdworkers is disclosed, and if the training data is publicly available. We find a wide divergence in whether such practices were followed and documented. Much of machine learning research and education focuses on what is done once a "gold standard" of training data is available, but we discuss issues around the equally-important aspect of whether such data is reliable in the first place.},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Digital Libraries,Computer Science - Machine Learning},
  file = {/media/cloud/Zotero/storage/FK7DGPQP/Geiger et al_2020_Garbage In, Garbage Out.pdf;/media/cloud/Zotero/storage/2SQ6KZR5/1912.html}
}

@article{geiger2021,
  title = {“{{Garbage}} in, Garbage out” Revisited: {{What}} Do Machine Learning Application Papers Report about Human-Labeled Training Data?},
  shorttitle = {“{{Garbage}} in, Garbage out” Revisited},
  author = {Geiger, R. Stuart and Cope, Dominique and Ip, Jamie and Lotosh, Marsha and Shah, Aayush and Weng, Jenny and Tang, Rebekah},
  date = {2021-11-05},
  journaltitle = {Quantitative Science Studies},
  shortjournal = {Quantitative Science Studies},
  volume = {2},
  number = {3},
  pages = {795--827},
  issn = {2641-3337},
  doi = {10.1162/qss_a_00144},
  url = {https://doi.org/10.1162/qss_a_00144},
  urldate = {2022-01-13},
  abstract = {Supervised machine learning, in which models are automatically derived from labeled training data, is only as good as the quality of that data. This study builds on prior work that investigated to what extent “best practices” around labeling training data were followed in applied ML publications within a single domain (social media platforms). In this paper, we expand by studying publications that apply supervised ML in a far broader spectrum of disciplines, focusing on human-labeled data. We report to what extent a random sample of ML application papers across disciplines give specific details about whether best practices were followed, while acknowledging that a greater range of application fields necessarily produces greater diversity of labeling and annotation methods. Because much of machine learning research and education only focuses on what is done once a “ground truth” or “gold standard” of training data is available, it is especially relevant to discuss issues around the equally important aspect of whether such data is reliable in the first place. This determination becomes increasingly complex when applied to a variety of specialized fields, as labeling can range from a task requiring little-to-no background knowledge to one that must be performed by someone with career expertise.},
  file = {/media/cloud/Zotero/storage/XXP35RTW/Geiger et al_2021_“Garbage in, garbage out” revisited.pdf;/media/cloud/Zotero/storage/SAC9NRXU/Garbage-in-garbage-out-revisited-What-do-machine.html}
}

@incollection{gibson2012,
  title = {The {{British Sermon}} 1689–1901: {{Quantities}}, {{Performance}}, and {{Culture}}},
  shorttitle = {The {{British Sermon}} 1689–1901},
  booktitle = {The {{Oxford Handbook}} of the {{British Sermon}} 1689-1901},
  author = {Gibson, William},
  editor = {Francis, Keith A. and Gibson, William and Morgan-Guy, John and Tennant, Bob and Ellison, Robert H.},
  date = {2012-10-04},
  pages = {0},
  publisher = {{Oxford University Press}},
  doi = {10.1093/oxfordhb/9780199583591.013.0001},
  url = {https://doi.org/10.1093/oxfordhb/9780199583591.013.0001},
  urldate = {2023-07-20},
  abstract = {The ubiquity of the sermon between 1689 and 1901 is an aspect of contemporary experience that has been largely forgotten. Yet sitting below a pulpit hearing a sermon is a cultural experience can be said to have been shared by all classes and conditions of people in Britain. This article begins by setting out figures for printed sermons and public preaching. It then discusses the material culture of the sermon.},
  isbn = {978-0-19-958359-1},
  file = {/media/cloud/Zotero/storage/XCIB6XWS/Gibson_2012_The British Sermon 1689–1901.pdf;/media/cloud/Zotero/storage/52LWRF7G/291459197.html}
}

@book{gillings2023,
  title = {Corpus-{{Assisted Discourse Studies}}},
  author = {Gillings, Mathew and Mautner, Gerlinde and Baker, Paul},
  date = {2023-05-31},
  edition = {1},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/9781009168144},
  url = {https://www.cambridge.org/core/product/identifier/9781009168144/type/element},
  urldate = {2023-03-30},
  abstract = {The breadth and spread of corpus-assisted discourse studies (CADS) indicate its usefulness for exploring language use within a social context. However, its theoretical foundations, limitations, and epistemological implications must be considered so that we can adjust our research designs accordingly. This Element offers a compact guide to which corpus linguistic tools are available and how they can contribute to finding out more about discourse. It will appeal to researchers both new and experienced, within the CADS community and beyond.},
  isbn = {978-1-00-916814-4 978-1-00-916815-1},
  langid = {english},
  keywords = {GELEZEN},
  file = {/media/cloud/Zotero/storage/Q8SYRFSZ/Gillings e.a. - 2023 - Corpus-Assisted Discourse Studies.pdf}
}

@article{graham2019,
  title = {Using {{Natural Language Processing}} to {{Search}} for {{Textual References}}},
  author = {Graham, Brett},
  date = {2019-05-14},
  journaltitle = {Ancient Manuscripts in Digital Culture},
  pages = {115--132},
  publisher = {{Brill}},
  doi = {10.1163/9789004399297_008},
  url = {https://brill.com/view/book/edcoll/9789004399297/BP000007.xml},
  urldate = {2022-03-10},
  abstract = {Abstract This paper explains how recent advances in NLP technology can be harnessed to search for allusions and influences. The algorithms that have been used in several recent projects in the Digital Humanities are briefly analyzed with the aim of determining how effective these algorithms are in detecting the variety of reference forms that were used in ancient literature. Using the lessons learned from this analysis, a generic NLP algorithm is presented, including a set of syntax rules for textual references. The algorithm is designed to be generic so that it can be used to detect any type of textual reference in any type of text (or even an oral allusion to an oral speech). Finally, the benefits of this approach are highlighted using the results from testing the algorithm on the Pastoral Epistles.},
  langid = {english},
  keywords = {Ancient Near East and Egypt,Biblical Studies,Classical Studies,Codicology,Epigraphy \& Papyrology,General,Jewish Studies,Literature \& Linguistics,Papyrology \& Philology,Religion in Antiquity,Religious Studies},
  file = {/media/cloud/Zotero/storage/QYCGJEZG/Graham - 2019 - Using Natural Language Processing to Search for Te.pdf}
}

@book{gries2013,
  title = {Statistics for {{Linguistics}} with {{R}}: {{A Practical Introduction}}},
  shorttitle = {Statistics for {{Linguistics}} with {{R}}},
  author = {Gries, Stefan Th},
  date = {2013-03-15},
  journaltitle = {Statistics for Linguistics with R},
  publisher = {{De Gruyter Mouton}},
  url = {https://www-degruyter-com.proxy-ub.rug.nl/view/title/300775},
  urldate = {2020-10-29},
  abstract = {{$<$}p{$>$}This book is the revised and extended second edition of {$<$}em{$>$}Statistics for Linguistics with R{$<$}/em{$>$}. The volume is an introduction to statistics for linguists using the open source software R. It is aimed at students and instructors/professors with little or no statistical background and is written in a non-technical and reader-friendly/accessible style. {$<$}/p{$>$}  {$<$}p{$>$}It first introduces in detail the overall logic underlying quantitative studies: exploration, hypothesis formulation and operationalization, and the notion and meaning of significance tests. It then introduces some basics of the software R relevant to statistical data analysis. A chapter on descriptive statistics explains how summary statistics for frequencies, averages, and correlations are generated with R and how they are graphically represented best. A chapter on analytical statistics explains how statistical tests are performed in R on the basis of many different linguistic case studies: For nearly every single example, it is explained what the structure of the test looks like, how hypotheses are formulated, explored, and tested for statistical significance, how the results are graphically represented, and how one would summarize them in a paper/article. A chapter on selected multifactorial methods introduces how more complex research designs can be studied: methods for the study of multifactorial frequency data, correlations, tests for means, and binary response data are discussed and exemplified step-by-step. Also, the exploratory approach of hierarchical cluster analysis is illustrated in detail. {$<$}/p{$>$}  {$<$}p{$>$}The book comes with many exercises, boxes with short think breaks and warnings, recommendations for further study, and answer keys as well as a statistics for linguists newsgroup on the companion website.{$<$}/p{$>$}  {$<$}p{$>$}Just like the first edition, it is aimed at students, faculty, and researchers with little or no statistical background in statistics or the open source programming language R. It avoids mathematical jargon and discusses the logic and structure of quantitative studies and introduces descriptive statistics as well as a range of monofactorial statistical tests for frequencies, distributions, means, dispersions, and correlations. The comprehensive revision includes new small sections on programming topics that facilitate statistical analysis, the addition of a variety of statistical functions readers can apply to their own data, a revision of overview sections on statistical tests and regression modeling, a complete rewrite of the chapter on multifactorial approaches, which now contains sections on linear regression, binary and ordinal logistic regression, multinomial and Poisson regression, and repeated-measures ANOVA, and a new visual tool to identify the right statistical test for a given problem and data set. The amount of code available from the companion website has doubled in size, providing much supplementary material on statistical tests and advanced plotting.{$<$}/p{$>$}},
  isbn = {978-3-11-030747-4},
  langid = {english},
  file = {/media/cloud/Zotero/storage/LSGXBUTT/gries_2013_statistics_for_linguistics.pdf;/media/cloud/Zotero/storage/ELBFCE9N/300775.html}
}

@book{gries2016,
  title = {Quantitative {{Corpus Linguistics}} with {{R}}: {{A Practical Introduction}}},
  shorttitle = {Quantitative {{Corpus Linguistics}} with {{R}}},
  author = {Gries, Stefan Th},
  date = {2016-10-25},
  edition = {2},
  publisher = {{Routledge}},
  location = {{New York}},
  doi = {10.4324/9781315746210},
  abstract = {As in its first edition, the new edition of Quantitative Corpus Linguistics with R demonstrates how to process corpus-linguistic data with the open-source programming language and environment R. Geared in general towards linguists working with observational data, and particularly corpus linguists, it introduces R programming with emphasis on: data processing and manipulation in general; text processing with and without regular expressions of large bodies of textual and/or literary data, and; basic aspects of statistical analysis and visualization. This book is extremely hands-on and leads the reader through dozens of small applications as well as larger case studies. Along with an array of exercise boxes and separate answer keys, the text features a didactic sequential approach in case studies by way of subsections that zoom in to every programming problem. The companion website to the book contains all relevant R code (amounting to approximately 7,000 lines of heavily commented code), most of the data sets as well as pointers to others, and a dedicated Google newsgroup. This new edition is ideal for both researchers in corpus linguistics and instructors who want to promote hands-on approaches to data in corpus linguistics courses.},
  isbn = {978-1-315-74621-0},
  pagetotal = {286},
  keywords = {GELEZEN},
  file = {/media/cloud/Zotero/storage/BJG3JZQB/Gries_2016_Quantitative Corpus Linguistics with R.pdf}
}

@article{gries2021,
  title = {A New Approach to (Key) Keywords Analysis: {{Using}} Frequency, and Now Also Dispersion},
  shorttitle = {A New Approach to (Key) Keywords Analysis},
  author = {Gries, Stefan Th},
  date = {2021-01-22},
  journaltitle = {Research in Corpus Linguistics},
  volume = {9},
  number = {2},
  pages = {1--33},
  issn = {2243-4712},
  doi = {10.32714/ricl.09.02.02},
  url = {https://ricl.aelinco.es/index.php/ricl/article/view/150},
  urldate = {2023-04-11},
  abstract = {A widely-used method in corpus-linguistic approaches to discourse analysis, register/text type/genre analysis, and educational/curriculum questions is that of keywords analysis, a simple statistical method aiming to identify words that are key to, i.e. characteristic for, certain discourses, text types, or topic domains. The vast majority of keywords analyses relied on the same statistical measure that most collocation studies are using, the log-likelihood ratio, which is performed on frequencies of occurrence in two corpora under consideration. In a recent paper, Egbert and Biber (2019) advocated a different approach, one that involves computing log-likelihood ratios for word types based on the range of their distribution rather than their frequencies in the target and reference corpora under consideration. In this paper, I argue that their approach is a most welcome addition to keywords analysis but can still be profitably extended by utilizing both frequency and dispersion for keyness computations. I am presenting a new two-dimensional approach to keyness and exemplifying it on the basis of the Clinton-Trump Corpus and the British National Corpus.},
  issue = {2},
  langid = {english},
  keywords = {association,British National Corpus,Clinton-Trump Corpus,dispersion,frequency,keyness},
  file = {/media/cloud/Zotero/storage/KSLNHEK7/Gries_2021_A new approach to (key) keywords analysis.pdf}
}

@book{grolemund2017,
  title = {R for {{Data Science}}},
  author = {Grolemund, Garrett and Wickham, Hadley},
  date = {2017},
  publisher = {{O'Reilly}},
  url = {https://r4ds.had.co.nz/},
  urldate = {2020-08-18},
  abstract = {This book will teach you how to do data science with R: You’ll learn how to get your data into R, get it into the most useful structure, transform it, visualise it and model it. In this book, you will find a practicum of skills for data science. Just as a chemist learns how to clean test tubes and stock a lab, you’ll learn how to clean data and draw plots—and many other things besides. These are the skills that allow data science to happen, and here you will find the best practices for doing each of these things with R. You’ll learn how to use the grammar of graphics, literate programming, and reproducible research to save time. You’ll also learn how to manage cognitive resources to facilitate discoveries when wrangling, visualising, and exploring data.},
  keywords = {GELEZEN,Important},
  file = {/media/cloud/Zotero/storage/3TSDWG94/r4ds.had.co.nz.html}
}

@book{grozinger2008b,
  title = {Homiletik. {{Lehrbuch Praktische Theologie}}},
  shorttitle = {Homiletik},
  author = {Grözinger, Albrecht},
  date = {2008},
  publisher = {{Gütersloher Verlagshaus}},
  location = {{Gütersloh}},
  file = {/media/cloud/Zotero/storage/RQNT6XDT/Grözinger_2008_Homiletik.pdf}
}

@incollection{hardie2011,
  title = {What Is Corpus Linguistics?},
  booktitle = {Corpus {{Linguistics}}: {{Method}}, {{Theory}} and {{Practice}}},
  editor = {Hardie, Andrew and McEnery, Tony},
  date = {2011},
  series = {Cambridge {{Textbooks}} in {{Linguistics}}},
  pages = {1--24},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge}},
  doi = {10.1017/CBO9780511981395.002},
  url = {https://www.cambridge.org/core/books/corpus-linguistics/what-is-corpus-linguistics/C16393FB65F2BA9D7C7EFF9284F99EAE},
  urldate = {2023-03-13},
  abstract = {IntroductionWhat is corpus linguistics? It is certainly quite distinct from most other topics you might study in linguistics, as it is not directly about the study of any particular aspect of language. Rather, it is an area which focuses upon a set of procedures, or methods, for studying language (although, as we will see, at least one major school of corpus linguists does not agree with the characterisation of corpus linguistics as a methodology). The procedures themselves are still developing, and remain an unclearly delineated set – though some of them, such as concordancing, are well established and are viewed as central to the approach. Given these procedures, we can take a corpus-based approach to many areas of linguistics. Yet precisely because of this, as this book will show, corpus linguistics has the potential to reorient our entire approach to the study of language. It may refine and redefine a range of theories of language. It may also enable us to use theories of language which were at best difficult to explore prior to the development of corpora of suitable size and machines of sufficient power to exploit them. Importantly, the development of corpus linguistics has also spawned, or at least facilitated the exploration of, new theories of language – theories which draw their inspiration from attested language use and the findings drawn from it. In this book, these impacts of corpus linguistics will be introduced, explored and evaluated.Before exploring the impact of corpora on linguistics in general, however, let us return to the observation that corpus linguistics focuses upon a group of methods for studying language. This is an important observation, but needs to be qualified. Corpus linguistics is not a monolithic, consensually agreed set of methods and procedures for the exploration of language. While some generalisations can be made that characterise much of what is called ‘corpus linguistics’, it is very important to realise that corpus linguistics is a heterogeneous field. Differences exist within corpus linguistics which separate out and subcategorise varying approaches to the use of corpus data. But let us first deal with the generalisations. We could reasonably define corpus linguistics as dealing with some set of machine-readable texts which is deemed an appropriate basis on which to study a specific set of research questions. The set of texts or corpus dealt with is usually of a size which defies analysis by hand and eye alone within any reasonable timeframe. It is the large scale of the data used that explains the use of machine-readable text. Unless we use a computer to read, search and manipulate the data, working with extremely large datasets is not feasible because of the time it would take a human analyst, or team of analysts, to search through the text. It is certainly extremely difficult to search such a large corpus by hand in a way which guarantees no error. The next generalisation follows from this observation: corpora are invariably exploited using tools which allow users to search through them rapidly and reliably. Some of these tools, namely concordancers, allow users to look at words in context. Most such tools also allow the production of frequency data of some description, for example a word frequency list, which lists all words appearing in a corpus and specifies for each word how many times it occurs in that corpus. Concordances and frequency data exemplify respectively the two forms of analysis, namely qualitative and quantitative, that are equally important to corpus linguistics.},
  isbn = {978-0-521-83851-1},
  file = {/media/cloud/Zotero/storage/J9KKJSM2/2011_What is corpus linguistics.pdf;/media/cloud/Zotero/storage/HITW362P/C16393FB65F2BA9D7C7EFF9284F99EAE.html}
}

@article{harris2018a,
  title = {An Annotation Scheme for {{Rhetorical Figures}}},
  author = {Harris, Randy Allen and Di Marco, Chrysanne and Ruan, Sebastian and O’Reilly, Cliff},
  date = {2018-01-01},
  journaltitle = {Argument \& Computation},
  volume = {9},
  number = {2},
  pages = {155--175},
  publisher = {{IOS Press}},
  issn = {1946-2166},
  doi = {10.3233/AAC-180037},
  url = {https://content.iospress.com/articles/argument-and-computation/aac037},
  urldate = {2023-07-17},
  abstract = {There is a driving need computationally to interrogate large bodies of text for a range of non-denotative meaning (e.g., to plot chains of reasoning, detect sentiment, diagnose genre, and so forth). But such meaning has always proven computationally},
  langid = {english},
  file = {/media/cloud/Zotero/storage/5LCD6GKV/Harris et al_2018_An annotation scheme for Rhetorical Figures.pdf}
}

@article{held2014,
  title = {Conscience, {{Voice}}, and {{Presence}}: {{Newman}}'s "{{University Sermons}}" and {{Victorian Platform Culture}}},
  shorttitle = {Conscience, {{Voice}}, and {{Presence}}},
  author = {Held, Joshua R.},
  date = {2014},
  journaltitle = {Victorian Review},
  volume = {40},
  number = {1},
  eprint = {24497046},
  eprinttype = {jstor},
  pages = {211--231},
  publisher = {{Victorian Studies Association of Western Canada}},
  issn = {0848-1512},
  url = {https://www.jstor.org/stable/24497046},
  urldate = {2023-10-24},
  file = {/media/cloud/Zotero/storage/MED8BSH9/Held_2014_Conscience, Voice, and Presence.pdf}
}

@incollection{helles2020,
  title = {Digital Methods for Media and Communication Research},
  booktitle = {A {{Handbook}} of {{Media}} and {{Communication Research}}},
  author = {Helles, Rasmus},
  editor = {Jensen, Klaus Bruhn},
  date = {2020-12-29},
  edition = {3},
  pages = {307--327},
  publisher = {{Routledge}},
  location = {{Third Edition. | New York: Routledge, 2021. | Revised edition of The handbook of media and communication research, 2012.}},
  doi = {10.4324/9781138492905-23},
  url = {https://www.taylorfrancis.com/books/9781351029377/chapters/10.4324/9781138492905-23},
  urldate = {2022-12-16},
  isbn = {978-1-138-49290-5},
  langid = {english},
  file = {/media/cloud/Zotero/storage/T8ZZJYZS/Helles - 2020 - Digital methods for media and communication resear.pdf}
}

@book{hocking2022,
  title = {The {{Impact}} of {{Everyday Language Change}} on the {{Practices}} of {{Visual Artists}}},
  author = {Hocking, Darryl},
  date = {2022-05-31},
  edition = {1},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/9781108909693},
  url = {https://www.cambridge.org/core/product/identifier/9781108909693/type/element},
  urldate = {2023-03-30},
  abstract = {The practices of visual artists can never be decontextualised from language. First, artists are constantly in dialogue with their peers, dealers, critics and audiences about their creative activities and these interactions impact on the work they produce. Second, artists’ conceptualisations of what artistic practice encompasses are always shaped by wider social discourses. These discourses, however, and their manifestation in the language of everyday life are subject to continual change and potentially reshape the way that artists conceptualise their practices. Using a 235,000-word diachronic corpus developed from artists’ interviews and statements, this Element investigates shifts in artists’ use of language to conceptualise their art practice from 1950 to 2019. It then compares these shifts to see if they align with changes in the wider English lexicon and whether there might be a relationship between everyday language change and the aesthetic and conceptual developments that take place in the art world.},
  isbn = {978-1-108-90969-3 978-1-108-84250-1 978-1-00-922573-1},
  langid = {english},
  file = {/media/cloud/Zotero/storage/J3GHF62B/Hocking - 2022 - The Impact of Everyday Language Change on the Prac.pdf}
}

@book{homiletischenarbeitsgruppe1973,
  title = {Die Predigt bei Taufe, Trauung und Begräbnis. Inhalt, Wirkung und Funktion. Eine Contentanalyse},
  shorttitle = {Die Predigt bei Taufe, Trauung und Begr�bnis},
  author = {{Homiletischen Arbeitsgruppe}},
  date = {1973},
  publisher = {{Kaiser}},
  location = {{München}},
  abstract = {Inhoudsanalyse van preken. Ondertitel: inhoud, werking, functie},
  langid = {ngerman}
}

@article{hussein2018,
  title = {A Survey on Sentiment Analysis Challenges},
  author = {Hussein, Doaa Mohey El-Din Mohamed},
  date = {2018-10-01},
  journaltitle = {Journal of King Saud University - Engineering Sciences},
  shortjournal = {Journal of King Saud University - Engineering Sciences},
  volume = {30},
  number = {4},
  pages = {330--338},
  issn = {1018-3639},
  doi = {10.1016/j.jksues.2016.04.002},
  url = {https://www.sciencedirect.com/science/article/pii/S1018363916300071},
  urldate = {2022-02-17},
  abstract = {With accelerated evolution of the internet as websites, social networks, blogs, online portals, reviews, opinions, recommendations, ratings, and feedback are generated by writers. This writer generated sentiment content can be about books, people, hotels, products, research, events, etc. These sentiments become very beneficial for businesses, governments, and individuals. While this content is meant to be useful, a bulk of this writer generated content require using the text mining techniques and sentiment analysis. But there are several challenges facing the sentiment analysis and evaluation process. These challenges become obstacles in analyzing the accurate meaning of sentiments and detecting the suitable sentiment polarity. Sentiment analysis is the practice of applying natural language processing and text analysis techniques to identify and extract subjective information from text. This paper presents a survey on the sentiment analysis challenges relevant to their approaches and techniques.},
  langid = {english},
  keywords = {Accuracy,Review structure,Sentiment analysis,Sentiment analysis challenges,Sentiments,Text analysis},
  file = {/media/cloud/Zotero/storage/295WZGBB/Hussein_2018_A survey on sentiment analysis challenges.pdf;/media/cloud/Zotero/storage/7HTWE2FI/S1018363916300071.html}
}

@inbook{hutchings2015,
  title = {Digital {{Humanities}} and the {{Study}} of {{Religion}}},
  booktitle = {Between {{Humanities}} and the {{Digital}}},
  author = {Hutchings, Tim},
  date = {2015},
  pages = {283--294},
  bookauthor = {Svensson, Patrik and Goldberg, David Theo},
  langid = {english},
  keywords = {NOG-LEZEN},
  file = {/media/cloud/Zotero/storage/VHJC4BID/Hutchings - Introduction Religion and the Digital Humanities.pdf}
}

@book{hutchings2021,
  title = {Digital {{Humanities}} and {{Christianity}}: {{An Introduction}}},
  shorttitle = {Volume 4 {{Digital Humanities}} and {{Christianity}}},
  author = {Hutchings, Tim and Clivaz, Claire},
  date = {2021-09-20},
  journaltitle = {Volume 4 Digital Humanities and Christianity},
  volume = {Volume 4},
  publisher = {{De Gruyter}},
  doi = {10.1515/9783110574043},
  url = {https://www.degruyter.com/document/doi/10.1515/9783110574043/html},
  urldate = {2021-09-15},
  abstract = {This volume provides the first comprehensive introduction to the intersections between Christianity and the digital humanities. DH is a well-established, fast-growing, multidisciplinary field producing computational applications and analytical models to enable new kinds of research. Scholars of Christianity were among the first pioneers to explore these possibilities, using digital approaches to transform the study of Christian texts, history and ideas, and innovative work is taking place today all over the world. This volume aims to celebrate and continue that legacy by bringing together 15 of the most exciting contemporary projects, grouped into four categories. “Canon, corpus and manuscript” examines physical texts and collections. “Words and meanings” explores digital approaches to language and linguistics. “Digital history” uses digital techniques to explore the Christian past, and “Theology and pedagogy” engages with digital approaches to teaching, formation and Christian ideas. This volume introduces key debates, shares exciting initiatives, and aims to encourage new innovations in analysis and communication. Christianity and the Digital Humanities is ideally suited as a starting point for students and researchers interested in this vast and complex field.},
  isbn = {978-3-11-057404-3},
  langid = {english},
  keywords = {Bible,Christianity,Digital humanities,Pedagogy},
  file = {/media/cloud/Zotero/storage/2MFVDPZ7/10.1515_9783110574043-toc.pdf;/media/cloud/Zotero/storage/7DHHGBYE/Hutchings_Clivaz_2021_Digital Humanities and Christianity.pdf;/media/cloud/Zotero/storage/9C388HKB/10.1515_9783110574043-003.pdf;/media/cloud/Zotero/storage/SV79Z62B/10.1515_9783110574043-002.pdf;/media/cloud/Zotero/storage/ZCEXLIAG/10.1515_9783110574043-011.pdf}
}

@inbook{inaba2019,
  title = {Grounded {{Text Mining Approach}}: {{A Synergy}} between {{Grounded Theory}} and {{Text Mining Approaches}}},
  shorttitle = {Grounded {{Text Mining Approach}}},
  booktitle = {The {{SAGE Handbook}} of {{Current Developments}} in {{Grounded Theory}}},
  author = {Inaba, Mitsuyuki and Kakai, Hisako},
  date = {2019},
  pages = {332--351},
  publisher = {{SAGE Publications Ltd}},
  location = {{1 Oliver's Yard,~55 City Road~London~EC1Y 1SP}},
  doi = {10.4135/9781526485656.n18},
  url = {http://methods.sagepub.com/book/the-sage-handbook-of-grounded-theory-2e/i2646.xml},
  urldate = {2022-02-11},
  bookauthor = {Bryant, Antony and Charmaz, Kathy},
  isbn = {978-1-4739-7095-3 978-1-5264-8565-6},
  file = {/media/cloud/Zotero/storage/5F3P4U5I/Inaba_Kakai_2019_Grounded Text Mining Approach.pdf}
}

@article{isasi2021,
  title = {Sentiment {{Analysis}} with 'syuzhet' Using {{R}}},
  author = {Isasi, Jennifer},
  date = {2021-03-23},
  journaltitle = {Programming Historian},
  url = {https://programminghistorian.org/en/lessons/sentiment-analysis-syuzhet},
  urldate = {2023-04-15},
  langid = {english},
  file = {/media/cloud/Zotero/storage/FV6HW3LS/sentiment-analysis-syuzhet.html}
}

@article{jacobs2019a,
  title = {Topic Models Meet Discourse Analysis: A Quantitative Tool for a Qualitative Approach},
  shorttitle = {Topic Models Meet Discourse Analysis},
  author = {Jacobs, Thomas and Tschötschel, Robin},
  date = {2019-09-03},
  journaltitle = {International Journal of Social Research Methodology},
  shortjournal = {International Journal of Social Research Methodology},
  volume = {22},
  number = {5},
  pages = {469--485},
  issn = {1364-5579, 1464-5300},
  doi = {10.1080/13645579.2019.1576317},
  url = {https://www.tandfonline.com/doi/full/10.1080/13645579.2019.1576317},
  urldate = {2023-04-15},
  abstract = {Quantitative text analysis tools have become increasingly popular methods for the operationalization of various types of discourse analysis. However, their application usually remains fairly simple and superficial, and fails to exploit the resources which the digital era holds for discourse analysis to their full extent. This paper discusses the discourse-analytic potential of a more complex and advanced text analysis tool, which is already frequently employed in other approaches to textual analysis, notably topic modelling. We argue that topic modelling promises advances in areas where discourse analysis has traditionally struggled, such as scaling, repetition, and systematization, which go beyond the contributions of simpler frequency and collocation counts. At the same time, it does not violate the epistemological premises and methodological ethos of even the more radical theories of discourse, we will demonstrate. Finally, we present two small case studies to show how topic modelling – when used with appropriate parameters – can straightforwardly enhance our ability to systematically investigate and interpret discourses in large collections of text.},
  langid = {english},
  file = {/media/cloud/Zotero/storage/2HJK2VP7/Jacobs en Tschötschel - 2019 - Topic models meet discourse analysis a quantitati.pdf}
}

@article{jarvis2013,
  title = {Capturing the {{Diversity}} in {{Lexical Diversity}}},
  author = {Jarvis, Scott},
  date = {2013},
  journaltitle = {Language Learning},
  volume = {63},
  number = {s1},
  pages = {87--106},
  issn = {1467-9922},
  doi = {10.1111/j.1467-9922.2012.00739.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9922.2012.00739.x},
  urldate = {2023-03-25},
  abstract = {The range, variety, or diversity of words found in learners’ language use is believed to reflect the complexity of their vocabulary knowledge as well as the level of their language proficiency. Many indices of lexical diversity have been proposed, most of which involve statistical relationships between types and tokens, and which ultimately reflect the rate of word repetition. These indices have generally been validated in accordance with how well they overcome sample-size effects and/or how well they predict language knowledge or behavior, rather than in accordance with how well they actually measure the construct of lexical diversity. In this article, I review developments that have taken place in lexical diversity research, and also describe obstacles that have prevented it from advancing further. I compare these developments with parallel research on biodiversity in the field of ecology, and show what language researchers can learn from ecology regarding the modeling and measurement of diversity as a multidimensional construct of compositional complexity.},
  langid = {english},
  keywords = {biodiversity,ecological approaches,GELEZEN,human judgments,lexical measures,Shannon's index,Simpson's index,vocabulary acquisition},
  file = {/media/cloud/Zotero/storage/UM2NB75J/Jarvis_2013_Capturing the Diversity in Lexical Diversity.pdf;/media/cloud/Zotero/storage/4RK6FJPC/j.1467-9922.2012.00739.html}
}

@book{jockers2014,
  title = {Text {{Analysis}} with {{R}} for {{Students}} of {{Literature}}},
  author = {Jockers, Matthew L.},
  date = {2014},
  series = {Quantitative {{Methods}} in the {{Humanities}} and {{Social Sciences}}},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-03164-4},
  url = {http://link.springer.com/10.1007/978-3-319-03164-4},
  urldate = {2022-01-31},
  isbn = {978-3-319-03163-7 978-3-319-03164-4},
  langid = {english},
  keywords = {NU-LEZEN},
  file = {/media/cloud/Zotero/storage/JB9BMLHX/Jockers - 2014 - Text Analysis with R for Students of Literature.pdf}
}

@incollection{jockers2015,
  title = {Text-{{Mining}} the {{Humanities}}},
  booktitle = {A {{New Companion}} to {{Digital Humanities}}},
  author = {Jockers, Matthew L. and Underwood, Ted},
  date = {2015},
  pages = {291--306},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781118680605.ch20},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1002/9781118680605.ch20},
  urldate = {2020-11-14},
  abstract = {This chapter provides a broad overview of how text mining can be usefully employed in humanistic research. The chapter begins by addressing the question of why scholars in the humanities should care about text mining and what they might expect to gain by embracing what are deeply computational and deeply quantitative methods. We then offer a quick synopsis of the key watersheds in the history of text mining. The bulk of the chapter discusses central methodologies used in humanistic text mining. Using examples from the humanities, we unpack the differences between supervised and unsupervised learning and discuss how tools developed by researchers in other fields can be usefully employed to address humanistic questions. Drawing from personal experience, we address some of the significant challenges associated with data quality, metadata, and copyright restrictions before moving to a discussion of a few exemplary projects and resources for further study.},
  isbn = {978-1-118-68060-5},
  langid = {english},
  keywords = {machine learning,NU-LEZEN,supervised learning,text analysis,text mining},
  file = {/media/cloud/Zotero/storage/VHTW4UUF/jockers_2015_text-mining_the_humanities.pdf}
}

@mvbook{jurczyk2022,
  title = {The Notion of "holy" in Ancient Armenian Texts from the Fifth Century CE: a comparative approach using digital tools and methods},
  shorttitle = {The Notion of "holy" in Ancient Armenian Texts from the Fifth Century CE},
  author = {Jurczyk, Thomas and {Förderung Deutsche Forschungsgemeinschaft (DFG) (SFB 1288)}},
  date = {2022},
  series = {Digital Humanities Research , 2749-1986},
  number = {5},
  publisher = {{Bielefeld University Press}},
  location = {{Bielefeld}},
  url = {https://www.degruyter.com/isbn/9783839461815},
  urldate = {2023-04-01},
  isbn = {978-3-8394-6181-5},
  langid = {Engels},
  volumes = {1 online resource (380 pages).},
  file = {/media/cloud/Zotero/storage/4TP9A8WI/Jurczyk_Förderung Deutsche Forschungsgemeinschaft (DFG) (SFB 1288)_2022_The Notion of holy in Ancient Armenian Texts from the Fifth Century CE.pdf;/media/cloud/Zotero/storage/PDNVEUWK/j.ctv2sbm7mj.4.pdf;/media/cloud/Zotero/storage/VMN6XP65/j.ctv2sbm7mj.6.pdf}
}

@article{karcher2020,
  title = {Praktische Theologie und Digital Humanities},
  author = {Karcher, Stefan},
  date = {2020-08-01},
  journaltitle = {Verkündigung und Forschung},
  volume = {65},
  number = {2},
  pages = {132--142},
  publisher = {{Gütersloher Verlagshaus}},
  issn = {2198-0454},
  doi = {10.14315/vf-2020-650209},
  url = {https://www.degruyter.com/document/doi/10.14315/vf-2020-650209/html},
  urldate = {2021-03-04},
  abstract = {Article Praktische Theologie und Digital Humanities was published on August 1, 2020 in the journal Verkündigung und Forschung (volume 65, issue 2).},
  langid = {ngerman},
  keywords = {GELEZEN},
  file = {/media/cloud/Zotero/storage/JX5ZA53J/Karcher_2020_Praktische Theologie und Digital Humanities.pdf;/media/cloud/Zotero/storage/2HSZP7SP/html.html}
}

@online{karcher2020a,
  type = {Billet},
  title = {“Predigthilfen aus dem Rechner”* 2.0? Digital Humanities und Homiletik},
  shorttitle = {“Predigthilfen aus dem Rechner”* 2.0?},
  author = {Karcher, Stefan},
  date = {2020-11-24},
  url = {https://theolab.hypotheses.org/610},
  urldate = {2023-04-08},
  abstract = {Inzwischen ist es gut 2 Jahre her, dass meine Kollegin Dr. Ines Rehbein (Institut für Computerlinguistik) und ich das Experiment gewagt haben, mit Student:innen der Theologie und der Computerlinguistik eine gemeinsame Veranstaltung an der Universität Heidelberg anzubieten. Es sollte darum gehen, dass die Studierenden der Computerlinguistik ihre Methoden an theologischen Texten einüben und erproben konnten, die Theologiestudierenden sollten sich hingegen eher experimentell und explorativ mit Tools und Methoden der computergestützten Korpuslinguistik auseinandersetzen. Für mich war das der Einstieg in einen Arbeitsbereich, den wir selbstbewusst „Computationelle Homiletik“ genannt haben, um damit unsere Herangehensweise an eine computergestützte Predigtanalyse zu beschreiben. Anmerkung: In der Community der Digital Humanities werden zurzeit intensive Diskussionen über Begriffsverwendungen und Definitionen geführt, an denen ich zum Teil – besonders beim Begriff „computationell“ – auch beteiligt bin. Diese Diskussionen sind mir daher bekannt. Mit dem Begriff „Computationelle Homiletik“ möchte ich hier noch keine besondere „Schule“ oder „Denkrichtung“ vertreten; wir haben metaphorisch damit ‘dem Kind nur einen eingängigen Namen gegeben’, um auszudrücken, dass wir mit computergestützten Methoden, die zum Teil auch Coding-Kenntnisse erfordern, Predigten analysieren. Die Begriffsdefinition führen wir an anderer Stelle weiter. Wie alle Arten von Reden eignen sich Predigten, um damit sämtliche statistischen Methoden der digitalen Textanalysen auszuprobieren und Textmining zu betreiben. Voraussetzung dafür sind entsprechende Tools oder Kenntnisse im Natural Language Processing (NLP). Im weitesten Sinne befinden wir uns damit in den Feldern der Korpuslinguistik und Meta-Daten-Analyse. In der Homiletik führen diese Bereiche allerdings zu zwei Problemen: Ein rein (korpus-)linguistischer Ansatz wurde bisher in der Homiletik kaum verfolgt, sodass es in diesem Bereich fast keine Theoriemodelle gibt, um rein linguistische Analysen mit einem homiletischen Erkenntnisinteresse zu verbinden. Birte Platow hat vor einigen Jahren mit ihrer Analyse des ‘Wortes zum Sonntag’ eine erste Untersuchung vorgelegt. Für die Predigtanalyse stelle ich daher in Frage, ob rein syntaktische und semantische Beobachtungen auf der Textebene überhaupt weiterführende Erkenntnisse in der homiletischen Praxis und Theoriebildung erbringen. Zudem ist ein ausschließlich semantisch-explorativer Ansatz keine große Herausforderung in den Digital Humanities. Es ist daher zu bezweifeln, dass sich mit einem korpuslinguistischen Ansatz und lediglich der Toolanwendung homiletische Erkenntnisse gewinnen lassen, die über die Beschreibung von Textphänomenen hinausgehen. Liegt das Interesse der Homiletik darin, Charakteristika des Textes zu benennen, die ihn als „Kommunikation des Evangeliums“ kennzeichnen, eignet sich die Korpuslinguistik allein nicht dafür, diese zu identifizieren. Eine Predigtanalyse mit einem klassisch kommunikationswissenschaftlichen Ansatz würde nämlich nach Merkmalen suchen, die nicht direkt aus dem Text zu erschließen sind, neben dem Inhalt nämlich nach Autor, Hörer:innen/Leser:innen, Medien und Effekt/Wirkung. In diesem Sinne hat die Homiletik also ein kommunikationswissenschaftliches Erkenntnisinteresse, für das eine computergestützte Analyse von Texten kaum zu gebrauchen ist. Trotz dieser Probleme muss für eine computergestützte Predigtanalyse bzw. computationelle Homiletik ein hoher Anspruch gelten, der sowohl dem eigenen Fach als auch zentralen Themen und Möglichkeiten der DH gerecht wird. Dieser Anspruch führt zu folgender Definition: Eine computationelle Homiletik sucht in großen Textkorpora nach sprachlichen (Text-)Phänomenen, die für eine homiletische Fragestellung eine argumentative Beweiskraft besitzen, und fasst die Homiletik als eine praktisch-theologische Kommunikationswissenschaft auf. Geht man bei...},
  langid = {ngerman},
  organization = {{TheoLab}},
  file = {/media/cloud/Zotero/storage/GL5DXS22/610.html}
}

@thesis{keller2018,
  type = {phdthesis},
  title = {Experiencing {{God}} in {{Words}}: {{Rhetoric}}, {{Logic}}, {{Imaginative Language}}, and {{Emotion}} in {{Jonathan Edwards}}’ {{Sermons}}: {{A Computational Analysis}}},
  shorttitle = {Experiencing {{God}} in {{Words}}},
  author = {Keller, M.S.},
  date = {2018},
  institution = {{VU}},
  location = {{Amsterdam}},
  keywords = {Jonathan Edwards,NOG-LEZEN},
  file = {/media/cloud/Zotero/storage/XY5MR4LQ/Keller_2018_Experiencing God in Words.pdf}
}

@article{khurana2023,
  title = {Natural Language Processing: State of the Art, Current Trends and Challenges},
  shorttitle = {Natural Language Processing},
  author = {Khurana, Diksha and Koli, Aditya and Khatter, Kiran and Singh, Sukhdev},
  date = {2023-01-01},
  journaltitle = {Multimedia Tools and Applications},
  shortjournal = {Multimed Tools Appl},
  volume = {82},
  number = {3},
  pages = {3713--3744},
  issn = {1573-7721},
  doi = {10.1007/s11042-022-13428-4},
  url = {https://doi.org/10.1007/s11042-022-13428-4},
  urldate = {2023-11-29},
  abstract = {Natural language processing (NLP) has recently gained much attention for representing and analyzing human language computationally. It has spread its applications in various fields such as machine translation, email spam detection, information extraction, summarization, medical, and question answering etc. In this paper, we first distinguish four phases by discussing different levels of NLP and components of Natural Language Generation followed by presenting the history and evolution of NLP. We then discuss in detail the state of the art presenting the various applications of NLP, current trends, and challenges. Finally, we present a discussion on some available datasets, models, and evaluation metrics in NLP.},
  langid = {english},
  keywords = {Natural language generation,Natural language processing,Natural language understanding,NLP applications,NLP evaluation metrics},
  file = {/media/cloud/Zotero/storage/WXC68LV7/Khurana et al_2023_Natural language processing.pdf}
}

@article{kim2020,
  title = {Dimensions of {{Religion}} and {{Spirituality}}: {{A Longitudinal Topic Modeling Approach}}},
  shorttitle = {Dimensions of {{Religion}} and {{Spirituality}}},
  author = {Kim, Seong-Hyeon and King, Pamela Ebstyne and Lee, Narae},
  date = {2020-03},
  journaltitle = {Journal for the Scientific Study of Religion},
  volume = {59},
  number = {1},
  pages = {62--83},
  issn = {0021-8294},
  url = {https://search.ebscohost.com/login.aspx?direct=true&db=rfh&AN=ATLAiFZU200331001411&site=ehost-live},
  urldate = {2023-04-17},
  keywords = {NOG-LEZEN,Religion and sociology --   Methodology,Spirituality,Transcendence,Typology (Sociology)},
  file = {/media/cloud/Zotero/storage/CXXZU6DA/Kim et al_2020_Dimensions of Religion and Spirituality.pdf}
}

@incollection{koester2022,
  title = {Building Small Specialised Corpora},
  booktitle = {The {{Routledge Handbook}} of {{Corpus Linguistics}}},
  author = {Koester, Almut},
  date = {2022},
  edition = {2},
  pages = {48--61},
  publisher = {{Routledge}},
  abstract = {The chapter focuses on computer-based language corpora – collections of texts that can be searched using special software – that are relatively small (compared to much larger such collections) and are specialised in some way, for example, in terms of the setting, text type, genre or topic. It begins by defining “a small corpus” and reviewing the different ways in which a corpus can be specialised with reference to a variety of existing corpora. Next, important considerations in designing and compiling a small, specialised corpus are discussed, in particular, how representativity and balance can be achieved for such smaller corpora. The final section looks at what can be learnt from small specialised corpora, illustrating this with examples from a range of corpus studies. The chapter argues that small, specialised corpora can have advantages over larger corpora due to the availability and retrievability of contextual features, which can be linked to specific language patterns.},
  pagetotal = {14},
  file = {/media/cloud/Zotero/storage/5992BSNG/Koester_2022_Building small specialised corpora.pdf}
}

@article{kuczok2010,
  title = {Conceptual {{Metaphors}} for the {{Notion}} of {{Christian Life}} in {{John Henry Newman}}’s {{Parochial}} and {{Plain Sermons}}},
  author = {Kuczok, Marcin},
  date = {2010},
  journaltitle = {Newman Studies Journal},
  volume = {7},
  number = {2},
  pages = {29--40},
  publisher = {{The Catholic University of America Press}},
  issn = {2153-6945},
  doi = {10.1353/nsj.2010.0010},
  url = {https://muse.jhu.edu/pub/16/article/735990},
  urldate = {2023-07-17},
  abstract = {From the perspective of cognitive linguistics, metaphor is a way of thinking and understanding rather than an ornamental device used for aesthetic purposes. Conceptual metaphor constitutes a natural device for comprehending those areas of reality that exceed what is describable by literal terms, including especially the sphere of religious experiences. The purpose of this essay is to analyze the conceptual metaphors employed by John Henry Newman in the first volume of his Parochial and Plain Sermons (1834) as a way of explaining the transcendental character of the concept of Christian life.}
}

@article{kuczok2014,
  title = {An Individual Realisation of the Linguistic Genre of Sermon in {{John Henry Newman}}'s {{Parochial}} and {{Plain Sermons}}},
  author = {Kuczok, Marcin},
  date = {2014-01-01},
  journaltitle = {Śląskie Studia Historyczno-Teologiczne 47/1 (2014), pp. 144-158},
  url = {https://www.academia.edu/6934168/An_individual_realisation_of_the_linguistic_genre_of_sermon_in_John_Henry_Newmans_Parochial_and_Plain_Sermons},
  urldate = {2022-05-28},
  abstract = {An individual realisation of the linguistic genre of sermon in John Henry Newman\&\#39;s Parochial and Plain Sermons},
  langid = {english},
  file = {/media/cloud/Zotero/storage/JG2M4JUT/Kuczok_An_individual_realisation.pdf;/media/cloud/Zotero/storage/7H5V7P6M/An_individual_realisation_of_the_linguistic_genre_of_sermon_in_John_Henry_Newmans_Parochial_and.html}
}

@book{kuczok2014a,
  title = {The {{Conceptualisation}} of the {{Christian Life}} in {{John Henry Newman}}'s {{Parochial}} and {{Plain Sermons}}},
  author = {Kuczok, Marcin},
  date = {2014},
  publisher = {{Cambridge Scholars Publishing}},
  location = {{Newcastle upon Tyne}},
  url = {https://www.academia.edu/6120489/The_Conceptualisation_of_the_Christian_Life_in_John_Henry_Newmans_Parochial_and_Plain_Sermons},
  urldate = {2022-05-28},
  abstract = {This book analyses the conceptual mechanisms behind the notion of “The Christian Life” in the collection of sermons preached by John Henry Newman (1801–1890) and published in eight volumes as Parochial and Plain Sermons (1834–1843). The study},
  langid = {english},
  file = {/media/cloud/Zotero/storage/3AR79EY4/The_Conceptualisation_of_the_Christian_Life_in_Joh..._----_(CHAPTER_TWO).pdf;/media/cloud/Zotero/storage/6ALRUU36/The_Conceptualisation_of_the_Christian_Life_in_Joh..._----_(CHAPTER_FIVE).pdf;/media/cloud/Zotero/storage/7CG98RCT/Kuczok_2014_The Conceptualisation of the Christian Life in John Henry Newman&#39\;s.pdf;/media/cloud/Zotero/storage/7DJAEZS9/The_Conceptualisation_of_the_Christian_Life_in_Joh..._----_(Pg_2--32).pdf;/media/cloud/Zotero/storage/942DJ48X/The_Conceptualisation_of_the_Christian_Life_in_Joh..._----_(FINAL_CONCLUSIONS).pdf;/media/cloud/Zotero/storage/GGEAW2SI/The_Conceptualisation_of_the_Christian_Life_in_Joh..._----_(CHAPTER_FOUR).pdf;/media/cloud/Zotero/storage/KKTECM3G/The_Conceptualisation_of_the_Christian_Life_in_Joh..._----_(PART_I).pdf;/media/cloud/Zotero/storage/YJHQD9ND/The_Conceptualisation_of_the_Christian_Life_in_Joh..._----_(CHAPTER_THREE).pdf;/media/cloud/Zotero/storage/ZTM4IAG8/The_Conceptualisation_of_the_Christian_Life_in_Joh..._----_(CHAPTER_SEVEN).pdf}
}

@book{kwartler2017,
  title = {Text {{Mining}} in {{Practice}} with {{R}}},
  author = {Kwartler, Ted},
  date = {2017-07-31},
  publisher = {{John Wiley \& Sons, Ltd}},
  location = {{Chichester, UK}},
  doi = {10.1002/9781119282105},
  url = {http://doi.wiley.com/10.1002/9781119282105},
  urldate = {2022-02-22},
  isbn = {978-1-119-28210-5 978-1-119-28201-3},
  langid = {english},
  file = {/media/cloud/Zotero/storage/723UJCIP/Kwartler - 2017 - Text Mining in Practice with R.pdf}
}

@online{lab,
  title = {Methods {{Bites}}},
  author = {Lab, MZES Social Science Data},
  url = {https://www.mzes.uni-mannheim.de/socialsciencedatalab},
  urldate = {2022-02-01},
  abstract = {Blog of the MZES Social Science Data Lab},
  langid = {english},
  organization = {{Methods Bites}},
  file = {/media/cloud/Zotero/storage/HDGJXURN/advancing-text-mining.html}
}

@article{labille2017,
  title = {Creating {{Domain-Specific Sentiment Lexicons}} via {{Text Mining}}},
  author = {Labille, Kevin and Gauch, Susan and Alfarhood, Sultan},
  date = {2017},
  pages = {8},
  abstract = {Sentiment analysis aims to identify and categorize customer’s opinion and judgments using either traditional supervised learning techniques or unsupervised approaches. Traditionally, Sentiment Analysis is performed using machine learning techniques such as a naive Bayes classification or support vector machines (SVM), or could make use of a sentiment lexicon, that is, a list of words that are mapped to a sentiment score. Our work focuses on generating a domain-specific lexicon using probabilities and information theoretic techniques. By employing text mining, we overcome the poor performance of transferred supervised machine learning techniques and remove the need to adapt an existing lexicon while maintaining accuracy. We show that text mining techniques performs as well as traditional approaches and we demonstrate that domain specific lexicons perform better than general lexicons in a sentiment analysis task. We further review and compare the generated lexicons.},
  langid = {english},
  file = {/media/cloud/Zotero/storage/ASFBCAX5/Labille e.a. - 2017 - Creating Domain-Specific Sentiment Lexicons via Te.pdf}
}

@article{lauro1996,
  title = {Computational Statistics or Statistical Computing, Is That the Question?},
  author = {Lauro, Carlo},
  date = {1996-11-15},
  journaltitle = {Computational Statistics \& Data Analysis},
  shortjournal = {Computational Statistics \& Data Analysis},
  series = {Classification},
  volume = {23},
  number = {1},
  pages = {191--193},
  issn = {0167-9473},
  doi = {10.1016/0167-9473(96)88920-1},
  url = {https://www.sciencedirect.com/science/article/pii/0167947396889201},
  urldate = {2023-06-08},
  langid = {english},
  file = {/media/cloud/Zotero/storage/RR5TC5D2/Lauro_1996_Computational statistics or statistical computing, is that the question.pdf;/media/cloud/Zotero/storage/4ZMLJ8YT/0167947396889201.html}
}

@article{leemans2018,
  title = {Mining {{Embodied Emotions}}: {{A Comparative Analysis}} of {{Sentiment}} and {{Emotion}} in {{Dutch Texts}}, 1600-1800.},
  shorttitle = {Mining {{Embodied Emotions}}},
  author = {Leemans, Inger and family=Zwaan, given=Janneke M., prefix=van der, useprefix=false and Maks, Isa and Kuijpers, Erika and Steenbergh, Kristine},
  date = {2018-01-08},
  journaltitle = {Digital Humanities Quarterly},
  shortjournal = {DHQ},
  volume = {011},
  number = {4},
  issn = {1938-4122},
  file = {/media/cloud/Zotero/storage/GBUKIFQW/000343.html}
}

@article{lei2021,
  title = {Conducting {{Sentiment Analysis}}},
  author = {Lei, Lei and Liu, Dilin},
  date = {2021-08},
  journaltitle = {Elements in Corpus Linguistics},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/9781108909679},
  url = {https://www-cambridge-org.proxy-ub.rug.nl/core/elements/conducting-sentiment-analysis/B00BACADE638BF1AD5F61972FEE4183D},
  urldate = {2023-03-29},
  abstract = {Cambridge Core - Applied Linguistics - Conducting Sentiment Analysis},
  isbn = {9781108909679 9781108829212},
  langid = {english},
  keywords = {NU-LEZEN},
  file = {/media/cloud/Zotero/storage/TZBJ66Z3/Lei_Liu_2021_Conducting Sentiment Analysis.pdf;/media/cloud/Zotero/storage/GGE2765H/B00BACADE638BF1AD5F61972FEE4183D.html}
}

@inproceedings{liebl2020,
  title = {“{{Shakespeare}} in the {{Vectorian Age}}” – {{An}} Evaluation of Different Word Embeddings and {{NLP}} Parameters for the Detection of {{Shakespeare}} Quotes},
  booktitle = {Proceedings of the {{The}} 4th {{Joint SIGHUM Workshop}} on {{Computational Linguistics}} for {{Cultural Heritage}}, {{Social Sciences}}, {{Humanities}} and {{Literature}}},
  author = {Liebl, Bernhard and Burghardt, Manuel},
  date = {2020-12},
  pages = {58--68},
  publisher = {{International Committee on Computational Linguistics}},
  location = {{Online}},
  url = {https://aclanthology.org/2020.latechclfl-1.7},
  urldate = {2022-03-10},
  abstract = {In this paper we describe an approach for the computer-aided identification of Shakespearean intertextuality in a corpus of contemporary fiction. We present the Vectorian, which is a framework that implements different word embeddings and various NLP parameters. The Vectorian works like a search engine, i.e. a Shakespeare phrase can be entered as a query, the underlying collection of fiction books is then searched for the phrase and the passages that are likely to contain the phrase, either verbatim or as a paraphrase, are presented in a ranked results list. While the Vectorian can be used via a GUI, in which many different parameters can be set and combined manually, in this paper we present an ablation study that automatically evaluates different embedding and NLP parameter combinations against a ground truth. We investigate the behavior of different parameters during the evaluation and discuss how our results may be used for future studies on the detection of Shakespearean intertextuality.},
  eventtitle = {{{CLFL-COLING-LaTeCH-LaTeCHCLfL}} 2020},
  file = {/media/cloud/Zotero/storage/6ZBRIM6Y/Liebl and Burghardt - 2020 - “Shakespeare in the Vectorian Age” – An evaluation.pdf}
}

@book{lischer2002b,
  title = {The {{Company}} of {{Preachers}}. {{Wisdom}} on {{Preaching}}, {{Augustine}} to the {{Present}}},
  shorttitle = {Company of {{Preachers}}},
  author = {Lischer, Richard},
  date = {2002},
  publisher = {{W.B. Eerdmans Pub. Co.}},
  location = {{Grand Rapids}}
}

@book{liu2020,
  title = {Sentiment analysis: mining opinions, sentiments, and emotions},
  shorttitle = {Sentiment analysis},
  author = {Liu, Bing},
  date = {2020},
  series = {Studies in natural language processing},
  edition = {Second edition},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge, United Kingdom}},
  url = {https://doi.org/10.1017/9781108639286},
  urldate = {2022-02-17},
  isbn = {978-1-108-63928-6},
  langid = {Engels},
  file = {/media/cloud/Zotero/storage/2TKTBMM6/aspect_and_entity_extraction.pdf;/media/cloud/Zotero/storage/3UCMZHH2/conclusion.pdf;/media/cloud/Zotero/storage/7AFZGZ5E/bibliography.pdf;/media/cloud/Zotero/storage/ECDYMCGE/sentence_subjectivity_and_sentiment_classification.pdf;/media/cloud/Zotero/storage/NPQY5S3G/sentiment_lexicon_generation.pdf;/media/cloud/Zotero/storage/NRBXG3XL/aspect_sentiment_classification.pdf;/media/cloud/Zotero/storage/P8WG29IJ/contents.pdf;/media/cloud/Zotero/storage/Q47R27US/document-sentiment-classification.pdf;/media/cloud/Zotero/storage/QBSC9RPX/problem_of_sentiment_analysis.pdf;/media/cloud/Zotero/storage/S65WY6CH/Liu_2020_Sentiment analysis.pdf;/media/cloud/Zotero/storage/T52RG7TW/appendix.pdf}
}

@article{ljubesica,
  title = {The {{LiLaH Emotion Lexicon}} of {{Croatian}}, {{Dutch}} and {{Slovene}}},
  author = {Ljubešić, Nikola and Markov, Ilia and Fišer, Darja and Daelemans, Walter},
  pages = {5},
  abstract = {In this paper, we present emotion lexicons of Croatian, Dutch and Slovene, based on manually corrected automatic translations of the English NRC Emotion lexicon. We evaluate the impact of the translation changes by measuring the change in supervised classification results of socially unacceptable utterances when lexicon information is used for feature construction. We further showcase the usage of the lexicons by calculating the difference in emotion distributions in texts containing and not containing socially unacceptable discourse, comparing them across four languages (English, Croatian, Dutch, Slovene) and two topics (migrants and LGBT). We show significant and consistent improvements in automatic classification across all languages and topics, as well as consistent (and expected) emotion distributions across all languages and topics, proving for the manually corrected lexicons to be a useful addition to the severely lacking area of emotion lexicons, the crucial resource for emotive analysis of text.},
  langid = {english},
  file = {/media/cloud/Zotero/storage/5B2XRGWF/LiLaH-HR-NL-SL.zip;/media/cloud/Zotero/storage/GKRPTV49/Ljubešić e.a. - The LiLaH Emotion Lexicon of Croatian, Dutch and S.pdf}
}

@article{lukin2023,
  title = {Adjectives and Adverbs as Stylometric Analysis Parameters},
  author = {Lukin, Eugenia and Roberts, James Cooper and Berdik, David and Mugar, Eliana and Juola, Patrick},
  date = {2023-05-22},
  journaltitle = {International Journal of Digital Humanities},
  shortjournal = {Int J Digit Humanities},
  issn = {2524-7840},
  doi = {10.1007/s42803-023-00065-y},
  url = {https://doi.org/10.1007/s42803-023-00065-y},
  urldate = {2023-06-01},
  abstract = {The present study considers the role of adjectives and adverbs in stylometric analysis and authorship attribution. Adjectives and adverbs allow both for variations in placement and order (adverbs) and variations in type (adjectives). This preliminary study examines a collection of 25 English-language blogs taken from the Schler Blog corpus, and the Project Gutenberg corpus with specific emphasis on 3 works. Within the blog corpora, the first and last 100 lines were extracted for the purpose of analysis. Project Gutenberg corpora were used in full. All texts were processed and part-of-speech tagged using the Python NLTK package. All adverbs were classified as sentence-initial, preverbal, interverbal, postverbal, sentence-final, or none-of-the-above. The adjectives were classified into types according to the universal English type hierarchy (Cambridge Dictionary Online, 2021; Annear, 1964) manually by one of the authors. Ambiguous adjectives were classified according to their context. For the adverbs, the initial samples were paired and used as training data to attribute the final samples. This resulted in 600 trials under each of five experimental conditions. We were able to attribute authorship with an average accuracy of 9.7\% greater than chance across all five conditions. Confirmatory experiments are ongoing with a larger sample of English-language blogs. This strongly suggests that adverbial placement is a useful and novel idiolectal variable for authorship attribution (Juola et al., 2021). For the adjective, differences were found in the type of adjective used by each author. Percent use of each type varied based upon individual preference and subject-matter (e.g. Moby Dick had a large number of adjectives related to size and color). While adverbial order and placement are highly variable, adjectives are subject to rigid restrictions that are not violated across texts and authors. Stylometric differences in adjective use generally involve the type and category of adjectives preferred by the author. Future investigation will focus, likewise, on whether adverbial variation is similarly analyzable by type and category of adverb.},
  langid = {english},
  keywords = {Authorship attribution,Individual language variation,Linguistic features,Stylometry,Syntax},
  file = {/media/cloud/Zotero/storage/B9UCXYUP/Lukin et al_2023_Adjectives and adverbs as stylometric analysis parameters.pdf}
}

@book{macenery2012,
  title = {Corpus linguistics: method, theory and practice},
  shorttitle = {Corpus linguistics},
  author = {MacEnery, Tony (Anthony Mark) and Hardie, Andrew},
  date = {2012},
  series = {Cambridge textbooks in linguistics},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge}},
  isbn = {978-0-521-83851-1 978-0-521-54736-9},
  langid = {Engels},
  pagetotal = {294},
  keywords = {NU-LEZEN},
  file = {/media/cloud/Zotero/storage/2UISKCBJ/11.0_pp_94_121_Corpus-based_studies_of_synchronic_and_diachronic_variation.pdf;/media/cloud/Zotero/storage/6YXXXXAQ/MacEnery_Hardie_2012_Corpus linguistics.pdf;/media/cloud/Zotero/storage/7YVSYQ3R/corpus_methods_and_functionalist_linguistics.pdf;/media/cloud/Zotero/storage/DJ25G8CR/15.0_pp_225_237_Conclusion.pdf;/media/cloud/Zotero/storage/NVZVTASQ/14.0_pp_192_224_The_convergence_of_corpus_linguistics_psycholinguistics_and_functionalist_linguistics.pdf;/media/cloud/Zotero/storage/SJECDL23/web_laws_and_ethics.pdf;/media/cloud/Zotero/storage/ZPXFXBIG/07.0_pp_1_24_What_is_corpus_linguistics.pdf}
}

@article{malmstrom2015,
  title = {Preaching in Uncertain Terms: {{The}} Place of Hedging Language in Contemporary Sermonic Discourse},
  shorttitle = {Preaching in Uncertain Terms},
  author = {Malmström, Hans},
  date = {2015-01-01},
  journaltitle = {Functions of Language},
  volume = {22},
  number = {3},
  pages = {332--361},
  publisher = {{John Benjamins}},
  issn = {0929-998X, 1569-9765},
  doi = {10.1075/fol.22.3.02mal},
  url = {https://www-jbe-platform-com.vu-nl.idm.oclc.org/content/journals/10.1075/fol.22.3.02mal},
  urldate = {2022-02-01},
  abstract = {This study investigates hedging (standardly assumed to express uncertainty, plausible reasoning and the like) in contemporary sermonic discourse as represented by sermon manuscripts from three Christian denominations in the UK. The article addresses three research questions: (i) To what extent is preaching employed as a discursive resource during preaching; (ii) What form does hedging take in sermonic discourse; and (iii) What are preachers’ rationale for hedging? The results suggest that hedging is indeed of central concern in sermonic discourse with some kind of hedging device being called upon once every 32 seconds. When preachers hedge they rely on standard and transparent linguistic expressions that typically perform this discourse function, and the repertoire includes both ‘conversational’ hedges and hedges that recall practices characteristic of written academic discourse. When preachers self-report on their rationale for hedging a multitude of different discourse functions become apparent. However, it seems that hedging is rarely used to convey lack of epistemic confidence; rather, hedging is seen as a productive interpersonal means to address one of the main objectives of contemporary, turn-to-the-listener, preaching — namely acknowledging sermon listeners as active partners in a sermonic experience.This Open Access article is available under a CC BY-NC 4.0 license.},
  langid = {english},
  file = {/media/cloud/Zotero/storage/WW8ABI7M/Malmström_2015_Preaching in uncertain terms.pdf;/media/cloud/Zotero/storage/M4DQBPEQ/fol.22.3.html}
}

@article{malmstrom2015a,
  title = {What Is Your Darkness?: {{An}} Empirical Study of Interrogative Practices in Sermonic Discourse},
  shorttitle = {What Is Your Darkness?},
  author = {Malmström, Hans},
  date = {2015},
  journaltitle = {International Journal of Practical Theology},
  shortjournal = {International Journal of Practical Theology},
  volume = {19},
  number = {2},
  pages = {247--270},
  issn = {1612-9768},
  doi = {10.1515/ijpt-2014-0019},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=rfh&AN=ATLAiG0N160711000931&site=ehost-live&scope=site},
  urldate = {2018-05-31},
  keywords = {Interdisciplinary study,Linguistics,Peer reviewed,Preaching,Questioning},
  file = {/media/cloud/Zotero/storage/Z78LAM7P/Malmström - 2015 - What is your darkness An empirical study of inte.pdf}
}

@article{malmstrom2015c,
  title = {The "{{Other}}" {{Voice}} in {{Preaching}}: {{Intertextual Form}} and {{Function}} in {{Contemporary English Sermonic Discourse}}},
  shorttitle = {The "{{Other}}" {{Voice}} in {{Preaching}}},
  author = {Malmström, Hans},
  date = {2015-22},
  journaltitle = {Journal of Communication \& Religion},
  volume = {38},
  number = {2},
  pages = {80--99},
  issn = {08942838},
  url = {https://search.ebscohost.com/login.aspx?direct=true&AuthType=sso&db=hsi&AN=114350656&lang=nl&site=ehost-live&custid=s3628809},
  urldate = {2021-12-02},
  abstract = {This study explores how and why contemporary preachers rely on intertext when preaching. The investigation, based on close reading of sermon manuscripts, semi-structured interviews with preachers, and frequency searches finds that (i) preachers use an intertextual reference approximately once every 90 seconds of preaching; (ii) intertextual sources are usually prominently foregrounded and the voice of the source frequently rendered directly rather than indirectly, suggesting that significant parts of the sermon are presented wholly from the perspective of the intertextual source; and (iii) preachers are sensitized to the multifunctionality of sermon intertext and exploit such functions in purposeful ways.},
  keywords = {DISCOURSE analysis,discourse function,ETHICS,homiletics,intertextuality,INTERTEXTUALITY,preaching,PREACHING,PUBLIC speaking,quotation,SERMON (Literary form),sermonic discourse},
  file = {/media/cloud/Zotero/storage/7IA6QN2R/Malmström_2015_The Other Voice in Preaching.pdf}
}

@unpublished{malmstrom2015d,
  type = {Academy of Homiletics},
  title = {Hedging, Boosting and Language Appeals. {{Preachers}} Exploring the Epistemic Continuum},
  author = {Malmström, Hans},
  date = {2015},
  file = {/media/cloud/Zotero/storage/YAJH9X85/Malmström_2015_Hedging, boosting and language appeals.pdf}
}

@article{malmstrom2016,
  title = {Engaging the {{Congregation}}: {{The Place}} of {{Metadiscourse}} in {{Contemporary Preaching}}},
  shorttitle = {Engaging the {{Congregation}}},
  author = {Malmström, Hans},
  date = {2016-08-01},
  journaltitle = {Applied Linguistics},
  shortjournal = {Applied Linguistics},
  volume = {37},
  number = {4},
  pages = {561--582},
  issn = {0142-6001},
  doi = {10.1093/applin/amu052},
  url = {https://doi.org/10.1093/applin/amu052},
  urldate = {2022-02-01},
  abstract = {Most forms of contemporary preaching elevate the role of the listeners, who are seen as co-constructors of the sermon. This article investigates how preachers may respond to a call from homiletics to ‘turn-to-the-listener’ through their deployment of metadiscourse, operationalized as a framework for understanding preaching not primarily as gospel proclamation, but as a form of social and communicative engagement. A quantitative analysis of 150 sermons from three well-established English Christian denominations finds that metadiscourse is a prolific feature in preaching, with almost one word in 10 performing a metadiscursive function, most prominently by recourse to interactional metadiscursive resources. A qualitative analysis shows that, across the three denominations, preachers adopt metadiscursive practices that may serve to address expectations in the turn-to-the-listener preaching paradigm. The findings will inform further applied analysis of religious discourse, and will add constructively to discussions within homiletics, not least by suggesting how empirical and applied approaches to language can be of service to homiletics and the teaching of preaching.},
  keywords = {GELEZEN},
  file = {/media/cloud/Zotero/storage/UTW6KRZK/Malmström_2016_Engaging the Congregation.pdf}
}

@article{manjavacas,
  title = {A {{Statistical Foray}} into {{Contextual Aspects}} of {{Intertextuality}}},
  author = {Manjavacas, Enrique and Karsdorp, Folgert and Kestemont, Mike},
  pages = {20},
  abstract = {Intertextuality is a highly productive concept in literary theory. The pervasiveness of intertextuality in literary texts has lead simultaneously to a proliferation of applications with often divergent interpretations of the concept of intertextuality, as well as a recurrent interest in studying it from a computational point of view. Despite the potential of data-driven, bottom-up approaches, most computational research into intertextuality has focused on the matter of text reuse detection, exploiting surface-level properties to improve the performance of retrieval systems. In the present study, we utilize the Patrologia Latina – a substantial collection of religious texts spanning over a millennium of Latin writing (3rd to 13th centuries) – to provide a large-scale systematic study of biblical intertexts. On the basis of multi-level statistical models, we investigate two axes of intertexts: the degree of lexical similarity, and the degree to which intertexts are thematically embedded in the context. Furthermore, we investigate the extent to which the following contextual sources of variation help explain the distribution of intertexts along the aforementioned axes: first, we analyze the effect of authorship: do authors differ in the way they compose their intertexts? Secondly, we inspect factors related to the source collection (i.e. the Bible) to elucidate whether the authority and tradition of particular books exert an influence on the observed intertexts: do certain books trigger a more allusive or quotational intertext type? Finally, we take into account the dominant topic surrounding the intertext location and examine associations between the distribution of dominant topics and intertext types. On the one hand, our analysis indicates that both axes (lexical similarity and thematic embedding) play partially complementary roles in our computational account of intertextual types. On the other hand, we find that biblical books and, more strongly, dominant topics constitute important factors of variation, while the authorial signal remains comparatively weak.},
  langid = {english},
  file = {/media/cloud/Zotero/storage/BK7QC3CJ/Manjavacas e.a. - A Statistical Foray into Contextual Aspects of Int.pdf}
}

@article{mccarthy2010,
  title = {{{MTLD}}, Vocd-{{D}}, and {{HD-D}}: {{A}} Validation Study of Sophisticated Approaches to Lexical Diversity Assessment},
  shorttitle = {{{MTLD}}, Vocd-{{D}}, and {{HD-D}}},
  author = {McCarthy, Philip M. and Jarvis, Scott},
  date = {2010-05-01},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behavior Research Methods},
  volume = {42},
  number = {2},
  pages = {381--392},
  issn = {1554-3528},
  doi = {10.3758/BRM.42.2.381},
  url = {https://doi.org/10.3758/BRM.42.2.381},
  urldate = {2023-03-25},
  abstract = {The main purpose of this study was to examine the validity of the approach to lexical diversity assessment known as the measure of textual lexical diversity (MTLD). The index for this approach is calculated as the mean length of word strings that maintain a criterion level of lexical variation. To validate the MTLD approach, we compared it against the performances of the primary competing indices in the field, which include vocd-D, TTR, Maas, Yule’s K, and an HD-D index derived directly from the hypergeometric distribution function. The comparisons involved assessments of convergent validity, divergent validity, internal validity, and incremental validity. The results of our assessments of these indices across two separate corpora suggest three major findings. First, MTLD performs well with respect to all four types of validity and is, in fact, the only index not found to vary as a function of text length. Second, HD-D is a viable alternative to the vocd-D standard. And third, three of the indices—MTLD, vocd-D (or HD-D), and Maas—appear to capture unique lexical information. We conclude by advising researchers to consider using MTLD, vocd-D (or HD-D), and Maas in their studies, rather than any single index, noting that lexical diversity can be assessed in many ways and each approach may be informative as to the construct under investigation.},
  langid = {english},
  keywords = {Divergent Validity,Factor Size,Hypergeometric Distribution,Latent Semantic Analysis,Specific Language Impairment},
  file = {/media/cloud/Zotero/storage/MQ8GEYFG/McCarthy_Jarvis_2010_MTLD, vocd-D, and HD-D.pdf}
}

@article{mcgillivray2020,
  title = {Digital {{Humanities}} and {{Natural Language Processing}}: {{Je}} t’aime... {{Moi}} Non Plus},
  shorttitle = {Digital {{Humanities}} and {{Natural Language Processing}}},
  author = {McGillivray, Barbara and Poibeau, Thierry and Fabo, Pablo Ruiz},
  date = {2020-06-19},
  journaltitle = {Digital Humanities Quarterly},
  shortjournal = {DHQ},
  volume = {014},
  number = {2},
  issn = {1938-4122},
  file = {/media/cloud/Zotero/storage/GHZ3B8Q4/000454.html}
}

@article{mercha2023a,
  title = {Machine Learning and Deep Learning for Sentiment Analysis across Languages: {{A}} Survey},
  shorttitle = {Machine Learning and Deep Learning for Sentiment Analysis across Languages},
  author = {Mercha, El Mahdi and Benbrahim, Houda},
  date = {2023-04},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {531},
  pages = {195--216},
  issn = {09252312},
  doi = {10.1016/j.neucom.2023.02.015},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231223001546},
  urldate = {2023-12-07},
  abstract = {The inception and rapid growth of the Web, social media, and other online forums have resulted in the continuous and rapid generation of opinionated textual data. Several real-world applications have been focusing on determining the sentiments expressed in these data. Owing to the multilinguistic nature of the generated data, there exists an increasing need to perform sentiment analysis on data in diverse languages. This study presents an overview of the methods used to perform sentiment analysis across languages. We primarily focus on multilingual and cross-lingual approaches. This survey covers the early approaches and current advancements that employ machine learning and deep learning models. We categorize these methods and techniques and provide new research directions. Our findings reveal that deep learning techniques have been widely used in both approaches and yield the best results. Additionally, the scarcity of multilingual annotated datasets limits the progress of multilingual and cross-lingual sentiment analyses, and therefore increases the complexity in comparing these techniques and determining the ones with the best performance.},
  langid = {english},
  file = {/media/cloud/Zotero/storage/E9MRREQ5/Mercha en Benbrahim - 2023 - Machine learning and deep learning for sentiment a.pdf}
}

@book{moehn2001,
  title = {"{{God Calls}} Us to His {{Service}}" : {{The Relation}} between {{God}} and His {{Audience}} in {{Calvin}}'s {{Sermons}} on {{Acts}}},
  shorttitle = {"{{God Calls}} Us to His {{Service}}"},
  author = {Moehn, Wilhelmus H. Th},
  date = {2001-01-01},
  eprint = {aLt1AJv0IrEC},
  eprinttype = {googlebooks},
  publisher = {{Librairie Droz}},
  abstract = {De 1549 à 1551, Jean Calvin prêchait le dimanche à partir des Actes des apôtres : c’est une part importante de ces sermons qu’étudie de manière détaillée W. Moehn. Plutôt qu’un dogmaticien rigoriste, c’est un orateur attentif à son auditoire qui se dégage et dont les préoccupations pastorales intègrent autant le cadre social – politique, travail, interdépendance – que les particularités confessionnelles de ses ouailles.},
  isbn = {978-2-600-00483-1},
  langid = {english},
  pagetotal = {286},
  keywords = {Religion / General}
}

@article{monroe,
  title = {Learning in the {{Rational Speech Acts Model}}},
  author = {Monroe, Will and Potts, Christopher},
  pages = {12},
  abstract = {The Rational Speech Acts (RSA) model treats language use as a recursive process in which probabilistic speaker and listener agents reason about each other’s intentions to enrich the literal semantics of their language along broadly Gricean lines. RSA has been shown to capture many kinds of conversational implicature, but it has been criticized as an unrealistic model of speakers, and it has so far required the manual specification of a semantic lexicon, preventing its use in natural language processing applications that learn lexical knowledge from data. We address these concerns by showing how to define and optimize a trained statistical classifier that uses the intermediate agents of RSA as hidden layers of representation forming a non-linear activation function. This treatment opens up new application domains and new possibilities for learning effectively from data. We validate the model on a referential expression generation task, showing that the best performance is achieved by incorporating features approximating well-established insights about natural language generation into RSA.},
  langid = {english},
  file = {/media/cloud/Zotero/storage/SNEISC9T/Monroe and Potts - Learning in the Rational Speech Acts Model.pdf}
}

@article{moors2013,
  title = {Norms of Valence, Arousal, Dominance, and Age of Acquisition for 4,300 {{Dutch}} Words},
  author = {Moors, Agnes and De Houwer, Jan and Hermans, Dirk and Wanmaker, Sabine and family=Schie, given=Kevin, prefix=van, useprefix=true and Van Harmelen, Anne-Laura and De Schryver, Maarten and De Winne, Jeffrey and Brysbaert, Marc},
  date = {2013-03-01},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behav Res},
  volume = {45},
  number = {1},
  pages = {169--177},
  issn = {1554-3528},
  doi = {10.3758/s13428-012-0243-8},
  url = {https://doi.org/10.3758/s13428-012-0243-8},
  urldate = {2022-02-16},
  abstract = {This article presents norms of valence/pleasantness, activity/arousal, power/dominance, and age of acquisition for 4,300 Dutch words, mainly nouns, adjectives, adverbs, and verbs. The norms are based on ratings with a 7-point Likert scale by independent groups of students from two Belgian (Ghent and Leuven) and two Dutch (Rotterdam and Leiden-Amsterdam) samples. For each variable, we obtained high split-half reliabilities within each sample and high correlations between samples. In addition, the valence ratings of a previous, more limited study (Hermans \& De Houwer, Psychologica Belgica, 34:115-139, 1994) correlated highly with those of the present study. Therefore, the new norms are a valuable source of information for affective research in the Dutch language.},
  langid = {english},
  file = {/media/cloud/Zotero/storage/X5P7RI8V/Moors et al_2013_Norms of valence, arousal, dominance, and age of acquisition for 4,300 Dutch.pdf}
}

@incollection{morris2012,
  title = {Preaching the {{Oxford Movement}}},
  booktitle = {The {{Oxford Handbook}} of the {{British Sermon}} 1689-1901},
  author = {Morris, Jeremy},
  editor = {Francis, Keith A. and Gibson, William and Morgan-Guy, John and Tennant, Bob and Ellison, Robert H.},
  date = {2012-10-04},
  pages = {0},
  publisher = {{Oxford University Press}},
  doi = {10.1093/oxfordhb/9780199583591.013.0025},
  url = {https://doi.org/10.1093/oxfordhb/9780199583591.013.0025},
  urldate = {2023-07-20},
  abstract = {This article explores the controversial Oxford Movement, which sought to reform the teaching and pastoral practice of the Church of England. It emphasizes the key role of the sermon, given the fact most of the movement’s leading figures were at one time or another parish priests. The sermons of a handful of leaders are considered as well as the preaching of some of the more minor Tractarian clergy. Of all the published sermons of the Oxford Movement, those of Newman proved to be the most enduring and influential, read as much for their literary style as for their theological content.},
  isbn = {978-0-19-958359-1},
  file = {/media/cloud/Zotero/storage/KIHBEY6M/291469379.html}
}

@article{mostrom,
  title = {A Comparison of Algorithms for Classifying out of Context Utterances with {{DAMSL}}},
  author = {Mostrom, Erik},
  pages = {18},
  langid = {english},
  file = {/media/cloud/Zotero/storage/HVES6DYL/Mostrom - A comparison of algorithms for classifying out of .pdf}
}

@article{mullen2015,
  title = {Textreuse: {{Detect Text Reuse}} and {{Document Similarity}}},
  shorttitle = {Textreuse},
  author = {Mullen, Lincoln},
  date = {2015-11-05},
  publisher = {{rOpenSci}},
  doi = {10.13021/G80W2B},
  url = {http://mars.gmu.edu/handle/1920/10077},
  urldate = {2023-03-23},
  abstract = {This R package provides a set of functions for measuring similarity among documents and detecting passages which have been reused. It implements shingled n-gram, skip n-gram, and other tokenizers; similarity/dissimilarity functions; pairwise comparisons; minhash and locality sensitive hashing algorithms; and a version of the Smith-Waterman local alignment algorithm suitable for natural language. It is broadly useful for, for example, detecting duplicate documents in a corpus prior to text analysis, or for identifying borrowed passages between texts. The classes provides by this package follow the model of other natural language processing packages for R, especially the NLP and tm packages. (However, this package has no dependency on Java, which should make it easier to install.)},
  annotation = {Accepted: 2016-03-01T16:42:04Z}
}

@inproceedings{muller2016a,
  title = {Machine {{Learning}} and {{Grounded Theory Method}}: {{Convergence}}, {{Divergence}}, and {{Combination}}},
  shorttitle = {Machine {{Learning}} and {{Grounded Theory Method}}},
  booktitle = {Proceedings of the 19th {{International Conference}} on {{Supporting Group Work}}},
  author = {Muller, Michael and Guha, Shion and Baumer, Eric P.S. and Mimno, David and Shami, N. Sadat},
  date = {2016-11-13},
  series = {{{GROUP}} '16},
  pages = {3--8},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2957276.2957280},
  url = {http://doi.org/10.1145/2957276.2957280},
  urldate = {2022-03-05},
  abstract = {Grounded Theory Method (GTM) and Machine Learning (ML) are often considered to be quite different. In this note, we explore unexpected convergences between these methods. We propose new research directions that can further clarify the relationships between these methods, and that can use those relationships to strengthen our ability to describe our phenomena and develop stronger hybrid theories.},
  isbn = {978-1-4503-4276-6},
  keywords = {axial coding,coding families,grounded theory,machine learning,supervised learning,unsupervised learning},
  file = {/media/cloud/Zotero/storage/BSG462A2/Muller et al_2016_Machine Learning and Grounded Theory Method.pdf}
}

@article{nartey2019,
  title = {Towards a Decade of Synergising Corpus Linguistics and Critical Discourse Analysis: A Meta-Analysis},
  shorttitle = {Towards a Decade of Synergising Corpus Linguistics and Critical Discourse Analysis},
  author = {Nartey, Mark and Mwinlaaru, Isaac N.},
  date = {2019-08-01},
  journaltitle = {Corpora},
  shortjournal = {Corpora},
  volume = {14},
  number = {2},
  pages = {203--235},
  publisher = {{Edinburgh University Press}},
  issn = {1749-5032},
  doi = {10.3366/cor.2019.0169},
  url = {https://www.euppublishing.com/doi/abs/10.3366/cor.2019.0169},
  urldate = {2023-04-03},
  abstract = {The incorporation of corpus linguistics (CL) methods within critical discourse analysis (CDA) has increasingly gathered momentum over the last decade. This paper surveys studies using this triangul...},
  keywords = {corpus linguistics,Critical Discourse Analysis,ideology and power,meta-analysis,NOG-LEZEN},
  file = {/media/cloud/Zotero/storage/XHMANKSV/Nartey_Mwinlaaru_2019_Towards a decade of synergising corpus linguistics and critical discourse.pdf}
}

@article{neves2021,
  title = {An Extensive Review of Tools for Manual Annotation of Documents},
  author = {Neves, Mariana and Ševa, Jurica},
  date = {2021-01-01},
  journaltitle = {Briefings in Bioinformatics},
  shortjournal = {Briefings in Bioinformatics},
  volume = {22},
  number = {1},
  pages = {146--163},
  issn = {1477-4054},
  doi = {10.1093/bib/bbz130},
  url = {https://doi.org/10.1093/bib/bbz130},
  urldate = {2022-01-31},
  abstract = {Annotation tools are applied to build training and test corpora, which are essential for the development and evaluation of new natural language processing algorithms. Further, annotation tools are also used to extract new information for a particular use case. However, owing to the high number of existing annotation tools, finding the one that best fits particular needs is a demanding task that requires searching the scientific literature followed by installing and trying various tools.We searched for annotation tools and selected a subset of them according to five requirements with which they should comply, such as being Web-based or supporting the definition of a schema. We installed the selected tools (when necessary), carried out hands-on experiments and evaluated them using 26 criteria that covered functional and technical aspects. We defined each criterion on three levels of matches and a score for the final evaluation of the tools.We evaluated 78 tools and selected the following 15 for a detailed evaluation: BioQRator, brat, Catma, Djangology, ezTag, FLAT, LightTag, MAT, MyMiner, PDFAnno, prodigy, tagtog, TextAE, WAT-SL and WebAnno. Full compliance with our 26 criteria ranged from only 9 up to 20 criteria, which demonstrated that some tools are comprehensive and mature enough to be used on most annotation projects. The highest score of 0.81 was obtained by WebAnno (of a maximum value of 1.0).},
  file = {/media/cloud/Zotero/storage/XVANAWH7/Neves_Ševa_2021_An extensive review of tools for manual annotation of documents.pdf;/media/cloud/Zotero/storage/LGREZKWI/5670958.html}
}

@article{nguyen2020,
  title = {How {{We Do Things With Words}}: {{Analyzing Text}} as {{Social}} and {{Cultural Data}}},
  shorttitle = {How {{We Do Things With Words}}},
  author = {Nguyen, Dong and Liakata, Maria and DeDeo, Simon and Eisenstein, Jacob and Mimno, David and Tromble, Rebekah and Winters, Jane},
  date = {2020},
  journaltitle = {Frontiers in Artificial Intelligence},
  shortjournal = {Front. Artif. Intell.},
  volume = {3},
  publisher = {{Frontiers}},
  issn = {2624-8212},
  doi = {10.3389/frai.2020.00062},
  url = {https://www.frontiersin.org/articles/10.3389/frai.2020.00062/full},
  urldate = {2020-11-18},
  abstract = {In this article we describe our experiences with computational text analysis involving rich social and cultural concepts. We hope to achieve three primary goals. First, we aim to shed light on thorny issues not always at the forefront of discussions about computational text analysis methods. Second, we hope to provide a set of key questions that can guide work in this area. Our guidance is based on our own experiences and is therefore inherently imperfect. Still, given our diversity of disciplinary backgrounds and research practices, we hope to capture a range of ideas and identify commonalities that resonate for many. This leads to our final goal: to help promote interdisciplinary collaborations. Interdisciplinary insights and partnerships are essential for realizing the full potential of any computational text analysis involving social and cultural concepts, and the more we bridge these divides, the more fruitful we believe our work will be.},
  langid = {english},
  keywords = {computational social science,computational text analysis,Cultural analytics,Digital Humanities,Natural Language Processing},
  file = {/media/cloud/Zotero/storage/QMHN4I2Y/nguyen_2020_how_we_do_things_with_words.pdf}
}

@book{okeeffe2022,
  title = {The {{Routledge Handbook}} of {{Corpus Linguistics}}},
  editor = {O'Keeffe, Anne and McCarthy, Michael J.},
  date = {2022-02-09},
  edition = {2},
  publisher = {{Routledge}},
  location = {{London}},
  doi = {10.4324/9780367076399},
  abstract = {The Routledge Handbook of Corpus Linguistics 2e provides an updated overview of a dynamic and rapidly growing area with a widely applied methodology. Over a decade on from the first edition of the Handbook, this collection of 47 chapters from experts in key areas offers a comprehensive introduction to both the development and use of corpora as well as their ever-evolving applications to other areas, such as digital humanities, sociolinguistics, stylistics, translation studies, materials design, language teaching and teacher development, media discourse, discourse analysis, forensic linguistics, second language acquisition and testing. The new edition updates all core chapters and includes new chapters on corpus linguistics and statistics, digital humanities, translation, phonetics and phonology, second language acquisition, social media and theoretical perspectives. Chapters provide annotated further reading lists and step-by-step guides as well as detailed overviews across a wide range of themes. The Handbook also includes a wealth of case studies that draw on some of the many new corpora and corpus tools that have emerged in the last decade. Organised across four themes, moving from the basic start-up topics such as corpus building and design to analysis, application and reflection, this second edition remains a crucial point of reference for advanced undergraduates, postgraduates and scholars in applied linguistics.},
  isbn = {978-0-367-07639-9},
  pagetotal = {754},
  file = {/media/cloud/Zotero/storage/38PGH6ZV/10.4324_9780367076399-37_chapterpdf.pdf;/media/cloud/Zotero/storage/3HRKSMZS/10.4324_9780367076399-36_chapterpdf.pdf;/media/cloud/Zotero/storage/982KC92Z/10.4324_9780367076399-5_chapterpdf.pdf;/media/cloud/Zotero/storage/DXH5EPJJ/10.4324_9780367076399-43_chapterpdf.pdf;/media/cloud/Zotero/storage/E5J68J3N/10.4324_9780367076399-17_chapterpdf.pdf;/media/cloud/Zotero/storage/GYXJFDDG/10.4324_9780367076399-35_chapterpdf.pdf;/media/cloud/Zotero/storage/J2BDREQV/10.4324_9780367076399-13_chapterpdf.pdf;/media/cloud/Zotero/storage/S6YPPXPQ/10.4324_9780367076399-42_chapterpdf.pdf;/media/cloud/Zotero/storage/T543E8R6/10.4324_9780367076399-10_chapterpdf.pdf;/media/cloud/Zotero/storage/VQ6DPQ99/10.4324_9780367076399-2_chapterpdf.pdf;/media/cloud/Zotero/storage/WSPGPYPL/10.4324_9780367076399-11_chapterpdf.pdf;/media/cloud/Zotero/storage/XHQB8ACY/O'Keeffe_McCarthy_2022_The Routledge Handbook of Corpus Linguistics.pdf;/media/cloud/Zotero/storage/ZR6ZDJ9P/10.4324_9780367076399-4_chapterpdf.pdf}
}

@book{old2007,
  title = {The {{Reading}} and {{Preaching}} of the {{Scriptures}} in the {{Worship}} of the {{Christian Church}}. {{Volume}} 6. {{The Modern Age}}},
  shorttitle = {The {{Modern Age}} ({{Vol}}. 6)},
  author = {Old, Hughes Oliphant},
  date = {2007},
  publisher = {{William B Eerdmans Publishing Co}},
  location = {{Grand Rapids}},
  file = {/media/cloud/Zotero/storage/45IIMV57/Old_2007_The Reading and Preaching of the Scriptures in the Worship of the Christian.pdf}
}

@book{ong2002,
  title = {Orality and {{Literacy}}},
  author = {Ong, Walter J.},
  date = {2002},
  edition = {Second},
  publisher = {{Routledge}},
  location = {{London}}
}

@inproceedings{oshea2010,
  title = {A {{Machine Learning Approach}} to {{Speech Act Classification Using Function Words}}},
  booktitle = {Agent and {{Multi-Agent Systems}}: {{Technologies}} and {{Applications}}},
  author = {O’Shea, James and Bandar, Zuhair and Crockett, Keeley},
  editor = {Jędrzejowicz, Piotr and Nguyen, Ngoc Thanh and Howlet, Robert J. and Jain, Lakhmi C.},
  date = {2010},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {82--91},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-13541-5_9},
  abstract = {This paper presents a novel technique for the classification of sentences as Dialogue Acts, based on structural information contained in function words. It focuses on classifying questions or non-questions as a generally useful task in agent-based systems. The proposed technique extracts salient features by replacing function words with numeric tokens and replacing each content word with a standard numeric wildcard token. The Decision Tree, which is a well-established classification technique, has been chosen for this work. Experiments provide evidence of potential for highly effective classification, with a significant achievement on a challenging dataset, before any optimisation of feature extraction has taken place.},
  isbn = {978-3-642-13541-5},
  langid = {english},
  keywords = {Classification,Decision Tree,Dialogue Act,Semantic Similarity,Speech Act},
  file = {/media/cloud/Zotero/storage/LHK865ZG/O’Shea et al. - 2010 - A Machine Learning Approach to Speech Act Classifi.pdf}
}

@unpublished{otte2022,
  title = {Een Computeranalyse van de Vermeende Letter-Geest Tegenstelling in 2 {{Kor}} 3:6},
  author = {Otte, Wim},
  date = {2022-10-28},
  url = {https://thijmgenootschap.nl/nieuws/studiemiddag-van-de-afdeling-katholieke-theologie-het-begin-was-het-wwoord},
  eventtitle = {Studiemiddag van de {{Afdeling Katholieke Theologie}}: {{In}} Het Begin Was Het w/{{Woord}}},
  venue = {{Den Bosch}},
  file = {/media/cloud/Zotero/storage/CVCNFTQ9/Otte-Thijmlezing_23-02-2023.pdf}
}

@inproceedings{Pagel2021ab,
  title = {Predicting Structural Elements in German Drama},
  booktitle = {Proceedings of the Second Conference on Computational Humanities Research},
  author = {Pagel, Janis and Sihag, Nidhi and {Nils Reiter}},
  date = {2021-11},
  file = {/media/cloud/Zotero/storage/3HM9EVRF/Pagel et al_2021_Predicting structural elements in german drama.pdf}
}

@article{peursen2023,
  title = {Computational {{Linguistic Analysis}} of the {{Biblical Text}}},
  author = {family=Peursen, given=Willem, prefix=van, useprefix=false},
  editor = {Ross, William A. and Robar, Elizabeth},
  date = {2023-09-25},
  journaltitle = {Linguistic Theory and the Biblical Text},
  pages = {223--272},
  publisher = {{Open Book Publishers}},
  doi = {10.11647/obp.0358.05},
  url = {https://www.openbookpublishers.com/books/10.11647/obp.0358/chapters/10.11647/obp.0358.05},
  urldate = {2023-09-26},
  abstract = {Historical linguistics enjoys a venerable history among the many subfields of linguistic study. Many of the tools employed in histori-cal linguistics, as well as some of its theoretical concepts, are well-suited for biblical studies which engage in the ancient languages of the Bible. Knowledge about the kinds of language change common to many of the world’s languages can be useful as one type of evi-dence for the periodisation of biblical books. Additionally, knowledge about the external linguistic influences that shaped the biblical languages, as well as their prior histories (both Semitic and Indo-European) provide a helpful context for studying many syn-chronic aspects of the texts of Scripture. Furthermore, text-critical judgments about the biblical text can be strengthened when in-formed by knowledge of language change across the manuscript tradition. In short, historical linguistics offers a number of unique insights for biblical scholars engaging in the study of the biblical languages. Key words: Historical linguistics, diachronic, sound change, anal-ogy, Indo-European, Semitic},
  langid = {english},
  file = {/media/cloud/Zotero/storage/SQE64FKT/Peursen_Th_2023_Computational Linguistic Analysis of the Biblical Text.pdf}
}

@online{pewresearchcenter2019,
  title = {The {{Digital Pulpit}}. {{A Nationwide Analysis}} of {{Online Sermons}}},
  author = {Pew Research Center},
  date = {2019-12-16},
  url = {https://www.pewforum.org/2019/12/16/the-digital-pulpit-a-nationwide-analysis-of-online-sermons/},
  urldate = {2019-12-30},
  abstract = {This Pew Research Center analysis harnesses computational techniques to identify, collect and analyze the sermons that U.S. churches livestream or share on their websites each week.},
  langid = {american},
  organization = {{Pew Research Center's Religion \& Public Life Project}},
  keywords = {GELEZEN},
  file = {/media/cloud/Zotero/storage/3MYHNST2/PF.12.16.19_sermons.analysis.report.pdf;/media/cloud/Zotero/storage/H2UVXUKV/PF.12.16.19_sermons.analysis.appendix.pdf;/media/cloud/Zotero/storage/E4HULMRX/the-digital-pulpit-a-nationwide-analysis-of-online-sermons.html}
}

@article{phillips2019,
  title = {Defining {{Digital Theology}}: {{Digital Humanities}}, {{Digital Religion}} and the {{Particular Work}} of the {{CODEC Research Centre}} and {{Network}}},
  shorttitle = {Defining {{Digital Theology}}},
  author = {Phillips, Peter and Schiefelbein-Guerrero, Kyle and Kurlberg, Jonas},
  date = {2019},
  journaltitle = {Open Theology},
  volume = {5},
  number = {1},
  pages = {29--43},
  issn = {2300-6579},
  doi = {10.1515/opth-2019-0003},
  url = {http://www.degruyter.com/view/j/opth.2019.5.issue-1/opth-2019-0003/opth-2019-0003.xml?format=INT},
  urldate = {2019-10-29},
  abstract = {This article seeks to define Digital Theology, first by exploring the development of the CODEC Research Centre at Durham University – perhaps the only centre developed to explore Digital Theology. The aims of the centre and some of its projects are explored leading to a discussion of CODEC’s place within Digital Humanities. The article concludes with a focus on different aspects and definitions of Digital Theology.},
  keywords = {CODEC,Computing for Humanities,digital culture,Digital Humanities,Digital Theology,GELEZEN,Theology},
  file = {/media/cloud/Zotero/storage/4AYAHI4E/Phillips et al_2019_Defining Digital Theology.pdf}
}

@article{pichler2021,
  title = {Zur {{Operationalisierung}} Literaturwissenschaftlicher {{Begriffe}} in Der Algorithmischen {{Textanalyse}}. {{Eine Annäherung}} Über {{Norbert Altenhofers}} Hermeneutische {{Modellinterpretation}} von {{KleistsDas Erdbeben}} in {{Chili}}},
  author = {Pichler, Axel and Reiter, Nils},
  date = {2021-12-01},
  journaltitle = {Journal of Literary Theory},
  volume = {15},
  number = {1-2},
  pages = {1--29},
  publisher = {{De Gruyter}},
  issn = {1862-8990},
  doi = {10.1515/jlt-2021-2008},
  url = {http://www.degruyter.com/document/doi/10.1515/jlt-2021-2008/html},
  urldate = {2022-01-11},
  abstract = {The present article discusses and reflects on possible ways of operationalizing the terminology of traditional literary studies for use in computational literary studies. By »operationalization«, we mean the development of a method for tracing a (theoretical) term back to text-surface phenomena; this is done explicitly and in a rule-based manner, involving a series of substeps. This procedure is presented in detail using as a concrete example Norbert Altenhofer’s »model interpretation« ( Modellinterpretation ) of Heinrich von Kleist’s The Earthquake in Chile . In the process, we develop a multi-stage operation –~reflected upon throughout in terms of its epistemological implications~– that is based on a rational-hermeneutic reconstruction of Altenhofer’s interpretation, which focuses on »mysteriousness« ( Rätselhaftigkeit ), a concept from everyday language. As we go on to demonstrate, when trying to operationalize this term, one encounters numerous difficulties, which is owing to the fact that Altenhofer’s use of it is underspecified in a number of ways. Thus, for instance, and contrary to Altenhofer’s suggestion, Kleist’s sentences containing »relativizing or perspectivizing phrases such as ›it seemed‹ or ›it was as if‹« (Altenhofer 2007, 45) do by no means, when analyzed linguistically, suggest a questioning or challenge of the events narrated, since the unreal quality of those German sentences only relates to the comparison in the subordinate clause, not to the respective main clause. Another indicator central to Altenhofer’s ascription of »mysteriousness« is his concept of a »complete facticity« ( lückenlose Faktizität ) which »does not seem to leave anything ›open‹« (Altenhofer 2007, 45). Again, the precise designation of what exactly qualifies facticity as »complete« is left open, since Kleist’s novella does indeed select for portrayal certain phenomena and actions within the narrated world (and not others). The degree of factuality in Kleist’s text may be higher than it is in other texts, but it is by no means »complete«. In the context of Altenhofer’s interpretation, »complete facticity« may be taken to mean a narrative mode in which terrible events are reported using conspicuously sober and at times drastic language. Following the critical reconstruction of Altenhofer’s use of terminology, the central terms and their relationship to one another are first explicated (in natural language), which already necessitates intensive conceptual work. We do so implementing a hierarchical understanding of the terms discussed: the definition of one term uses other terms which also need to be defined and operationalized. In accordance with the requirements of computational text analysis, this hierarchy of terms should end in »directly measurable« terms~–~i.{$\mkern1mu$}e., in terms that can be clearly identified on the surface of the text. This, however, leads to the question of whether (and, if so, on the basis of which theoretical assumptions) the terminology of literary studies may be traced back in this way to text-surface phenomena. Following the pragmatic as well as the theoretical discussion of this complex of questions, we indicate ways by which such definitions may be converted into manual or automatic recognition. In the case of manual recognition, the paradigm of annotation –~as established and methodologically reflected in (computational) linguistics –~will be useful, and a well-controlled annotation process will help to further clarify the terms in question. The primary goal, however, is to establish a recognition rule by which individuals may intersubjectively and reliably identify instances of the term in question in a given text. While it is true that in applying this method to literary studies, new challenges arise –~such as the question of the validity and reliability of the annotations~–, these challenges are at present being researched intensively in the field of computational literary studies, which has resulted in a large and growing body of research to draw on. In terms of computer-aided recognition, we examine, by way of example, two distinct approaches: 1) The kind of operationalization which is guided by precedent definitions and annotation rules benefits from the fact that each of its steps is transparent, may be validated and interpreted, and that existing tools from computational linguistics can be integrated into the process. In the scenario used here, these would be tools for recognizing and assigning character speech, for the resolution of coreference and the assessment of events; all of these, in turn, may be based on either machine learning, prescribed rules or dictionaries. 2) In recent years, so-called end-to-end systems have become popular which, with the help of neural networks, »infer« target terms directly from a numerical representation of the data. These systems achieve superior results in many areas. However, their lack of transparency also raises new questions, especially with regard to the interpretation of results. Finally, we discuss options for quality assurance and draw a first conclusion. Since numerous decisions have to be made in the course of operationalization, and these, in practice, are often pragmatically justified, the question quickly arises as to how »good« a given operationalization actually is. And since the tools borrowed from computational linguistics (especially the so-called inter-annotator agreement) can only partially be transferred to computational literary studies and, moreover, objective standards for the quality of a given implementation will be difficult to find, it ultimately falls to the community of researchers and scholars to decide, based on their research standards, which operationalizations they accept. At the same time, operationalization is the central link between the computer sciences and literary studies, as well as being a necessary component for a large part of the research done in computational literary studies. The advantage of a conscious, deliberate and reflective operationalization practice lies not only in the fact that it can be used to achieve reliable quantitative results (or that a certain lack of reliability at least is a known factor); it also lies in its facilitation of interdisciplinary cooperation: in the course of operationalization, concrete sets of data are discussed, as are the methods for analysing them, which taken together minimizes the risk of misunderstandings, »false friends« and of an unproductive exchange more generally.},
  langid = {english},
  file = {/media/cloud/Zotero/storage/YYMBX5DK/Pichler_Reiter_2021_Zur Operationalisierung literaturwissenschaftlicher Begriffe in der.pdf}
}

@book{pieterse1995,
  title = {Desmond {{Tutu}}'s Message a Qualitative Analysis},
  shorttitle = {Desmond {{Tutu}}'s Message a Qualitative Analysis},
  author = {Pieterse, H. J. C.},
  date = {1995},
  series = {Theologie \& Empirie 24},
  publisher = {{Kok}},
  location = {{Kampen}},
  isbn = {90.390.0522.2 (Kok) 3.89271.620.X (DSV)},
  pagetotal = {158},
  keywords = {{Tutu, D.}}
}

@article{pieterse2020a,
  title = {A Short History of Empirical Homiletics in {{South Africa}}},
  author = {Pieterse, Hendrik J. C.},
  date = {2020},
  journaltitle = {Stellenbosch Theological Journal},
  volume = {6},
  number = {2},
  pages = {345--364},
  publisher = {{Pieter de Waal Neethling Trust}},
  issn = {2413-9467},
  doi = {10.17570/stj.2020.v6n2.a15},
  url = {http://www.scielo.org.za/scielo.php?script=sci_abstract&pid=S2413-94672020000200016&lng=en&nrm=iso&tlng=en},
  urldate = {2023-03-17},
  file = {/media/cloud/Zotero/storage/H3L9B6A4/Pieterse_2020_A short history of empirical homiletics in South Africa.pdf}
}

@inproceedings{piper2021,
  title = {Narrative {{Theory}} for {{Computational Narrative Understanding}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Piper, Andrew and So, Richard Jean and Bamman, David},
  date = {2021-11},
  pages = {298--311},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.26},
  url = {https://aclanthology.org/2021.emnlp-main.26},
  urldate = {2022-02-01},
  abstract = {Over the past decade, the field of natural language processing has developed a wide array of computational methods for reasoning about narrative, including summarization, commonsense inference, and event detection. While this work has brought an important empirical lens for examining narrative, it is by and large divorced from the large body of theoretical work on narrative within the humanities, social and cognitive sciences. In this position paper, we introduce the dominant theoretical frameworks to the NLP community, situate current research in NLP within distinct narratological traditions, and argue that linking computational work in NLP to theory opens up a range of new empirical questions that would both help advance our understanding of narrative and open up new practical applications.},
  eventtitle = {{{EMNLP}} 2021},
  file = {/media/cloud/Zotero/storage/HURX9JYM/long22.pdf;/media/cloud/Zotero/storage/IBD44AEV/Piper et al_2021_Narrative Theory for Computational Narrative Understanding.pdf}
}

@incollection{pleizier2022e,
  title = {Impact of {{Religious Speech}}. {{Theological}} and {{Anthropological Considerations}}},
  shorttitle = {Impact of {{Religious Speech}}},
  booktitle = {Dynamics of {{Speaking}} and {{Doing Religion}}},
  author = {Pleizier, Theo},
  editor = {Tulebaeva, Baktygul and Ojha, Deepak},
  date = {2022},
  series = {Ressourcenkulturen},
  number = {18},
  pages = {19--33},
  publisher = {{University Press}},
  location = {{Tübingen}},
  url = {https://publikationen.uni-tuebingen.de/xmlui/handle/10900/125631},
  abstract = {Does religious speech have a specific kind of impact? And if so, what research strategies are needed? This article argues that we need an intra-disciplinary approach that integrates anthropological and theological analyses. This approach is illustrated with two different research projects. The first case demonstrates that in the study of sermon reception of Protestant preaching the emerging concept of ‘actualisation of faith’ indicates a specific impact of religious speech. Impact then refers to the process how hearers of sermons become religiously involved. The second case presents the sermons of military chaplains. The religious qualities of their speeches are very diverse and the impact they aim for differs from the impact of regular congregational preaching. The paper concludes that religious impact should be understood in relation to religious cultures. The impact of religious speech reflects the diversity of religious cultures and thus enriches the understanding of religion as resource in society.},
  langid = {american},
  keywords = {homiletics,military chaplaincy,TO-DO},
  file = {/media/cloud/Zotero/storage/M7PQRR7B/featured-ressourcenkulturen18.png;/media/cloud/Zotero/storage/NDYK5J7T/pleizier_2022_impact_of_religious_speech.pdf}
}

@incollection{pleizier2023a,
  title = {6.1.4 {{Netherlands}}. {{Enhanced Streaming}} and the {{Rediscovery}} of {{Phone Calls}}. {{Experiences}} of {{Pastors}} during the First {{COVID-19 Lockdown}} in {{The Netherlands}}},
  booktitle = {Churches {{Online}} in {{Times}} of {{Corona}}. {{The CONTOC}} Study: {{Empirical Insights}}, {{Interpretations}}, and {{Perspectives}}},
  author = {Pleizier, Theo and family=Roest, given=Henk, prefix=de, useprefix=false},
  editor = {Schlag, Thomas and Nord, Ilona and Beck, Wolfgang and Bünker, Arnd and Lämmlin, Georg and Müller, Sabrina and Pock, Johann and Rothgangel, Martin},
  date = {2023},
  pages = {in press},
  publisher = {{Springer VS}},
  langid = {english},
  keywords = {TO-DO}
}

@article{pustejovsky,
  title = {{{TimeML}}: {{Robust Speciﬁcation}} of {{Event}} and {{Temporal Expressions}} in {{Text}}},
  author = {Pustejovsky, James and Castano, Jose and Ingria, Robert and Saurı, Roser and Gaizauskas, Robert and Setzer, Andrea and Katz, Graham},
  pages = {12},
  abstract = {In this paper we provide a description of TimeML, a rich specification language for event and temporal expressions in natural language text, developed in the context of the AQUAINT program on Question Answering Systems. Unlike most previous work on event annotation, TimeML captures three distinct phenomena in temporal markup: (1) it systematically anchors event predicates to a broad range of temporally denotating expressions; (2) it orders event expressions in text relative to one another, both intrasententially and in discourse; and (3) it allows for a delayed (underspecified) interpretation of partially determined temporal expressions. We demonstrate the expressiveness of TimeML for a broad range of syntactic and semantic contexts, including aspectual predication, modal subordination, and an initial treatment of lexical and constructional causation in text.},
  langid = {english},
  file = {/media/cloud/Zotero/storage/PGXJ224U/Pustejovsky e.a. - TimeML Robust Speciﬁcation of Event and Temporal .pdf}
}

@book{pustejovsky2013,
  title = {Natural Language Annotation for Machine Learning},
  author = {Pustejovsky, J. and Stubbs, Amber},
  date = {2013},
  publisher = {{O'Reilly Media}},
  location = {{Sebastopol, CA}},
  isbn = {978-1-4493-0666-3},
  langid = {english},
  pagetotal = {326},
  keywords = {Corpora (Linguistics),Data processing,GELEZEN,Machine learning,Natural language processing (Computer science)},
  annotation = {OCLC: ocn794362649},
  file = {/media/cloud/Zotero/storage/EI82CEKC/iso_tc37_sc4_N385_wg2_iso-timeml_provo2007_beamer_utf8.pdf;/media/cloud/Zotero/storage/ERIGNCR7/Pustejovsky en Stubbs - 2013 - Natural language annotation for machine learning.pdf}
}

@manual{rcoreteam2021,
  type = {manual},
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  date = {2021},
  location = {{Vienna, Austria}},
  url = {https://www.R-project.org/},
  organization = {{R Foundation for Statistical Computing}}
}

@article{reiter2014,
  title = {An {{NLP-based}} Cross-Document Approach to Narrative Structure Discovery},
  author = {Reiter, Nils and Frank, Anette and Hellwig, Oliver},
  date = {2014-12-01},
  journaltitle = {Literary and Linguistic Computing},
  shortjournal = {Literary and Linguistic Computing},
  volume = {29},
  number = {4},
  pages = {583--605},
  issn = {0268-1145},
  doi = {10.1093/llc/fqu055},
  url = {https://doi.org/10.1093/llc/fqu055},
  urldate = {2023-03-23},
  abstract = {Structural similarities across narratives play an important role in many areas of humanities research. In this article, we describe a methodology and an implementation to uncover such similarities automatically in two application scenarios. In both scenarios—ritual and folktale studies—existing research examines similarities of narratives on a structural level and discusses structural principles that govern the combination of individual events to tales or rituals. We present a largely unsupervised and fully automated alignment-based approach for the detection of structural similarities of narratives that allows for data-driven quantitative studies of narrative structure. Our approach makes use of an adaptable, computational linguistic processing architecture that creates integrated discourse representations of events, participants, and their relations. Our contributions are twofold and crucially build on the automatically constructed discourse representations: (1) We examine different ‘semantics-driven cross-document alignment’ algorithms that determine (sequences of) events shared between narratives, to support the search for recurrent elements in their structure. The alignment algorithms are evaluated in two experiments. (2) We develop ‘tools for exploration and interpretation’ that we offer to humanities researchers for investigation of the analyzed data. These include search facilities, visualizations, statistical overviews, and a graph-based algorithm that identifies densely aligned regions across documents for targeted inspection.},
  keywords = {NOG-LEZEN},
  file = {/media/cloud/Zotero/storage/P2U4JUJE/Reiter e.a. - 2014 - An NLP-based cross-document approach to narrative .pdf}
}

@incollection{reiter2020,
  title = {Die {{Erstellung}} von {{Annotationsrichtlinien}} Als {{Community-Aufgabe}} Für Die {{Digitalen Geisteswissenschaften}}},
  booktitle = {Die {{Erstellung}} von {{Annotationsrichtlinien}} Als {{Community-Aufgabe}} Für Die {{Digitalen Geisteswissenschaften}}},
  author = {Reiter, Nils and Willand, Marcus and Gius, Evelyn},
  date = {2020-10-26},
  pages = {325--350},
  publisher = {{De Gruyter}},
  doi = {10.1515/9783110689112-015},
  url = {https://www.degruyter.com/document/doi/10.1515/9783110689112-015/html},
  urldate = {2022-01-11},
  abstract = {Die Erstellung von Annotationsrichtlinien als Community-Aufgabe für die Digitalen Geisteswissenschaften was published in Annotations in Scholarly Editions and Research on page 325.},
  isbn = {978-3-11-068911-2},
  langid = {english},
  file = {/media/cloud/Zotero/storage/BW5ZTRHQ/Reiter et al_2020_Die Erstellung von Annotationsrichtlinien als Community-Aufgabe für die.pdf}
}

@incollection{robinson2009,
  title = {Preaching},
  booktitle = {The {{Cambridge Companion}} to {{John Henry Newman}}},
  author = {Robinson, Denis},
  editor = {Ker, Ian and Merrigan, Terrence},
  date = {2009-04-02},
  edition = {1},
  pages = {241--254},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/CCOL9780521871860.012},
  url = {https://www.cambridge.org/core/product/identifier/CBO9781139002448A016/type/book_part},
  urldate = {2023-04-12},
  isbn = {978-0-521-87186-0 978-0-521-69272-4 978-1-139-00244-8},
  keywords = {NU-LEZEN},
  file = {/media/cloud/Zotero/storage/7AP8RA3T/Robinson_2009_Preaching.pdf}
}

@article{robinson2022,
  title = {An Approach to Complex Texts in Multiple Documents},
  author = {Robinson, Peter M W},
  date = {2022-12-01},
  journaltitle = {Digital Scholarship in the Humanities},
  shortjournal = {Digital Scholarship in the Humanities},
  volume = {37},
  number = {4},
  pages = {1179--1196},
  issn = {2055-7671},
  doi = {10.1093/llc/fqab108},
  url = {https://doi.org/10.1093/llc/fqab108},
  urldate = {2023-03-29},
  abstract = {This article describes an approach to the treatment of texts in complex large textual traditions. Editors are interested in the text as it appears line-by-line in each document, and in how the versions of the text differ from document to document. It is useful to define a text as the record of an act of communication, inscribed in a document: thus, the instance of the act of communication we identify as Geoffrey Chaucer’s Canterbury Tales, as it appears in the Hengwrt manuscript. In this view, every text has a dual aspect: it is both the words as they are inscribed in a particular document, and as they constitute an act of communication and its parts. This presents challenges for scholars who wish to record both aspects. In encoding implementations, these two aspects are commonly treated as ‘overlapping hierarchies’. However, the ‘overlapping hierarchy’ model does not deal with cases where text segments are not contiguous in either aspect and cannot overlap cleanly. To meet these cases, the Textual Communities project developed an architecture in which the two aspects are represented as distinct and independent hierarchies (trees), with text segments referenced to nodes on each tree. The linking of text segments to the two trees is managed by a JSON database, accessed through transcription and collation tools presented in a Web interface. Textual Communities does not implement the whole of this architecture in terms of validation, ingestion, and processing. Full exploration and implementation of the architecture here described are challenges for future scholars.},
  keywords = {NOG-LEZEN},
  file = {/media/cloud/Zotero/storage/J4DKCR6M/Robinson_2022_An approach to complex texts in multiple documents.pdf;/media/cloud/Zotero/storage/VI3TJU3R/6538753.html}
}

@article{rus,
  title = {Automated {{Discovery}} of {{Speech Act Categories}} in {{Educational Games}}},
  author = {Rus, Vasile and Moldovan, Cristian and Niraula, Nobal},
  pages = {8},
  abstract = {In this paper we address the important task of automated discovery of speech act categories in dialogue-based, multi-party educational games. Speech acts are important in dialogue-based educational systems because they help infer the student speaker’s intentions (the task of speech act classification) which in turn is crucial to providing adequate feedback and scaffolding. A key step in the speech act classification task is defining the speech act categories in an underlying speech act taxonomy. Most research to date has relied on taxonomies which are guided by experts’ intuitions, which we refer to as an extrinsic design of the speech act taxonomies. A pure data-driven approach would discover the natural groupings of dialogue utterances and therefore reveal the intrinsic speech act categories. To this end, this paper presents a fully-automated data-driven method to discover speech act taxonomies based on utterance clustering. Experiments were conducted on three datasets from three online educational games. This work is a step towards building speech act taxonomies based on both extrinsic (expert-driven) and intrinsic aspects (datadriven) of the target domain.},
  langid = {english},
  file = {/media/cloud/Zotero/storage/983MEZIR/Rus et al. - Automated Discovery of Speech Act Categories in Ed.pdf}
}

@incollection{scott2012,
  title = {Sermons in {{British Catholicism}} to the {{Restoration}} of the {{Hierarchy}} (1689–1850)},
  booktitle = {The {{Oxford Handbook}} of the {{British Sermon}} 1689-1901},
  author = {Scott, Geoffrey},
  editor = {Francis, Keith A. and Gibson, William and Morgan-Guy, John and Tennant, Bob and Ellison, Robert H.},
  date = {2012-10-04},
  pages = {0},
  publisher = {{Oxford University Press}},
  doi = {10.1093/oxfordhb/9780199583591.013.0008},
  url = {https://doi.org/10.1093/oxfordhb/9780199583591.013.0008},
  urldate = {2023-07-20},
  abstract = {This article examines Catholic sermons in the eighteenth century. Published Catholic sermons were extremely rare in the years after the Revolution, with only one appearing to have survived—a sermon for Corpus Christi published in London in 1695. The passage of two Catholic Relief Acts for England in 1778 and 1791, and similar Acts for Ireland and Scotland, provided Catholics some degree of religious toleration, which led to the publication of additional sermons. Sermons also reflected the tensions within the Catholic community, between those who were willing to cooperate with the English establishment in exchange for further relief measures and those opposed to any surrender of Catholic principles.},
  isbn = {978-0-19-958359-1},
  file = {/media/cloud/Zotero/storage/9KKKBLMX/Scott_2012_Sermons in British Catholicism to the Restoration of the Hierarchy (1689–1850).pdf}
}

@book{silge2017,
  title = {Text {{Mining}} with {{R}}. {{A Tidy Approach}}},
  author = {Silge, Julia and Robinson, David},
  date = {2017},
  publisher = {{O'Reilly}},
  url = {https://www.tidytextmining.com/},
  urldate = {2021-09-17},
  abstract = {A guide to text analysis within the tidy data framework, using the tidytext package and other tidy tools},
  langid = {english},
  keywords = {GELEZEN},
  file = {/media/cloud/Zotero/storage/3TIQR92E/index.html}
}

@book{stefanowitsch2020,
  title = {Corpus Linguistics},
  author = {Stefanowitsch, Anatol},
  date = {2020-04-01},
  journaltitle = {Language Science Press},
  publisher = {{Language Science Press}},
  doi = {10.5281/zenodo.3735822},
  url = {https://langsci-press.org/catalog/view/148/2402/1811-1},
  urldate = {2023-03-22},
  abstract = {Corpora are widely used in linguistics, but not always wisely. This book attempts to frame corpus linguistics systematically as a variant of the observational method. The first part introduces the reader to the general methodological discussions surrounding corpus data as well as the practice of doing corpus linguistics, including issues such as the scientific research cycle, research design, extraction of corpus data and statistical evaluation. The second part consists of a number of case studies from the main areas of corpus linguistics (lexical associations, morphology, grammar, text and metaphor), surveying the range of issues studied in corpus linguistics while at the same time showing how they fit into the methodology outlined in the first part.},
  isbn = {978-3-96110-224-2},
  langid = {english},
  file = {/media/cloud/Zotero/storage/T8FYMM5S/Stefanowitsch_2020_Corpus linguistics.pdf}
}

@inproceedings{straka2017,
  title = {Tokenizing, {{POS Tagging}}, {{Lemmatizing}} and {{Parsing UD}} 2.0 with {{UDPipe}}},
  booktitle = {Proceedings of the {{CoNLL}} 2017 {{Shared Task}}: {{Multilingual Parsing}} from {{Raw Text}} to {{Universal Dependencies}}},
  author = {Straka, Milan and Straková, Jana},
  date = {2017-08},
  pages = {88--99},
  publisher = {{Association for Computational Linguistics}},
  location = {{Vancouver, Canada}},
  doi = {10.18653/v1/K17-3009},
  url = {https://aclanthology.org/K17-3009},
  urldate = {2022-03-17},
  abstract = {Many natural language processing tasks, including the most advanced ones, routinely start by several basic processing steps – tokenization and segmentation, most likely also POS tagging and lemmatization, and commonly parsing as well. A multilingual pipeline performing these steps can be trained using the Universal Dependencies project, which contains annotations of the described tasks for 50 languages in the latest release UD 2.0. We present an update to UDPipe, a simple-to-use pipeline processing CoNLL-U version 2.0 files, which performs these tasks for multiple languages without requiring additional external data. We provide models for all 50 languages of UD 2.0, and furthermore, the pipeline can be trained easily using data in CoNLL-U format. UDPipe is a standalone application in C++, with bindings available for Python, Java, C\# and Perl. In the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, UDPipe was the eight best system, while achieving low running times and moderately sized models.},
  eventtitle = {{{CoNLL}} 2017},
  file = {/media/cloud/Zotero/storage/T54CDDWU/Straka_Straková_2017_Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.pdf}
}

@incollection{stubbs2010,
  title = {Three concepts of keywords},
  booktitle = {Keyness in texts},
  author = {Stubbs, Michael},
  editor = {Bondi, Marina and Scott, Mike},
  date = {2010},
  series = {Studies in corpus linguistics},
  number = {v. 41},
  publisher = {{John Benjamins Pub. Co.}},
  location = {{Amsterdam ;}},
  url = {http://www.dawsonera.com/abstract/9789027287663},
  urldate = {2023-04-01},
  isbn = {978-90-272-8766-3 978-1-282-89733-5},
  langid = {Engels},
  volumes = {1 online resource (vi, 251 pages) : illustrations}
}

@article{suhartono2020a,
  title = {Argument Annotation and Analysis Using Deep Learning with Attention Mechanism in {{Bahasa Indonesia}}},
  author = {Suhartono, Derwin and Gema, Aryo Pradipta and Winton, Suhendro and David, Theodorus and Fanany, Mohamad Ivan and Arymurthy, Aniati Murni},
  date = {2020-10-19},
  journaltitle = {Journal of Big Data},
  shortjournal = {Journal of Big Data},
  volume = {7},
  number = {1},
  pages = {90},
  issn = {2196-1115},
  doi = {10.1186/s40537-020-00364-z},
  url = {https://doi.org/10.1186/s40537-020-00364-z},
  urldate = {2023-06-29},
  abstract = {Argumentation mining is a research field which focuses on sentences in type of argumentation. Argumentative sentences are often used in daily communication and have important role in each decision or conclusion making process. The research objective is to do observation in deep learning utilization combined with attention mechanism for argument annotation and analysis. Argument annotation is argument component classification from certain discourse to several classes. Classes include major claim, claim, premise and non-argumentative. Argument analysis points to argumentation characteristics and validity which are arranged into one topic. One of the analysis is about how to assess whether an established argument is categorized as sufficient or not. Dataset used for argument annotation and analysis is 402 persuasive essays. This data is translated into Bahasa Indonesia (mother tongue of Indonesia) to give overview about how it works with specific language other than English. Several deep learning models such as CNN (Convolutional Neural Network), LSTM (Long Short-Term Memory), and GRU (Gated Recurrent Unit) are utilized for argument annotation and analysis while HAN (Hierarchical Attention Network) is utilized only for argument analysis. Attention mechanism is combined with the model as weighted access setter for a better performance. From the whole experiments, combination of deep learning and attention mechanism for argument annotation and analysis arrives in a better result compared with previous research.},
  keywords = {Argument analysis,Argument annotation,Attention mechanism,Bahasa Indonesia,Deep learning,NOG-LEZEN},
  file = {/media/cloud/Zotero/storage/U84CX3IA/Suhartono et al_2020_Argument annotation and analysis using deep learning with attention mechanism.pdf;/media/cloud/Zotero/storage/75KGVSPV/s40537-020-00364-z.html}
}

@book{sutinen2021,
  title = {Digital {{Theology}}: {{A Computer Science Perspective}}},
  shorttitle = {Digital {{Theology}}},
  author = {Sutinen, Erkki and Cooper, Anthony-Paul},
  date = {2021-07-27},
  publisher = {{Emerald Group Publishing}},
  abstract = {Digital Theology is a rapidly emerging field of academic research and gaining traction with scholars of Computer Science, Theology, Sociology of Religion and the wider Humanities. This book explores Digital Theology from a Computer Science perspective, providing a comprehensive definition of the subject and setting the agenda for future work in the field for both academics and practitioners. A range of Digital Theology case studies highlight the challenges, and successes, and the lessons learned which can be applied to future situations. The book also includes a timely analysis of the role that digital technology has played in the response of the global church to specific world events; clarifying a number of turning points which have driven dramatic and rapid change in church operating models.},
  isbn = {978-1-83982-536-1},
  langid = {english},
  pagetotal = {107},
  keywords = {NU-LEZEN,Social Science / Anthropology / Cultural \& Social,Social Science / Media Studies,Social Science / Sociology / General,Social Science / Sociology of Religion,Social Science / Technology Studies},
  file = {/media/cloud/Zotero/storage/BKQ2RQWD/Digital_Theology_A_Computer_Science_Perspective_----_(5_What_Might_the_Future_of_Digital_Theology_Look_Like_).pdf;/media/cloud/Zotero/storage/CKBTVL37/Digital_Theology_A_Computer_Science_Perspective_----_(6_Conclusion).pdf;/media/cloud/Zotero/storage/N9YHIFNK/Digital_Theology_A_Computer_Science_Perspective_----_(4_How_to_Research_Digital_Theology_).pdf;/media/cloud/Zotero/storage/PWYAZV2M/__Sutinen_Cooper_2021_Digital Theology.pdf;/media/cloud/Zotero/storage/RRKNIBQ5/Digital_Theology_A_Computer_Science_Perspective_----_(References).pdf;/media/cloud/Zotero/storage/WC934MS6/Digital_Theology_A_Computer_Science_Perspective_----_(3_Why_Explore_Digital_Theology_).pdf;/media/cloud/Zotero/storage/WEK4GQDT/Digital_Theology_A_Computer_Science_Perspective_----_(1_Introduction_Towards_A_Dialogue_Of_The_Theological_And_The_Computati...).pdf;/media/cloud/Zotero/storage/YZMU5P9Z/Digital_Theology_A_Computer_Science_Perspective_----_(2_What_is_Digital_Theology_).pdf}
}

@article{torruella2013,
  title = {Lexical {{Statistics}} and {{Tipological Structures}}: {{A Measure}} of {{Lexical Richness}}},
  shorttitle = {Lexical {{Statistics}} and {{Tipological Structures}}},
  author = {Torruella, Joan and Capsada, Ramon},
  date = {2013-10},
  journaltitle = {Procedia - Social and Behavioral Sciences},
  shortjournal = {Procedia - Social and Behavioral Sciences},
  volume = {95},
  pages = {447--454},
  issn = {18770428},
  doi = {10.1016/j.sbspro.2013.10.668},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1877042813041888},
  urldate = {2023-03-27},
  abstract = {For some time now research has been carried out in the field of lexicometry into the statistical indices that enable lexical richness to be evaluated. The main problem lies in the fact that there should be no influence at all in the results of the formula of the length of the text in terms of the number of words it contains. Therefore, different indices have been designed, which are increasingly complex and sophisticated. This work is a review of the most important indices for calculating lexical richness, in order of complexity, looking into whether or not they are dependent on text length and a comparative analysis of the results of the different indices for different text types is presented.},
  langid = {english},
  keywords = {GELEZEN},
  file = {/media/cloud/Zotero/storage/9YG9F6D8/Torruella en Capsada - 2013 - Lexical Statistics and Tipological Structures A M.pdf}
}

@article{trilling2018,
  title = {Automatische Inhoudsanalyse van {{Nederlandstalige}} Data: {{Een}} Overzicht En Onderzoeksagenda},
  shorttitle = {Automatische Inhoudsanalyse van {{Nederlandstalige}} Data},
  author = {Trilling, D. and Boumans, J.},
  date = {2018},
  journaltitle = {Tijdschrift voor Communicatiewetenschap},
  volume = {46},
  url = {https://dare.uva.nl/search?identifier=b369613f-ba69-41ed-97e2-6e4dcf55cc29},
  urldate = {2022-02-17},
  langid = {english},
  file = {/media/cloud/Zotero/storage/SYJ59XPM/Trilling_Boumans_2018_Automatische inhoudsanalyse van Nederlandstalige data.pdf;/media/cloud/Zotero/storage/AHLGEDJ6/search.html}
}

@article{tutika2019,
  title = {Restaurant Reviews Classification Using {{NLP Techniques}}},
  author = {Tutika, Anuradha and Nagesh, M Y V},
  date = {2019},
  journaltitle = {Journal of Information and Computational Science},
  volume = {9},
  number = {11},
  pages = {8},
  abstract = {One of the most effective tools any restaurant has is the ability to track food and beverage sales daily. Currently, Recommender systems plays an important role in both academia and industry. These are very helpful for managing information overload. In this paper, we applied machine learning techniques for user reviews and analyze valuable information in the reviews. Reviews are useful for making decisions for both customers and owners. We build a machine learning model with Natural Language Processing techniques that can capture the user's opinions from users’ reviews. For experimentation, the python language was used.},
  langid = {english},
  file = {/media/cloud/Zotero/storage/IXP9RAPZ/Tutika en Nagesh - 2019 - Restaurant reviews classification using NLP Techni.pdf}
}

@article{tweedie1998,
  title = {How {{Variable May}} a {{Constant}} Be? {{Measures}} of {{Lexical Richness}} in {{Perspective}}},
  shorttitle = {How {{Variable May}} a {{Constant}} Be?},
  author = {Tweedie, Fiona J. and Baayen, R. Harald},
  date = {1998-09-01},
  journaltitle = {Computers and the Humanities},
  shortjournal = {Computers and the Humanities},
  volume = {32},
  number = {5},
  pages = {323--352},
  issn = {1572-8412},
  doi = {10.1023/A:1001749303137},
  url = {https://doi.org/10.1023/A:1001749303137},
  urldate = {2023-03-25},
  abstract = {A well-known problem in the domain of quantitative linguistics and stylistics concerns the evaluation of the lexical richness of texts. Since the most obvious measure of lexical richness, the vocabulary size (the number of different word types), depends heavily on the text length (measured in word tokens), a variety of alternative measures has been proposed which are claimed to be independent of the text length. This paper has a threefold aim. Firstly, we have investigated to what extent these alternative measures are truly textual constants. We have observed that in practice all measures vary substantially and systematically with the text length. We also show that in theory, only three of these measures are truly constant or nearly constant. Secondly, we have studied the extent to which these measures tap into different aspects of lexical structure. We have found that there are two main families of constants, one measuring lexical richness and one measuring lexical repetition. Thirdly, we have considered to what extent these measures can be used to investigate questions of textual similarity between and within authors. We propose to carry out such comparisons by means of the empirical trajectories of texts in the plane spanned by the dimensions of lexical richness and lexical repetition, and we provide a statistical technique for constructing confidence intervals around the empirical trajectories of texts. Our results suggest that the trajectories tap into a considerable amount of authorial structure without, however, guaranteeing that spatial separation implies a difference in authorship.},
  langid = {english},
  keywords = {lexical statistics,Monte Carlo methods,vocabulary richness},
  file = {/media/cloud/Zotero/storage/5DP4L6WV/Tweedie_Baayen_1998_How Variable May a Constant be.pdf}
}

@article{vanatteveldt2021,
  title = {The {{Validity}} of {{Sentiment Analysis}}: {{Comparing Manual Annotation}}, {{Crowd-Coding}}, {{Dictionary Approaches}}, and {{Machine Learning Algorithms}}},
  shorttitle = {The {{Validity}} of {{Sentiment Analysis}}},
  author = {family=Atteveldt, given=Wouter, prefix=van, useprefix=true and family=Velden, given=Mariken A. C. G., prefix=van der, useprefix=true and Boukes, Mark},
  date = {2021-04-03},
  journaltitle = {Communication Methods and Measures},
  shortjournal = {Communication Methods and Measures},
  volume = {15},
  number = {2},
  pages = {121--140},
  issn = {1931-2458, 1931-2466},
  doi = {10.1080/19312458.2020.1869198},
  url = {https://www.tandfonline.com/doi/full/10.1080/19312458.2020.1869198},
  urldate = {2022-06-27},
  abstract = {Sentiment is central to many studies of communication science, from negativity and polarization in political communication to analyzing product reviews and social media comments in other sub-fields. This study provides an exhaustive comparison of sentiment ana­ lysis methods, using a validation set of Dutch economic headlines to compare the performance of manual annotation, crowd coding, numerous dictionaries and machine learning using both traditional and deep learning algorithms. The three main conclusions of this article are that: (1) The best performance is still attained with trained human or crowd coding; (2) None of the used dictionaries come close to acceptable levels of validity; and (3) machine learning, especially deep learning, substantially outperforms dictionary-based methods but falls short of human performance. From these findings, we stress the importance of always validating automatic text analysis methods before usage. Moreover, we provide a recommended step-bystep approach for (automated) text analysis projects to ensure both efficiency and validity.},
  langid = {english},
  keywords = {GELEZEN},
  file = {/media/cloud/Zotero/storage/5M4IVAZY/van Atteveldt e.a. - 2021 - The Validity of Sentiment Analysis Comparing Manu.pdf}
}

@book{vanderven1993h,
  title = {Practical {{Theology}}. {{An Empirical Approach}}},
  shorttitle = {Practical {{Theology}}},
  author = {family=Ven, given=J. A., prefix=van der, useprefix=true},
  date = {1993},
  publisher = {{Kok Pharos}},
  location = {{Kampen}},
  keywords = {GELEZEN,Important}
}

@book{veale2016,
  title = {Metaphor: {{A Computational Perspective}}},
  shorttitle = {Metaphor},
  author = {Veale, Tony and Shutova, Ekaterina and Klebanov, Beata Beigman},
  date = {2016-02-29},
  eprint = {_ripCwAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Morgan \& Claypool Publishers}},
  abstract = {The literary imagination may take flight on the wings of metaphor, but hard-headed scientists are just as likely as doe-eyed poets to reach for a metaphor when the descriptive need arises. Metaphor is a pervasive aspect of every genre of text and every register of speech, and is as useful for describing the inner workings of a "black hole" (itself a metaphor) as it is the affairs of the human heart. The ubiquity of metaphor in natural language thus poses a significant challenge for Natural Language Processing (NLP) systems and their builders, who cannot afford to wait until the problems of literal language have been solved before turning their attention to figurative phenomena. This book offers a comprehensive approach to the computational treatment of metaphor and its figurative brethren—including simile, analogy, and conceptual blending—that does not shy away from their important cognitive and philosophical dimensions. Veale, Shutova, and Beigman Klebanov approach metaphor from multiple computational perspectives, providing coverage of both symbolic and statistical approaches to interpretation and paraphrase generation, while also considering key contributions from philosophy on what constitutes the "meaning" of a metaphor. This book also surveys available metaphor corpora and discusses protocols for metaphor annotation. Any reader with an interest in metaphor, from beginning researchers to seasoned scholars, will find this book to be an invaluable guide to what is a fascinating linguistic phenomenon.},
  isbn = {978-1-62705-851-3},
  langid = {english},
  pagetotal = {162},
  keywords = {Computers / Artificial Intelligence / General,Computers / Artificial Intelligence / Natural Language Processing,Language Arts \& Disciplines / Linguistics / General,NOG-LEZEN},
  file = {/media/cloud/Zotero/storage/2MRBWQJW/Metaphor_A_Computational_Perspective_----_(Knowledge_Acquisition_and_Metaphor).pdf;/media/cloud/Zotero/storage/42C8ZCS9/Metaphor_A_Computational_Perspective_----_(Computational_Approaches_to_Metaphor_Theoretical_Foundations).pdf;/media/cloud/Zotero/storage/AUKCL4NL/Metaphor_A_Computational_Perspective_----_(Metaphor_Annotation).pdf;/media/cloud/Zotero/storage/SRGX474Q/Veale et al_2016_Metaphor.pdf}
}

@article{vicente2021,
  title = {Leveraging {{Machine Learning}} to {{Explain}} the {{Nature}} of {{Written Genres}}},
  author = {Vicente, Marta and Maestre, María Miró and Lloret, Elena and Cueto, Armando Suárez},
  date = {2021},
  journaltitle = {IEEE Access},
  volume = {9},
  pages = {24705--24726},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3056927},
  abstract = {The analysis of discourse and the study of what characterizes it in terms of communicative objectives is essential to most tasks of Natural Language Processing. Consequently, research on textual genres as expressions of such objectives presents an opportunity to enhance both automatic techniques and resources. To conduct an investigation of this kind, it is necessary to have a good understanding of what defines and distinguishes each textual genre. This research presents a data-driven approach to discover and analyze patterns in several textual genres with the aim of identifying and quantifying the differences between them, considering how language is employed and meaning expressed in each particular case. To identify and analyze patterns within genres, a set of linguistic features is first defined, extracted and computed by using several Natural Language Processing tools. Specifically, the analysis is performed over a corpora of documents-containing news, tales and reviews-gathered from different sources to ensure an heterogeneous representation. Once the feature dataset has been generated, machine learning techniques are used to ascertain how and to what extent each of the features should be present in a document depending on its genre. The results show that the set of features defined is relevant for characterizing the different genres. Furthermore, the findings allow us to perform a qualitative analysis of such features, so that their usefulness and suitability is corroborated. The results of the research can benefit natural language discourse processing tasks, which are useful both for understanding and generating language.},
  eventtitle = {{{IEEE Access}}},
  keywords = {Applied computing,communicative objectives,discourse analysis,Feature extraction,genre characterization,human language technologies,Linguistics,Market research,natural language processing,Natural language processing,Semantics,Task analysis,Tools},
  file = {/media/cloud/Zotero/storage/W9M9U2BE/Vicente et al_2021_Leveraging Machine Learning to Explain the Nature of Written Genres.pdf;/media/cloud/Zotero/storage/94ZGJASZ/9345774.html}
}

@book{wallis2020,
  title = {Statistics in {{Corpus Linguistics Research}}: {{A New Approach}}},
  shorttitle = {Statistics in {{Corpus Linguistics Research}}},
  author = {Wallis, Sean},
  date = {2020-11-23},
  publisher = {{Routledge}},
  location = {{New York}},
  doi = {10.4324/9780429491696},
  abstract = {Traditional approaches focused on significance tests have often been difficult for linguistics researchers to visualise. Statistics in Corpus Linguistics Research: A New Approach breaks these significance tests down for researchers in corpus linguistics and linguistic analysis, promoting a visual approach to understanding the performance of tests with real data, and demonstrating how to derive new intervals and tests. Accessibly written, this book discusses the ‘why’ behind the statistical model, allowing readers a greater facility for choosing their own methodologies. Accessibly written for those with little to no mathematical or statistical background, it explains the mathematical fundamentals of simple significance tests by relating them to confidence intervals. With sample datasets and easy-to-read visuals, this book focuses on practical issues, such as how to: • pose research questions in terms of choice and constraint; • employ confidence intervals correctly (including in graph plots); • select optimal significance tests (and what results mean); • measure the size of the effect of one variable on another; • estimate the similarity of distribution patterns; and • evaluate whether the results of two experiments significantly differ. Appropriate for anyone from the student just beginning their career to the seasoned researcher, this book is both a practical overview and valuable resource.},
  isbn = {978-0-429-49169-6},
  pagetotal = {382},
  file = {/media/cloud/Zotero/storage/4RYRPGP8/Wallis_2020_Statistics in Corpus Linguistics Research.pdf}
}

@incollection{watanabe2022a,
  title = {Text as {{Data}}},
  booktitle = {Elgar Encyclopedia of Technology and Politics},
  author = {Watanabe, Kohei},
  editor = {Ceron, Andrea},
  date = {2022},
  series = {Elgar Encyclopedias in the Social Sciences Series},
  pages = {94--97},
  publisher = {{Edward Elgar Publishing}},
  location = {{Northampton}},
  isbn = {978-1-80037-425-6},
  file = {/media/cloud/Zotero/storage/WKFRBQZW/Text-as-Data.pdf}
}

@article{welbers2017,
  title = {Text {{Analysis}} in {{R}}},
  author = {Welbers, Kasper and Van Atteveldt, Wouter and Benoit, Kenneth},
  date = {2017-10-02},
  journaltitle = {Communication Methods and Measures},
  volume = {11},
  number = {4},
  pages = {245--265},
  publisher = {{Routledge}},
  issn = {1931-2458},
  doi = {10.1080/19312458.2017.1387238},
  url = {https://doi.org/10.1080/19312458.2017.1387238},
  urldate = {2022-03-17},
  abstract = {Computational text analysis has become an exciting research field with many applications in communication research. It can be a difficult method to apply, however, because it requires knowledge of various techniques, and the software required to perform most of these techniques is not readily available in common statistical software packages. In this teacher’s corner, we address these barriers by providing an overview of general steps and operations in a computational text analysis project, and demonstrate how each step can be performed using the R statistical software. As a popular open-source platform, R has an extensive user community that develops and maintains a wide range of text analysis packages. We show that these packages make it easy to perform advanced text analytics.},
  keywords = {GELEZEN},
  file = {/media/cloud/Zotero/storage/DGGRXQFL/Welbers et al_2017_Text Analysis in R.pdf;/media/cloud/Zotero/storage/M6IV7ZRR/19312458.2017.html}
}

@article{werse2014,
  title = {The {{Preaching Power}} of {{Cardinal John Henry Newman}}},
  author = {Werse, Nicholas R.},
  date = {2014},
  journaltitle = {Practical Theology},
  volume = {7},
  number = {2},
  pages = {109--124},
  issn = {1756-073X},
  url = {https://www.academia.edu/6977157/The_Preaching_Power_of_Cardinal_John_Henry_Newman},
  urldate = {2023-03-31},
  abstract = {Cardinal John Henry Newman is remembered as one of the greatest preachers of the 19th century. His influence is unrivaled as he drew enormous crowds to hear his Sunday afternoon sermons at St. Mary’s at Oxford. Yet many historical accounts of his},
  langid = {english},
  keywords = {GELEZEN},
  file = {/media/cloud/Zotero/storage/Z9LR5P2H/Werse_2014_The Preaching Power of Cardinal John Henry Newman.pdf;/media/cloud/Zotero/storage/2R72CNZA/The_Preaching_Power_of_Cardinal_John_Henry_Newman.html}
}

@article{whissell2009,
  title = {Using the {{Revised Dictionary}} of {{Affect}} in {{Language}} to Quantify the Emotional Undertones of Samples of Natural Language.},
  author = {Whissell, Cynthia},
  date = {2009},
  journaltitle = {Psychological reports},
  volume = {105},
  number = {2},
  pages = {509--21},
  issn = {0033-2941},
  file = {/media/cloud/Zotero/storage/52K5E4HE/Whissell_2009_Using the Revised Dictionary of Affect in Language to quantify the emotional.pdf}
}

@inbook{wildman2021,
  title = {Advanced {{Computational Methods}}},
  booktitle = {The {{Routledge Handbook}} of {{Research Methods}} in the {{Study}} of {{Religion}}},
  author = {Wildman, Wesley J. and Diallo, Saikou Y. and Shults, F. LeRon},
  date = {2021-11-11},
  edition = {2},
  pages = {136--153},
  publisher = {{Routledge}},
  location = {{London}},
  doi = {10.4324/9781003222491-10},
  url = {https://www.taylorfrancis.com/books/9781003222491/chapters/10.4324/9781003222491-10},
  urldate = {2023-04-17},
  bookauthor = {Engler, Steven and Stausberg, Michael},
  isbn = {978-1-00-322249-1},
  langid = {english},
  keywords = {GELEZEN},
  file = {/media/cloud/Zotero/storage/Z4TJJ2JI/Wildman e.a. - 2021 - Advanced Computational Methods.pdf}
}

@article{williams2020,
  title = {Charles {{Spurgeon}}'s Contribution to Preaching in the 21st Century.},
  author = {Williams, Kevin},
  date = {2020},
  url = {https://urresearch.rochester.edu/institutionalPublicationPublicView.action?institutionalItemId=35270&versionNumber=1},
  urldate = {2023-03-13},
  file = {/media/cloud/Zotero/storage/FGIJNVVY/Williams_2020_Charles Spurgeon's contribution to preaching in the 21st century.pdf;/media/cloud/Zotero/storage/RGGVTTJQ/institutionalPublicationPublicView.html}
}

@inbook{winkler1989,
  title = {Aus Der {{Geschichte}} Der {{Predigt}} Und {{Homiletik}}},
  booktitle = {Handbuch Der {{Predigt}}. {{Voraussetzungen}}, {{Inhalte}}, {{Praxis}}},
  author = {Winkler, Eberhard},
  date = {1989},
  pages = {571--614},
  publisher = {{Evangelische Verlagsanstalt}},
  location = {{Berlin}},
  bookauthor = {Bieritz, Karl-Heinrich and Henkys, Jürgen and Jenssen, Hans-Hinrich and Kiesow, Ernst Rüdiger and Winkler, Eberhard}
}

@article{wu2023,
  title = {The {{Fundamental Principles}} of {{Corpus Linguistics}}. {{Tony McEnery}} and {{Vaclav Brezina}}},
  author = {Wu, Chenghui},
  date = {2023-06-01},
  journaltitle = {Digital Scholarship in the Humanities},
  shortjournal = {Digital Scholarship in the Humanities},
  volume = {38},
  number = {2},
  pages = {916--917},
  issn = {2055-7671},
  doi = {10.1093/llc/fqad031},
  url = {https://doi.org/10.1093/llc/fqad031},
  urldate = {2023-06-01},
  abstract = {In the past decades, corpus methods have been extensively utilized in the research among social scientists and linguists in digital humanities (Bach, 2019; Luhmann and Burghardt, 2021). However, there is still some variance and misuse in the way how research problems are approached in the field of corpus linguistics, as a dynamic and fast-developing perspective to studying language use. It is the case for some scholars that they choose the corpus method in order to ‘use the corpus’ rather than truly facilitate their research methodology. Focusing on a number of fundamental issues in corpus linguistics, this thought-provoking volume The Fundamental Principles of Corpus Linguistics, written by Tony McEnery and Vaclav Brezina, seeks to spell out the reasonable grounds of how corpus data can be most effectively relied on to advance our understanding of language in digital humanities.},
  file = {/media/cloud/Zotero/storage/DFVQ5DHJ/Wu_2023_The Fundamental Principles of Corpus Linguistics.pdf}
}

@book{wymer2021,
  title = {Introduction to {{Digital Humanities}}: {{Enhancing Scholarship}} with the {{Use}} of {{Technology}}},
  shorttitle = {Introduction to {{Digital Humanities}}},
  author = {Wymer, Kathryn C.},
  date = {2021-04-07},
  publisher = {{Routledge}},
  location = {{New York}},
  doi = {10.4324/9781003149378},
  abstract = {Introduction to Digital Humanities is designed for researchers, teachers, and learners in humanities subject areas who wish to align their work with the field of digital humanities. Many institutions are encouraging digital approaches to the humanities, and this book offers guidance for students and scholars wishing to make that move by reflecting on why and when digital humanities tools might usefully be applied to engage in the kind of inquiry that is the basis for study in humanities disciplines. In other words, this book puts the "humanities" before the "digital" and offers the reader a conceptual framework for how digital projects can advance research and study in the humanities. Both established and early career humanities scholars who wish to embrace digital possibilities in their research and teaching will find insights on current approaches to the digital humanities, as well as helpful studies of successful projects.},
  isbn = {978-1-00-314937-8},
  pagetotal = {106},
  keywords = {NU-LEZEN},
  file = {/media/cloud/Zotero/storage/9VKAH28J/Wymer - 2021 - Introduction to Digital Humanities Enhancing Scho.pdf}
}

@online{zotero-13215,
  title = {{{INCEpTION}} - {{Welcome}}},
  url = {https://inception-project.github.io/},
  urldate = {2022-01-31},
  abstract = {Towards an Infrastructure for the Distributed Exploration and Annotation of Large Corpora and Knowledge Bases},
  langid = {english},
  organization = {{INCEpTION}},
  file = {/media/cloud/Zotero/storage/T22PED3E/inception-project.github.io.html}
}

@online{zotero-15407,
  title = {John {{Henry Newman}}, Rhetorician and Preacher: {{An}} Analysis of Selected Sermons - {{ProQuest}}},
  shorttitle = {John {{Henry Newman}}, Rhetorician and Preacher},
  url = {https://www.proquest.com/openview/1f35639a8b491b3dff379fecda9377be/1?pq-origsite=gscholar&cbl=18750&diss=y},
  urldate = {2023-03-31},
  abstract = {Explore millions of resources from scholarly journals, books, newspapers, videos and more, on the ProQuest Platform.},
  langid = {english}
}

@online{zotero-18421,
  title = {The {{Spurgeon Library}} | {{Simeon}}, {{Spurgeon}}, and the {{Need}} for {{Preaching Models}}},
  url = {https://www.spurgeon.org/resource-library/blog-entries/simeon-spurgeon-and-the-need-for-preaching-models/},
  urldate = {2023-10-24},
  organization = {{The Spurgeon Center}}
}
