---
title: "Computational Sermon Analysis"
author: "Theo Pleizier"
date: "2023-12-15"
output:
  bookdown::word_document2: 
    global_numbering: TRUE
bibliography: chapter-csa.bib
suppress-bibliography: TRUE
csl: "zeitschrift-fur-religionswissenschaft-note.csl"
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "../../gen") })
---

# Introduction

Computational sermon analysis (CSA) locates sermon analysis within a wider family of digital research methods. In digital humanities these methods have different names, such as 'text mining'[@jockers2015], 'corpus linguistics'[@gries2016] or 'natural language processing'[@mcgillivray2020]. These methods have their origins in various disciplinary traditions, however, they all point to the interaction of the study of human discourse and computer science. Increasingly, computational methods are used to study sermons. Hans Malmström researched meta-discourse of three denominational types of sermons [@malmstrom2015;@malmstrom2015a]. Robert Boss and Michael keller applies digital humanities methods and visualisation techniques to a corpus with sermons by Jonathan Edwards [@boss2020; @keller2018]. Constantine Boussalis and others studied the nature of political speech in a dataset of over 110,000 US sermons [@boussalis2020]. Anne Agersnap created a corpus of over 11,000 Danish sermons [@agersnap2020] and locates the computational study of sermons in the interdisciplinary domain of sociology of religion (lived religion), homiletics (the empirical turn) and digital humanities (computational methods) [@agersnap2021a,14-16,36-37]. 

<!--Though preaching is primarily an oral event [@ong2002], a sermon can also be understood as a 'textual object'. This object is constructed by the preacher ('sermon preparation'), it is performed within the liturgical community ('the preaching event') and it is processed by listeners ('sermon reception'). As a *textual object*, the sermon is a complex whole of intertextual references, speech acts, and a mixture of religious and everyday language. Two features of the sermon are important to mention here. First, the content of a sermon is *structured language* [@grozinger2008b,177]. Second, a sermon is *subjective speech*, it reflects the personal style of a preacher [@engemann2019b,Chapter 2]. Further, sermons have time dimensions. They have a considerable length; this makes sermon analysis time-consuming. Further, a sermon is *weekly* speech.[^6] Only computers can process these enormous amount of textual data ('big data') and the field of *natural language processing* consists of methods to do so [@khurana2023]. -->

This chapter explores the potential of CSA. For that purpose a dataset is constructed from the sermons from John Henry Newman (1801-1890) and Charles Haddon Spurgeon (1834-1892), preachers that belong to the 'golden age of sermon culture in Britain' [@gibson2012]. Their sermons are available in the public domain. CSA provides new angles to study this material (section \@ref(casestudy)) as it combines two strands of research: sermon analysis and digital theology (section \@ref(digitaltheology)). The major part of the chapter illustrates various methods for data collection and analysis. Though Newman's and Spurgeon's sermons can be downloaded freely for research purposes, constructing a dataset (or 'corpus' as it is called in corpus linguistics) requires a specific type of workflow (section \@ref(workflow)). Further, two types of computational analyses are presented. First, basic operations in corpus linguistics are introduced to answer relative straightforward research questions on the use of language in sermons (section \@ref(basic)). Further, two advanced techniques demonstrate the application of artificial intelligence in sermon analysis. Following the typology of Van Attenveld and others, these techniques include inductive methods (unsupervised machine learning) and deductive methods (supervised machine learning) [@atteveldt2022,chapter 11.1] (section \@ref(advanced)). The chapter closes with a reflection on the implications and limitations of CSA for the study and theory of preaching (section \@ref(conclusions)). 

# A Computational Case Study: Preaching in the Victorian Era {#casestudy}

In the year Friedrich Schleiermacher died, Charles Haddon Spurgeon (1834-1892) was born. The popular Baptist preacher from London was a contemporary of John Henry Newman (1801-1890). Newman started as an Anglican parish priest and served as university chaplain in Oxford, but in 1845 he converted to Roman Catholicism.[^5]  

Spurgeon and Newman are among the most famous preachers in the Victorian Age. Their sermons have survived due to intentional publication strategies. The publication of sermons was part of Nineteenth century preaching culture [@gibson2012]. Several historical and homiletical studies researched aspects of Spurgeon's preaching [@edwards2004,455-461; @old2007,222-243]. Richard Lischer characterises his preaching as a combination of 'oratorical prowess and evangelical fervor with a deep concern for social issues' [@lischer2002b,316]. Spurgeon's sermons are included in many online repositories, such as the *Christian Classics Ethereal Library* (CCEL) which holds 63 volumes with full-text sermons in open access.[^2] Equally important as a preacher was John Henry Newman. Several researchers have tried to capture the uniqueness of his preaching [@chavasse2009;@kuczok2010;@robinson2009;@werse2014]. His sermons have been published according to the two phases of his ecclesial life: his Anglican sermons and those after he became Roman-Catholic in 1845. Newman's literary legacy has been digitally processed, including the handwritten manuscripts of his sermons.[^site] The fulltext, open source availability of his sermons, the *Parochial and Plain Sermons* (1834-1843) and some occasional sermons can be found at the digital repository *Newman Reader*. A dataset with 3,500+ (Spurgeon) and 217 (Newman) sermons provides an opportunity for CSA because the analysis requires the use of a computer. A CSA project starts with collecting the data. Downloading a large number of sermons can hardly be done manually; they need to be automatically scraped from websites.[^4] 

The assumption behind the project for this chapter depends on Albrecht Grözinger's  distinction between two levels of personal preaching, a conceptual and an actual level. 'I would like to be present in my sermon personally', is an example of the conceptual level. For the purposes of empirical research, however, the actual level is especially relevant. According to Grözinger, 'every preacher is present in preaching, independent whether the preacher is aware of this personal presence' [@grozinger2008b,132; See also @engemann2019b, Chapter 2]. In other words, it is an empirical fact that the subject of the preacher is embedded in the sermon. This leads to the following hypothesis: 

> *Hypothesis Preacher* The language of preaching reflects features of the preacher, the preacher's theology, spirituality and personality. 

The differences between the theological backgrounds of Spurgeon and Newman and the large number of sermons are ideal circumstances to explore this hypothesis. Therefore, the analysis starts with a comparison of preaching vocabularies (section \@ref(basic)), and it terminates in the development of an algorithm that is able to predict the name of the preacher (section \@ref(advanced)). First, however, we put computational research of sermons in the broader context of empirical sermon analysis and the rise of digital theology. 

[^site]: https://digitalcollections.newmanstudies.org/. The digital collection contains digitized books of Newman's sermons, all files contain photographs of the books with handwritten sermons. The sermons used for the analysis in this chapter are scraped from the freely available full-text repository at https://www.newmanreader.org/. 

# Digital theology and sermon analysis {#digitaltheology}

```{r init 02, message=FALSE, warning=FALSE, include=FALSE}
library(here)
source(here("src/05-analysis-basic","word_frequencies.R"))

```
The first approaches to the study of word frequencies in sermons can be traced back to 1973. The Homiletical Workgroup ('Homiletische Arbeitsgruppe') applied content analysis to sermons [@homiletischenarbeitsgruppe1973], 'a large-scale project that could only be carried out with the aid of a computer' [@engemann2019b,384]. Computer-assisted sermon analysis combines word frequencies (quantitative approach) with thematic coding (qualitative approach) [@pieterse1995]. 

At the turn of the century the availability of powerful personal computers and the accessibility of specialised software opened up new possibilities for research. For example, Georg and Matthias Ballod analysed a corpus of Pentecost sermons using software for calculating lexical diversity, word frequencies and key words. They performed various statistical calculations to study the readability of the sermons, the themes related to Pentecost, and the language that enables the listeners to be involved in the sermon  [@ballod2000]. Further, the results of the study of Pew Research demonstrated the relationship between denominations and the length and content of sermons in a wide variety of US churches based upon a dataset of almost 50,000 sermons. Its use of computational methods reflects the current state of research: 

> The custom-built program used a machine learning model to identify pages likely to contain sermons and a set of specially designed algorithms to collect media files with dates from those pages, identify the files containing sermons and transcribe those files for further analysis. [@pewresearchcenter2019,21]

Likewise, the analyses in this chapter have been created by small custom-built computer programs. Computer programming consists of a sequence of calculations that transform texts into numbers. Document-term-matrices and word-frequency-tables are examples of text-turned-into-numbers. First, a document-term-matrix (DTM) compresses a large corpus into one single matrix. These matrices are often used to apply machine learning tasks. Table \@ref(tab:dtm) shows an example of a DTM of five randomly sampled documents from the balanced corpus of Spurgeon's and Newman's sermons. It presents the documents (left column) in relation to the times the words occur in the document (other columns). 

```{r dtm, echo=FALSE, message=FALSE, warning=FALSE, out.width="100%"}
#example_dfm

knitr::kable(example_dfm, booktabs = TRUE, caption = "document-term-matrix") 

```
Unlike document-term-matrices, frequency tables are more straightforward representations of word counts in a corpus. After creating a balanced corpus of 56 sermons of Spurgeon and Newman based upon the same biblical texts[^balance], frequency tables can be calculated.

[^balance]: For the construction of the corpus, see section \@ref(workflow). The number of sermons in each corpus is very different (N = 3,516 for Spurgeon; N = 217 for Newman). Therefore, a sample is drawn from both corpuses, N = 150 for each preacher. Next, two other balanced corpora are created, based upon the same biblical texts (56 texts for each preacher) and upon the same bible chapters (148 chapters for each preacher). See the list of the biblical texts and chapters in both balanced corpora: https://github.com/ttjpleizier/computational-sermon-analysis/blob/master/data/list_chapters-texts.csv.

```{r topwords, echo=FALSE, message=FALSE, warning=FALSE, out.width = "100%"}
top_words$rank <- 1:15
top_words <- top_words[,c(7,1:6)]
knitr::kable(top_words, booktabs = TRUE, caption = "Top-15 words in balanced corpus with shared Biblical texts")

```

The first column in table \@ref(tab:topwords) represents the raw, unfiltered data. The fifteen most frequently used terms are listed in order of frequency, with 'the' being the most frequently used word in the corpus. However, this list is not very insightful as the terms correspond with the most used terms in the English language (column 1). Therefore, data scientists apply a filter with so-called stopwords. This results in a list of words indicating that we might be dealing with a the specific genre 'sermon' (column 2). Finally, the other columns identify the top-15 words in Spurgeon's and Newman's sermons respectively. The result provides a glance in the sermonic vocabulary of both preachers (column 3-6).

The table with word frequencies seems evident, yet it raises several issues. First, the sermons are not of equal length. Spurgeon's sermons are significantly longer than Newman's, the frequencies already suggest this. Second, the frequencies cover the sermons of each preacher taken from the 56 sermons that share the same biblical text. Third, there is something striking with two items in the list with Newman's words: the 'st' (term `r which(top_words$newman_only == "st")`) suggest a combination with other words (such as 'St. Paul') and 'God's' (term `r which(top_words$newman_only == "god's")`) is interpreted as a single token. The field of *quantitative corpus linguistics* [@gries2016] has solutions to deal with these issues. Compounds are a special kind of word-pairs (also called: bigrams). A frequency table of all compounds that start with 'st' shows indeed that Newman uses 'st' in combination with biblical saints (table \@ref(tab:compound)), with St. Paul being the champion with `r example_st$frequency[1]` occurrences in `r example_st$docfreq[1]` (out of 56) sermons. 

```{r compound, echo=FALSE, message=FALSE, warning=FALSE, fig.cap = "compound st_* in Newman", out.width = "100%"}

knitr::kable(example_st, booktabs = TRUE, caption = "Compound St_* in Newman")

```

Collocations are another central method in corpus linguistics: 'Collocations are co-occurrences of words, which are then referred to as *collocates*' [@gries2016,16]. A collocation display (table \@ref(tab:collocation)) with the term 'God's' shows the most frequent combinations, starting with "God's grace" which occurs `r example_coll$count[1]` times in the entire corpus of Newman's sermons.[^lambda]

[^lambda]: *Lamda* and *z* are statistical metrics concerning the associations. *Lambda* refers to the collocation's strength; it describes the probability that these terms appear in this order.   

```{r collocation, echo=FALSE, message=FALSE, warning=FALSE, out.width="100%"}

knitr::kable(example_coll, booktabs = TRUE, caption = "Collocation 'God's' in Newman")

```

Theologians are known with the phenomenon 'concordance'. In quantitative linguistics, concordances provide tables with keywords in context. For example, a sample of five occurrences of the keyword 'God's grace' in the corpus with Newman's sermons demonstrates how the keyword-in-context function enables the researcher to study keywords in their context (table \@ref(tab:kwic)).

```{r kwic, echo=FALSE, message=FALSE, warning=FALSE, out.width="100%"}
#flextable(as.data.frame(example_kwic)) %>% 
#  autofit() %>% 
#  delete_columns(j="pattern")

example_kwic_df <- as.data.frame(example_kwic)
knitr::kable(example_kwic_df[,-7], booktabs = TRUE, caption = "Concordance of 'God's grace' in Newman")
```


In sum, frequency tables entail more complex techniques than simple word counts. Further, the examples of 'st' and 'God's' also show that the term 'word' is not unambiguous.

Word frequencies, stop-words, tokens, machine learning, document-term-matrices and collocations. A computational approach adds new terminology to practical theology, often derived from the conglomerate of disciplines and methods that shaped digital humanities in the last decades. These methods have in common that they provide the research community with new empirical approaches, often relying upon statistical techniques. However, the developments are still very recent and the distinction between disciplines remains fluid and fuzzy [@wymer2021,2]. The digital and the computational are distinct according to this helpful description: 'the term *digital* refers to information in binary form while *computational* refers to processes performed by algorithms' [@drucker2021,1] 

In the pioneering work of Peter Phillips and others digital theology is described according to four 'waves'. They distinguish between: 1. the use of digital theology in teaching; 2. theological research enabled by digitality; 3. reflexive theologically-resourced engagement with digitality; and 4. a prophetic re-appraisal of digitality [@phillips2019, 37-40]. Erkki Sutinen and Anthony-Paul Cooper understand digital theology as the dialogue between theology and computer science. They specifically locate digital theology in *practices*, namely the 'interrelatedness of faith and its tangible expressions' [@sutinen2021,1]. Stefan Karcher proposes a similar understanding of the relationship between digital humanities and theology. Besides the study of online communication and the historical texts, Karcher specifically mentions sermon analysis as an example in which digital humanities and (practical) theology fruitfully interact [@karcher2020,141].


# A Workflow to Create a Corpus: Programming, Pipelines and Preparing Data {#workflow}

In their article 'Sermons as Data', Anne Agersnap and others describe the construction of a corpus of sermons [@agersnap2020]. They describe the three stages of sampling, cleaning and annotating, common in a data science workflow. The workflow starts with acquiring the raw data. Next, the raw data is cleaned to create a consistent dataset. Finally, the data is annotated with so-called metadata. Each of these operations is done by a sequence of small computer programs ('code'), a list of so-called pipelines. This section introduces programming, pipelines and preparation of data. By way of practical illustration, it presents the various steps to create a corpus of the sermons by Newman and Spurgeon, the dataset that will be used in the various analyses that are presented in the next sections. 

## Programming

Computational sermon analysis includes selecting and learning a *programming language*. Instead of using existing software suites for textanalysis, such as *LancsBox* or *AntConc*[^freewaretools], computational analysts often use one - or a combination - of two major programming languages: Python[^python] or R[^R]. The examples presented below use R[@rcoreteam2021], a language developed for statistical analyses and widely used in academic research. R is open source and it has many community-developed libraries to add specific functionalities; its documentation is amply available. It can be used to read all sorts of database-structured files or to scrape data from websites.[^start] After installing R we need to install the libraries that we need for the examples in this chapter, especially *Qanteda*.[^quanteda] 

[^freewaretools]: http://corpora.lancs.ac.uk/lancsbox/; https://www.laurenceanthony.net/software/antconc/.
[^python]: https://www.python.org/. 
[^R]: https://cran.r-project.org/. R is often used in a integrated develop environment (IDE) RStudio: https://posit.co/products/open-source/rstudio/. 
[^start]: A good starting point to learn R is *R for Data Science*, https://r4ds.had.co.nz/. An introduction to R specifically focusing upon quantitative linguistics is @gries2016. An accessible introduction in using both R and Python in digital humanities with emphasis on communication, see @atteveldt2022. 
[^quanteda]: *Quanteda* is a toolbox for quantitative analysis of textual data, see @benoit2018. 

First, the data need to be fetched from a local repository or a source that is available online. A script downloads or scrapes the data from the internet. It requires a little research where the sermons can be found and how they are stored on the servers. It appears that Spurgeon's sermons are stored at the *Christian Classics Ethereal Library* website in 63 volumes containing more than 3,500 sermons; the volumes can be downloaded as separate textfiles. So we need a script that downloads all 63 textfiles in one flow. Newman's sermons can be found at *Newman Reader*, the indexes of 8 volumes of the *Parochial and Plain Sermons* can be found on separate pages; a 9th volume with occasional sermons (*Sermons on Subjects of the Day*) contains an additional 26 sermons. In total 217 sermons by Newman are available in plain-text in the public domain. The scripts that download the sermons consists of a larger 'loop' that walks through all the pages, downloads the file or scrapes the webpage and stores the content into separate text-files. 

The R-program that scrapes Newman's sermons looks like this: 

```
# load packages
library(here)
library(rvest)
library(stringr)

# input: construct urls for the 8 volumes with sermons
volumes <- 8
newmanurl <- "https://www.newmanreader.org/works/parochial/volume"
newmanurls <- paste0(newmanurl,1:volumes,"/")
outputpath <-"gen/newman/txt/" 
if(!dir.exists(here(outputpath))) dir.create(here(outputpath), recursive = TRUE)

# loop over the volumes to find urls for the sermons
for (u in seq_len(volumes)){
  newman_volume <- read_html(newmanurls[u])
  sermonlist <- newman_volume %>% html_nodes("a") %>% html_attr("href")
  sermonlist <- sermonlist[str_detect(sermonlist,"sermon\\d")]
  sermonlist <- sermonlist[!is.na(sermonlist)]
  sermonurls <- paste0(newmanurls[u], sermonlist)

  # loop over the urls of the sermons to scrape the sermon
  for(s in seq_along(sermonurls)){
    newman_sermon <- read_html(sermonurls[s])
    sermontext <- newman_sermon %>% html_text2()
    sermonfile <- paste0(str_pad(u,2, pad = "0"),"_",
                         str_pad(s,2, pad = "0"),".txt")
                         
    # output: store the sermon in a file on the computer
    writeLines(sermontext, here(outputpath, sermonfile))
  }
}

```

Comments in the code (prefixed with a hash-sign: #) explain what the code is doing. For reasons of readability and brevity, the next examples will present pseudocode:[^8] 

```
SET number_of_volumes to 8
COMBINE the main-url with the number_of_volumes urls
FOR each volume 
	READ content of the url
	EXTRACT list of urls that contain individual sermons
	FOR each sermon-url
		READ content of the sermon-url
		SET filename to volume_sermon.txt
		SAVE content of sermon-url to filename
	ENDFOR
ENDFOR
```

The code output is a list with 217 text-files, each containing one of Newman's sermons. A similar program is written to download ('scrape') the sermons by Spurgeon. The resulting textfiles, however, need a lot of data cleaning. They contain information that not properly belongs to the sermon, such as a sermon number, a title, the biblical text, a copyright statement and footnotes. This information must be removed from the files to analyse the sermon's text. Further, some of this extra information is worth keeping, such as the date when the sermon was preached, the biblical text or the number of the sermon and the volume where it came from. This is called 'meta-data'. To extract the meta-data from the files and to clean the files from information that is not part of the actual sermon, we need a new part of our computational pipeline. 

## Pipelines

Like any empirical method a computational approach has a structural logic and a toolbox. This logic in is expressed in so-called 'pipelines'; a sequence of instructions that a computer can process, usually starting with 'input', followed by a sequence of operations, and resulting in 'output'. The output becomes 'input' for a new pipeline.[^goodpractice] For example, in their study of 11,955 sermons Anne Agersnap and Kristoffer Laigaard Nielbo describe three computational pipelines, a pipeline for wordcount, a pipeline for seeded semantic network analysis, and a pipeline for topic modeling. These pipelines consists of several steps, like tokenization, frequency tables, word embeddings, concatenation and visualisations [@agersnap2022]. Pipelines structure a computational workflow. A simple example of a pipeline - or actually a stage in a larger pipeline - is the following: 

```
# input: read a folder with textfiles
sermon_texts <- readtext("gen/newman/txt/*")

# apply the function 'corpus' from the quanteda-package
sermon_corpus <- quanteda::corpus(sermon_texts) 

# output: save the corpus in a file
save(sermon_corpus, file = "gen/newman_corpus") 

```

These lines of code take a folder with a large collection of text-files as its *input* and stores it in an object called 'sermon_texts'. Next, a function is applied to the object, namely to store the entire collection of text-files in a special format, a 'corpus' format. The function is part of the 'Quanteda' package. Finally, the corpus-object is saved to a file called 'newman-corpus'. This file is the *output*. This object can be loaded as input for a new series of operations. For the Victorian-preaching project the following pipeline was created to prepare the data for analysis:

1. fetch data: download the volumes with sermons from Spurgeon and Newman
2. process data: extract the text of the sermons from the volumes, based upon headings and footers. Some information (such as dates, biblical texts) is stored as meta-data.
3. clean data: remove internal references such as footnotes and biblical texts from the sermons. Correct errors in the Spurgeon-files based upon the downloaded index-file.
4. build corpus: two large corpus-files are built from the two sets of text-files. The Spurgeon-corpus consists of 3,516 sermons, the Newman-corpus contains 217 sermons. Balanced corpus files are created based upon a sample with sermons by both preachers, and upon sermons that that share the same biblical text. 

This pipeline is common for a computational project, but the individual scripts are adapted to the specific datasets, document variables of a corpus, analytic goals or reporting purposes, and based upon research questions. Here, computational analysis does not differ from any methodology that includes the steps of data collection, data analysis and reporting data. 

[^goodpractice]: A good practice in working with pipelines is a standard organisation of files in folders, for instance datasets in `/data`,  scripts in `/src`, and the generated output in `/gen`. This description is based upon the TIER protocol: https://www.projecttier.org/tier-protocol/protocol-4-0/. The data for our example of Victorian preaching is all fetched through a script, hence the output is stored in `/gen/newman` and `/gen/spurgeon` respectively. The data-folder contains only one file: an index of all 3,500+ Spurgeon-sermons was found online, created in 2012 by Mark Barnes. Barnes' index can be found on: https://community.logos.com/forums/t/44342.aspx (accessed 10-4-2023). The index was used for automatic collection of meta-data. 

## Preparing data

After fetching the data from websites, the data should be prepared for analysis. Sermons are large texts, consisting of many words and sentences. Often the texts are not entirely consistent or contain words or sentences that do not properly belong to the sermon, such as a brief heading that explains the situation in which a sermon was preached or a few tail-lines that contain information on the publication of the sermon. Data preparation aims to create a coherent and balanced dataset. Finally, a corpus can be created.  

In general, data preparation consists of three steps. First, the downloaded and scraped sermons stored in single files (*fetch data*) are processed. Each file is automatically checked. Headings and footers are removed, according to rules defined after manually inspecting a few single files. In the case of the sermons of Newman and Spurgeon, the headings contain relevant metadata. For instance, the raw data - one of the Spurgeon-files - starts with the following header: 

```
                           Characteristics of Faith

   A Sermon

   (No. 317)

   Delivered on Sabbath Morning, May 27th, 1860, by the

   REV. C.H. SPURGEON

   At Exeter Hall, Strand.

   "Then said Jesus unto him, Except ye see signs and wonders, ye will not
   believe."--John 4:48.

   YOU WILL REMEMBER that Luke, in his letter to Theophilus, speaks of
   things which Jesus began both to do and to teach, as if there was a
   connection between his doings and his teachings. In fact, there was a
```

The actual sermon starts with the sentence: 'You will remember' Yet, the other lines contain relevant information: the title of the sermon (*Characteristics of Faith*), the date of preaching (*May 27th, 1860*), the place (*Exeter Hall*) and the Biblical text (*John 4:48*). A random check learns that this structure is similar for each sermon. It appears that the Biblical texts mark the end of the headings. A script determines the line where the heading ends and the sermon starts. The script defines a function to find the biblical reference at the start of each new sermon: 

```
DEFINE pattern to extract Biblical text
DEFINE function fetch_volume
	(code)
DEFINE function fetch_sermon
	(code)
DEFINE function find_scripture 
	CALL fetch_volume to select volume-file
	SELECT sermon from volume
	FOR each line of the sermon until the 90th line
		IF line contains pattern THEN
			biblical text found
			STORE line number
			break loop
		ENDIF
	ENDFOR
	
```

Another script extracts the metadata (title, date, place, text) from the heading and stores it in a table. For Newman, an additional table is needed to reconstruct the dates of the sermons. This table is available online, so the script contains code to connect the sermons with the dates earlier researchers have reconstructed.[^newmandates] For each sermon, two dates are added: when the sermon was preached (date of delivery) and when the sermon was published (date of print). In the case of Spurgeon's sermons the dates of the sermons are a bit more complex. Quite a few sermons include an extra date of publication. This date refers to the date when the sermon was published in a magazine before it was added to the collection of printed sermons. Further, Spurgeon's sermons often indicate whether the sermon was preached on a Sunday or a weekday (15%). Time indicators are missing from 583 sermons (16.5%). 

[^newmandates]: See for the table with sources for the reconstruction of the dates: https://www.newmanreader.org/controversies/guides/dates.html, accessed 4-12-2023. 

Second, the processed files are cleaned. It appears that there is material in the files that should be removed, such as footnotes added by the publisher, biblical references between brackets (in Newman's sermons) or - as it turned out - larger parts called 'expositions' (in Spurgeon's sermons). Though manual checks find those out, it is undoable to check and clean over 3,000 files. Again, a script that attempts to remove these pieces from the text based upon textual markers is written. For instance, 'expositions' always start with the word 'Exposition' at the beginning of a new paragraph and footnotes are surrounded by square brackets. 

The third and final part of the pipeline 'data preparation' is the creation of single files with the sermons. Based upon the tables with metadata the sermons are extracted from the raw data.  This results in 217 text-files with sermons by John Henry Newman and 3,516 text-files with sermons by Charles Haddon Spurgeon. Now we are ready to create a corpus, or actually, multiple corpora.

<!-- A corpus is a dataset. Yet it is different in size from a dataset that is collected or generated for qualitative analysis: -->

<!--
> The set of texts or *corpus* dealt with is usually of a size which defies analysis by hand and eye alone within any reasonable timeframe. It is the large scale of the data used that explains the use of machine-readable text.[@hardie2011]
-->

Stefan Gries provides a definition of a 'corpus' within linguistics: 

> a machine-readable collection of (spoken or written) texts that were produced in a natural communicative setting, and in which the collection of texts is compiled with the intention (1) to be representative and balanced with respect to a particular linguistic language, variety, register, or genre and (2) to be analyzed linguistically. [@gries2016,7]

In computational analysis, a corpus usually refers to a single object that the computer can read and analyse, rather than a folder full of files. Therefore, we need to combine all the single files into one object (a 'corpus'). Usually, relevant metadata is added to the corpus. Metadata can be used to make specific selections from the corpus, such as all sermons by Newman, preached in 1843. Based upon the metadata of both corpora, it is possible to compare the sermons by both preachers in relation to the Biblical texts. A comparison of both corpora demonstrates that both preachers have sermons on 56 similar biblical texts and 148 similar biblical chapters. Hence, in the final stage of creating a corpus, three different balanced corpora are created: (1) a random sample of 150 sermons by each preacher; (2) 56 sermons on the same biblical texts by each preacher; (3) 148 sermons on the same biblical chapter by each preacher. The randomly sampled balanced corpus is created according to the following instructions:

```
LOAD corpus-spurgeon
LOAD corpus-newman
ADD document variable with the name of the preacher to both corpora
SET sample-length to 150
DRAW sample from corpus-newman with size sample-length
DRAW sample from corpus-spurgeon with size sample-length
CREATE corpus-balanced by combining both samples
```

Data preparation finishes with the creation of a corpus. Finally, the phase of analysis can start. The next two sections present a selection of analytic techniques to study the data These techniques explore the data, they generate new hypotheses and research questions, and they generate models. 

# Basic Operations in Corpus Linguistics {#basic}

Corpus linguistics is about frequencies [@gries2016,141]. Frequencies are often based upon single words ('tokens') or combinations of words ('n-grams'). Tokens differ from words in so far that for tokens specific decisions have to be made about what counts as the most basic linguistic unit. For example, do we treat the unit 'God's' - as it occurs relatively often in Newman's sermons - similarly to the noun (God) or does the analyst take it as part of a compound, such as in 'God's grace'?  Collocational and concordance analysis can shed more light on this (see tables \@ref(tab:collocation) and \@ref(tab:kwic) above). They are among the four basic methods for computational linguistics: frequency analysis ('tokens'), dispersion information, lexical co-occurrence (collocations) and grammatical co-occurance (concordance) [@gries2016,chapter 2]. These techniques are foundational to perform more advanced statistical operations, such as *keyness* (determining key-terms), *lexical diversity* (measuring lexical variety) or *document similarity* (clustering documents). 

<!--
Often tokenization - the breaking up of a corpus in single 'words' - is followed by automatic tagging. For example, part-of-speech (POS) tagging and Named-entity-recognition (NER) tagging add grammatical information (POS) to or identifies proper names (NER) in the data. Tagging is often done by trained algorithms and is language-dependent. For sermon analysis, POS tagging makes it possible to study the use of nouns or verbs in sermons and NER tagging helps to identify proper names in sermons to study how biblical characters are portayed in sermons, how Trinitarian language is distributed in the sermon or how names of contemporary persons or places function in homiletic discourse. 
-->

## Exploratory Data Analysis


```{r eda, message=FALSE, warning=FALSE, include=FALSE}
source(here::here("src/05-analysis-basic","eda.R"))
```

Data science and computational analyses often start with exploratory data analysis (EDA), invented by John Tukey in 1977. The researcher needs to know the data before more complex analyses are performed. EDA often involve frequency tables (see section \@ref(digitaltheology)), descriptions of document variables of a corpus, and visualisations of the data. EDA can result in more complex statistical operations such as cluster analysis [@atteveldt2022,chapter 7.3]. Grolemund and Hadley describe EDA as an iterative cycle from generating questions about the data, through searching for answers by visualising; transforming, and modeling data, to refine questions and/or generate new questions [@grolemund2017,chapter 7]. This section presents four examples of EDA: a simple table that counts sermons, a visualisation of the length of sermons over time, a lexical dispersion plot, and a visualisation of the different vocabularies of the preachers (keyness). 

### Counting sermons

A simple question in quantitative studies is: how many? The Spurgeon-corpus contains sermons from 1854 to 1891, 37 years of preaching ministry. Being a Baptist, Spurgeon didn't necessarily follow a lectionary. His choice of texts therefore gives an impression of his theology. So, we ask how many of his sermons are from the same biblical chapter? A small script is written to make the calculation.

```
LOAD the Spurgeon-corpus
TRANSFORM the column with Scripture-references into chapter-references
	REMOVE from the references everything after the verse indicator ":" 
	STORE the references in book-chapter variable
CREATE a table that counts how often chapters are referred to 
SORT the table in decreasing order (high values first)
PRINT the top of the table (20 sermons or more)
```

The resulting table \@ref(tab:most-preached) is already insightful. Nine biblical chapters have been preached from 20 or more times. Spurgeon preferred to preach from New Testament passages, with the book of Romans and the gospel of John being his favorites. Psalm 119 clearly was his most often used passage to preach from. Theologically, it is striking that two passages linked to the classic doctrine of atonement (Isaiah 53 and 2 Corinthians 5) are among the most preached passages. 

```{r most-preached, echo=FALSE, message=FALSE, warning=FALSE}

knitr::kable(most_preached, caption = "Biblical chapters with 20 or more sermons")

```

### Sermon length over time

'How many' could also mean: 'how many words per sermons'. From the assumption that evangelical-baptist preaching may result in longer sermons than Anglican preaching, the hypothesis could be stated that Spurgeon's sermons are longer than Newman's. A small script calculates the mean length of all the sermons by one preacher grouped per year. 

```
SELECT columns in dataframe with all sermons: preacher, tokens, year
GROUP by preacher and year
SUMMARIZE mean of tokens per year
PLOT for each preacher
  X-axis year
  Y-axis mean length in tokens
```

Figure \@ref(fig:length-time) visualises the sermons grouped by preacher and projected for each year mean length of the sermons.  Given the number of sermons (18 years of preaching by Newman, 37 years by Spurgeon), the plot also provides insights in the developments of the length of sermons over time.


```{r length-time, echo=FALSE, message=FALSE, warning=FALSE, out.width="100%", fig.cap="Sermon length during Newman's and Spurgeon's preaching careers"}

#plot_sermon_length

knitr::include_graphics(here("gen/images","plot_sermon_length.tiff"))

```

The plot confirms the assumption about the length of preaching. Over the years Spurgeon's sermons are considerably longer than Newman's sermons. Newman's sermons have length between 3600-4600 tokens, while Spurgeon's sermons range between 6250-8000 tokens. The plot also reveals that in Newman's corpus the sermons become slightly longer after 1832. The first sermon in 1833 was preached in July 1833 when Newman had returned from a trip to the continent. It suggests that after his journey he started to preach longer sermons. The plot of Spurgeon's sermons shows a significant decrease of length around 1891. Spurgeon fell ill in 1891 and died in January 1892. His sermons continued to be read until 1905. Since most of these sermons were also preached earlier, it seems that from the published sermons shorter sermons were chosen to be read after his death. Though this hypothesis needs to be tested, it illustrates the significance of exploratory data analysis. 

### Lexical dispersion

Lexical dispersion concerns the distribution of a certain term in a corpus. Gries comments that it is 'important not to rely only on the frequency of words, but also to consider how (un)evenly words are dispersed.' [@gries2016, 15].  Gries also suggests a measure for dispersion (DP) which can be computed for each corpus file, depending upon the size of the file and the relative frequency of the word that is studied [@gries2016,179]. Given the frequency lists (table \@ref(tab:topwords)), we may wonder how the terms 'church' and 'world are distributed in Newman's corpus. 

To illustrate the technique of lexical dispersion, a sample of 5 random sermons are drawn from the corpus and the terms 'church' and 'world' are indicated in each sermon. This gives an idea where in the sermons Newman uses the terms and how often the terms occur in individual sermons. The plot (figure \@ref(fig:lexical-dispersion)) quickly shows that one particular sermon has 'world' as its topic, namely corpus-file 09_07.txt. This is a sermon on Proverbs 11:21. The sermon with more interest in 'church' is sermon 09_23.txt, a sermon on John 4:42.

```{r lexical-dispersion, echo=FALSE, message=FALSE, warning=FALSE, out.width="100%", fig.cap = "Lexical dispersion plot"}

#example_dispersion

knitr::include_graphics(here("gen/images","plot_dispersion.tiff"))

```

Lexical dispersion plots relate the frequency of a word to its location in the corpus. The researcher does not need to do all the calculations since the R-package *Quanteda* takes care of this. The plot above can be generated through a few simple lines of code.

```
DEFINE search-terms
CREATE sample corpus by drawing a sample of five documents from the corpus
TOKENIZE the sample corpus
APPLY keyword-in-context on the search-terms to the tokenized corpus
PLOT the keyword-in-context with the function textplot_xray()
```
Now the lexical dispersion of the two terms of 'church' and 'world' has been clarified for the Newman's sermons, the question emerges how Newman uses these terms in relation to Spurgeon.

### Keyness 

Keyness measures the frequencies of a target corpus against a reference corpus and calculates the 'relative attraction of each word to the target corpus of a keyness statistic' [@gries2016,197]. One could, for instance, relate the terms of the sermons (target corpus) against a larger database of spoken English (reference corpus). It will become clear that sermons will reflect a specific type of language, language that is connect to church as a social institution. The language of a sermon is connected to the social roles of preachers and congregations: 'a preacher cannot give a sermon unless there is (at least) a (potential) congregation' [@stubbs2010,34]. Keyness is a statistical measure (e.g. Chi2) that measures how certain words characterise individual texts [@stubbs2010,25]. 

In the first table on word frequencies (table \@ref(tab:topwords)), we already got an impression of the different vocabularies of Newman and Spurgeon. The calculation of keyness provides an indicator how different the vocabularies are. To make a relatively fair comparison, the corpus with the same biblical texts is used. The code for calculating and plotting keyness looks like the following:

```
READ the balanced corpus on the same biblical texts
TOKENIZE the balanced corpus and remove punctuation, symbols and numbers
COMBINE a few phrases, such as 'I am' and combination with 'St' (such as St. Paul)
CREATE a document-term-matrix for each preacher
CALCULATE keyness of Newman (target) against Spurgeon (reference)
PLOT the top 20 terms
```

```{r keyness, echo=FALSE, message=FALSE, warning=FALSE, out.width="100%",fig.cap="Keyness: Newman's keyterms against Spurgeon's"}

#example_keyness

knitr::include_graphics(here("gen/images","plot_keyness.tiff"))

```

From figure \@ref(fig:keyness) new hypotheses emerge. Newman significantly uses 'or' more often in his sermon than Spurgeon does. Does this indicate that he is more likely to have his listeners choose between alternatives? Denis Robinson points out that Newman often uses 'binaries' as a rhetorical device: "Newman often places two or three terms together, drawing dramatic contrasts and inviting comparison, yet remaining rather vague as regards the way in which the terms actually fit together." [@robinson2009,245]. On the other hand, Spurgeon addresses the audience far more directly in the second person, 'you'. Perhaps this fits Oliphant Old's description of Spurgeon's sermons as evangelistic [@old2007,426-432], the 'you' being the audience addressed by the one who proclaims the gospel. 

Exploratory data analysis is endless. Each plot that is generated or technique that is applied opens up new findings in the data and stimulates the formulation of new hypotheses. 

## Sentiment analysis

```{r init sentiments, message=FALSE, warning=FALSE, include=FALSE}

rm(list = ls())

source(here::here("src/05-analysis-basic","medians_plot-sentiments.R"))


```
Sentiment analysis is among the well-known tools in *natural language processing*. It has been applied to study positive and negative sentiments in customer reviews (movies, restaurants) or in social media messages (tweets). Anthony-Paul Cooper, for instance, studied the relationship between the sentiment of church-related tweets and church growth. Cooper applied *SentiStrength*, an algorithm that 'provides a rating of the magnitude of positive sentiment (on a scale of 1 to 5) and a rating of the magnitude of negative sentiment (on a scale of -1 to -5)' [@cooper2017]. Sentiment analysis offers the possibility to visualise the 'mood' of a sermon. Sermons engage the emotions of listeners. This addresses the rhetorical aspect of *pathos*. 

There are three flavours of sentiment analysis. First, there is a rule-based approach. This approach applies a sentiment dictionary to the data. Positive and negative words are scored. More sophisticated dictionaries rate affect or emotions. In the study of Jonathan Edwards' sermons, Michael Keller [@keller2018] applied the dictionary of affect in language (DAL). Sentiment analysis attaches emotional values to certain words, but a word-based approach leaves out the larger context of words. Second, supervised approaches are based upon annotated datasets. Supervised approaches are more energy consuming, because they start with the annotation of the documents according to sentiment values. After annotation, machine learning is invoked to develop a model that can classify new documents according to the sentiment values used in the annotation [@lei2021,17]. Third, deep-learning algorithms are often able to apply sentiment analysis to multiple languages and they yield better results than dictionary-based and supervised approaches [@mercha2023a]. 

The example below applies a deep-learning approach to the corpus with sermons on the same biblical texts. The 'sentiment.ai' algorithm[^10] is applied to all 112 sermons in the dataset with 56 biblical texts that Newman and Spurgeon both preached on. Sentiment is taken as an indicator for pathos. The code to determine the sentiments looks like this:

```
LOAD library sentiment.ai
INIT conda for tensorflow environment to run the model
LOAD balanced corpus
TRANSFORM balanced corpus into sentences
PREDICT sentiment score for each sentence (run model)
STORE sentiment scores, sentences and metadata in table 
```

After determining the sentiment scores for each sentence in the balanced corpus with biblical texts, the overall sentiment value for each of the 112 sermons is calculated. The median as central tendency measure is selected to determine the sentiment value of the document.[^12] This means the sentiment value of the sermon is calculated as the value of which half of the other sentences has a higher and half of the sentences has a lower value. 

```{r median-sentiments, echo=FALSE, message=FALSE, warning=FALSE, out.width="100%"}

#sample_medians

knitr::kable(toptail_medians, caption = "Three lowest and three highest differences of sentiment values between the preachers")

```
Next, the differences between the preachers are calculated. The bottom row in table \@ref(tab:median-sentiments) shows the largest difference; Newman uses more negative language than Spurgeon. Thus, the sermon on Hebrews 10:38 appears to be the sermon in which the overall sentiment of both preachers differs most (`r round(toptail_medians$difference[nrow(toptail_medians)],2)`). Based upon this comparison, the sermon on Hebrews 10:38 is a candidate to compare the emotional narratives in both preachers. A visualisation of the sentiment trajectories in both sermons illustrates how sentiment analysis can be used to project the narrative of entire sermons. Figure \@ref(fig:plot-sentiments) shows that Newman has more negatively toned sentences in his sermon, while Spurgeon clearly uses more positive language. 

```{r plot-sentiments, echo=FALSE, message=FALSE, warning=FALSE, out.width="100%", fig.cap="Sentiment trajectories in Newman and Spurgeon on Hebrews 10:38"}

#plot_least_similar
knitr::include_graphics(here("gen/images","plot_sentiments.tiff"))

```

A brief look at the titles of the sermons shows that Newman focussed on the second part of the verse in Hebr. 10: 'if any man draws back, My soul shall have no pleasure in him'. The title of Newman's sermon is: Transgressions and Infirmities. Spurgeon on the other hand preaches on the first part of the verse, 'Now, the just shall live by faith'. His sermon is titled: 'the vital force'. Already from the choice of the preachers to take a certain perspective on the text, the different tone of the sermons can be explained. Compare the opening line of each sermon, Spurgeon using uplifting language: 

> See here the germ of the Christian's life! See, too, how it blooms,
   blossoms and bears! But observe it is not said the just shall live for
   his faith, or because of the merit of his believing in God. (Spurgeon, volume 15, sermon 891)

Newman, on the other hand, starts with a warning: 

> Warnings such as this would not be contained in Scripture, were there no danger of our drawing back, and thereby losing that "life" in God's presence which faith secures to us.  (Newman, volume 5, sermon 14)

Therefore, sentiment analysis suggests that the language of preaching interacts with the biblical passage.

# Advanced Computational Techniques {#advanced}

From the basic operations in the previous sections, two insights concerning the two Victorian preachers in the dataset emerge. First, the differences in the language of preaching raise the issue of the *personalized* language in sermons. This observation reflects the hypothesis 'Preacher' that was formulated at the end of section \@ref(casestudy), namely that the language of preaching reflects features of the preacher, the preacher's theology, spirituality and personality. This hypothesis is sustained by the frequency tables (table \@ref(tab:topwords)). These tables contain various lists of keywords that occur most in the sermons of each preacher. Further, the plot that visualises the keyness of Newman in reference to Spurgeon (figure \@ref(fig:keyness)) also indicates that the language of both preachers differs significantly. 

The second insight points to another aspect of the language of preaching. Some of the differences between the preachers seem to be related to the biblical text. This leads to a next hypothesis:

> *Hypothesis Bible* The language of preaching reflects the language of the biblical passage that is explained in the sermon. 

Hypothesis 'Bible' was suggested by the results of sentiment analysis. The emotional tone of the sermons reflected the tone of the biblical text (table \ref@(tab:median-sentiments)). In the case of Hebrews 10:38 (figure \@ref(fig:plot-sentiments)) it turned out that Spurgeon preached in a positive tone because he reflected on the first half of the biblical text, while Newman's negative sentiment can be explained by his interest in the second half of the text. 

This section introduces advanced computational methods to explore these two hypotheses a further. The distinction between basic and advanced relies upon the difference between *applying* developed rules and models on the one hand, and *developing* new rules and models on the other. The basic operations used statistical measures (keyness) or applied algorithms (sentiment analysis) to the datasets. The advanced methods introduce the development of new models, to fit these models to the data and to validate the accuracy of these models. Van Atteveldt and others distinguish between two types of automatic text analysis, unsupervised and supervised modeling. Supervised modeling works with a priori defined categories, while in unsupervised modeling the categories are inductively constructed by the computer [@atteveldt2022, chapter 11]. Unsupervised modeling is applied to test the hypothesis 'Bible'. The computer clusters the words of the sermons into 'topics' and the model tests whether the topics are related to the biblical passages or not. Supervised modeling is used to test the hypothesis 'Preacher'. The computer 'learns' to classify sermon fragments and the classificion model predicts whether a sermon fragment should be classified as a 'Newman' or a 'Spurgeon' sermon.  

## Unsupervised Text Analysis: Topic modeling

In the study of religion topic modeling has been used by various researchers to cluster data into distinct categories. John Berneau generated a topic model of 74 topics and visualised the shifts of topics over time in the discourse on pastoral care in 4,054 articles of the *Journal of Pastoral Care and Counselling* between 1947-2018 [@bernau2021]. He concluded that traditional religious language has largely disappeared from the journal's content. Anne Agersnap and Kristoffer Nielbo applied topic modeling to a corpus of 11,955 Danish sermons [@agersnap2022]. Their topic model reconstructed the holidays of the liturgical calendar from the corpus with sermons. Their findings suggest a relationship between the holiday signal and the content of the sermons. Constantine Boussalis and others studied political speech in religious sermons. Political topics such as economy, war and welfare emerge from the corpus of 110,000 sermons. On the level of pastors, 7 out of 10 pastors delivered at least one sermon with political content [@boussalis2020]. Topic models have been described as a 'quantitative tool for a qualitative approach' [@jacobs2019a]. Agersnap and Nielbo describe the procedure of topic modeling as follows:

> A topic model makes the assumption[s] that a document is a distribution over a finite set of latent topics [...] [and] that the topics within a text have been chosen by the author before writing the text [...] With topic modeling, you try to reverse this process and trace the words back to the topics they came from, and consider the documents in terms of their distribution of topics. [@agersnap2022,93]

We use topic modeling to test the hypothesis 'Bible'. First, the most frequently preached from chapters in Spurgeon's corpus are selected. From this list, the chapters from the same book are filtered, keeping only the most often preached chapter from the book (see also table \@ref(tab:most-preached)). This means that John 6 is included, but John 14 and John 1 are removed. This leaves the following chapters: Psalm 119, Romans 8,  John 6, Hebrews 11, Isaiah 53, 2 Corinthians 5. Next, 20 sermons are sampled from each of these chapters. In total 120 sermons are included in the topic modeling exercise. 

```
ADD a document variable to the corpus: chapters
FIND the most preached chapters in the corpus
SAMPLE for each of these chapters 20 sermons from the corpus
TOKENIZE the sampled sermons 
CREATE document-term-matrix 
RUN LDA model for 13 topics (k = 13) on the document-term-matrix
```

Topic modeling consists of multiple runs. The variable *k* indicates the number of topics that need to be generated. After a few tries with different values of *k*, a model consisting of 13 topics gives a good result. Some chapters have terms in different topics, but the general tendency is that the chapters are connected to one or two dominant topics. Some biblical chapters have a dominant topic that is shared with other chapters. For instance, the sermons from John 6 are mainly put in topic 13, but topic 13 also contains several sermons on 2 Corinthians 5 and Romans 8. However, the sermons on Hebrews 11, Isaiah 53, Romans 8, 2 Corinthians 5, and Psalm 119, almost entirely make up one particular topic. The sermons and the top-10 words for each topic are projected below:

```
2 Cor. 5 > topic 3 [reconciliation, stead, ambassadors, wilt, yea, dost, shalt, to-day, wrought, thyself]

Hebrews 11 > topic 8 [staff, pharaoh, pharaoh's, rewarder, rahab, well-pleasing, decided, translated, egyptians, egyptian]

Isaiah 53 > topic 4 [remedy, cures, contemplation, acquaintance, blows, scourged, scourging, application, shrink, lament]

Psalm 119 > topic 9 [statues, petition, teacher, visit, custom, advocate, commandment, catch, uphold, besetting]

Romans 8 > topic 10 [conformed, predestined, first-born, conformity, heirship, undefiled, knock, amidst, relation, firstborn]
```

In conclusion, topic modeling suggests a positive evaluation of the hypothesis 'Bible'. Sermons on the same biblical text are grouped in one or two topics. This indicates that the terms used in these sermons are specifically connected to the preacher's text. 

## Supervised Text Analysis: Machine Learning and Classification

Hypothesis 'Preacher' investigates the relationship between the language of the sermon and the preacher. It states that the language of preaching reflects features of the preacher. We use machine learning to classify sermons according to the preacher.[^ml] The task is relatively easy: the computer has to decide between two classes. Does a sermon fragment fits Newman's style of preaching? If not, the computer decides that it probably closer to Spurgeon's preaching style. Though this task is minimal and has multiple limitations, it is straightforward and suited for the illustrative purposes in this chapter. 

First, a sample of 100 sermons from each preacher is drawn. The sample is tokenised into chunks of 200, meaning that all units for the machine learning task are of equal size: 200 tokens. Each chunk is automatically labelled with the name of the preacher. Next, the chunks are divided into a training and a test set. The chunks in the training set are used to build the model based upon naïeve Bayes; an algorithm that performs well to distinguish two classes (binary classification) [@atteveldt2022, 8.3.1]. 

[^ml]: For an introduction to machine learning, see @boehmke2019.

```
LOAD corpora spurgeon & newman
SET sample length to 100
SET chunksize and window to 200 and 10 respectively
FUNCTION create chunks with argument sermon-id
	TOKENIZE sermon-id: remove punctuation, numbers, and symbols
	SET tokens to lowercase
	REMOVE stopwords-list
	DIVIDE tokens into chunks of 200 (chunksize) with an overlap of 10 (window)
	REMOVE the chunks that are shorter than 200 (chunksize)
SAMPLE 100 (sample length) sermons from corpora
ADD document variables to sample corpus: preacher and unique sermon-id
REMOVE all document variables, except preacher and sermon-id
OUTPUT function 'create chunks' to all sermon-ids to table with sermon-chunks
BALANCE chunks by drawing an equal sample of all chunks from each preacher
CREATE document-term-matrix from the chunks
DIVIDE matrix into train (80%) and test set (20%)
BUILD model on the train set, use naive Bayes (quanteda: textmodel_nb)
```

Finally, the model is evaluated. The first evaluation is on the test set. If the test set performs well, the model is tested against other sermons by Spurgeon and Newman. Models are evaluated by creating a confusion matrix. This matrix projects the positive and negative predicted classes. Important are false positives (Newman is predicted, while the fragment is actually from Spurgeon) and false negatives (a fragment is falsely attributed to Spurgeon). Measurements like accuracy, precision, recall and F1 determine the model's performance [@boehmke2019,2.6.2]. Since all measurements are around 0.97 (on a scale of 0 to 1, 1 meaning perfect classification), the model performs very well on the test set. A similar result is obtained when the model is tested against sermons not included in the original sample. 

```
Confusion Matrix and Statistics

            predicted_class
actual_class newman spurgeon
    newman      189        7
    spurgeon      4      226
                                         
               Accuracy : 0.9742         
                 95% CI : (0.9543, 0.987)
    No Information Rate : 0.5469         
    P-Value [Acc > NIR] : <2e-16         
                                         
                  Kappa : 0.948          
                                         
 Mcnemar's Test P-Value : 0.5465         
                                         
            Sensitivity : 0.9793         
            Specificity : 0.9700         
         Pos Pred Value : 0.9643         
         Neg Pred Value : 0.9826         
              Precision : 0.9643         
                 Recall : 0.9793         
                     F1 : 0.9717         
             Prevalence : 0.4531         
         Detection Rate : 0.4437         
   Detection Prevalence : 0.4601         
      Balanced Accuracy : 0.9746         
                                         
       'Positive' Class : newman
```

Because the measures indicate that the algorithm performs very well in distinguising between Newman and Spurgeon, the credibility of hypothesis 'Preacher' increases. Given a certain sermon fragment, the model predicts the preacher. The language of sermons indeed very much reflects a preacher. 

Finally, can the model be used against other preachers? Here the limitations of the model must be taken into account. The model predicts whether a sermon segment belongs to Newman. If not, it assigns it to Spurgeon. The binary model (Newman or not-Newman)  could be used to predict of a new sermon fragment fits the model's features. Newman being a 19th Century Anglican preacher - at least before 1843 - should therefore be compared to an other Anglican preacher. Therefore, the model is tested against four sermons held at Lambeth conferences, from the current (Welby) and a previous Archbishops (Williams, Carey and Ramsey) of the Anglican church (figure \@ref(fig:classification)). 

```{r message=FALSE, warning=FALSE, include=FALSE}
source(here("src/06-analysis-advanced","test_preacher-model.R"))
```


```{r classification, echo=FALSE, message=FALSE, warning=FALSE, out.width="100%", fig.cap="Predicting the preacher of Lambeth sermons"}

# plot_lambeth

knitr::include_graphics(here("gen/images","plot_lambeth.tiff"))

```

Besides the differences in length between the sermons, the plot shows that Ramsey's sermon during the Lambeth conference (1968) reflects Newman's preaching style. At the end of the 20th Century, it seems that fewer sermon fragments fit Newman's style and are thus classified as similar to Spurgeon. Because of the binary classification the computer is forced to put the sermon fragments in one of both categories. A more refined model would be able to distinghuish with more nuances. However, a much larger dataset that includes multiple preachers is needed for this. 

Finally, the test of the model suggests a new hypothesis. Hypothesis 'Preacher' may be too much focussed upon the subject of the individual preacher. It is more likely that the language of groups of preachers have certain features in common. This inspires a new hypothesis:

> *Hypothesis Tradition* The language of preaching reflects features of a shared preaching tradition.   


# Implications for homiletical research and theory {#conclusions}

The language of preaching is somehow connected to person, Bible and tradition. These insights have not been formulated on the basis of a deductive, pre-empirical, theological theory, but they emerge from inductive analysis of empirical data. 

Computational methods enrich the toolbox of the empirical homiletician, in scope as well as in interconnectedness and innovation. 

1. *Scope*. It is possible to study large datasets with sermons in one project. For instance, the classifier that is able to recognize Newman's style of preaching, was based upon data from 200 sermons. These methods make it possible to pursue research questions that are impossible to answer due to the limitations of manual coding and analysis. 

2. *Interconnected*. Word frequencies, document-term-matrices, tagging, models and visualisations come together in complex pipelines in which several methods are combined, only limited by the creativity and technological skills of the researcher. 

3. *Innovation*. Computational tools are part of an ongoing process of technological *innovation*. Sentiment analysis is an interesting example. Deep-learning algorithms provide major innovations of a technique that has been around since the early days of computational research.

This chapter has been written by a theologian with a strong interest in empirical methods and computational approaches. Its main aim has been to explore an multidisiplinary territory. Hans van der Ven argued in his framework for empirical theology that the theologian needs to adopt an intradisciplinary attitude: borrow concepts, methods and techniques from other fields, adapt them to the specific theological questions, and integrate them into research projects [@vanderven1993h,101]. The features of computational research make it necessary for theologians to develop their statistical and data science competences or, and even better, to participate in research teams with computational scientists. This chapter only scratched the surface of computational analysis. Many approaches, such as parts-of-speech tagging (POS), named-entity recognition (NER), word embeddings and intertextuality have not been dealt with. 

Three limitations close this introduction to computational sermon analysis. First, computational analysis should be aware of the critical voices within digital humanities. Augustine Farinola, for instance, challenges the distinction of 'close reading' versus 'distant reading' which forms the theoretical basis for computational methods. He argues for human agency in acts of interpretation [@farinola2023]. Next, this chapter started with building a corpus from sermons available in the public domain. This emphasizes the need for large databases of sermons. The nature and organisation of data raise epistemological issues and generate practical challenges[@wildman2021,139-145]. Finally, Mathew Gillings and others have argued that working with corpora should not move the researcher away from discourse [@gillings2023,47-49]. For homiletics, this means that computational sermon analysis should always remain connected to the actual practice of preaching.


[^2]: https://ccel.org/. Recently, more of Spurgeon's sermons have been rediscovered and published in a series *The Lost Sermons* (7 volumes between 2017 and 2022). This collection consists of photographs of Spurgeon's handwritten sermons.  
[^4]: Web scraping is a method used in  data science and digital humanities to extract data from the internet. Cf. @black2016. Black argues that the Internet could be used as a low-cost and resource-light source for research data. He also discusses the legal aspects of webscraping for academic purposes. 
[^5]: Due to digital availability, for this case study only Newman's Anglican sermons are included: 8 volumes of his *Parochial and Plain Sermons* and a volume called *Sermons on Subjects of the Day* which includes his farewell sermon entitled 'The Parting of Friends'. The sermons are preached between 1825 and 1843. On Newman's Catholic sermons, see [@scott2012]. 
[^8]: For the technique of pseudocode, see @gries2016,175. Gries refers to John Dalbey who developed the standard for pseudocode in 2003. Retrieved November 26, 2023, from http://users.csc.calpoly.edu/~jdalbey/SWE/pdl_std.html. 
[^10]: https://benwiseman.github.io/sentiment.ai/. 
[^12]: The method proposed here is simple, but can suffice for illustrative purposes. For a sophisticated approach of determining document sentiments, see @liu2020, chapter 3.

# Supplemental Material {-}

R scripts written for this chapter are available at Github, https://github.com/ttjpleizier/computational-sermon-analysis. An OSF (open science framework) project is available at https://osf.io/4a2yz/ and includes the downloadable corpora that have been constructed for this project.

# Acknowledgments {-}

Parts of this chapter have been presented at various research workshops and conferences. The authors thanks colleagues at ISERT Assissi 2022, IAPT Seoul 2023, and the Colloquium Techno-Humanism of the Theological Faculty of Pretoria and the Protestant Theological University. A special word of thanks to Wim Otte, clinical epidemiologist specialized in predictive modeling and theologian, for travelling along this project. 


# Selected Bibliography {-}